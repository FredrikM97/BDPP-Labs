\documentclass[../main.tex]{subfiles}
\begin{document}


\section{Conclusion}
The models performed quite poorly based on accuracy. However the the models perform better with individual states than running the entire dataset at once. This means that the time to operate is shortened and provide higher precision of the model. Based on the result from Spark the analysis, preprocessing and machine learning performed smooth and most of the operations were done in parallel. Since the classifiers had 4 values to choose from and data consist of none binary and non linear data which in this case make it more difficult to process a model. According to other models using the same dataset \footnote {\url{https://www.kaggle.com/phip2014/ml-to-predict-accident-severity-pa-mont}} that archived 95.4\% accuracy for Logistic regression when using the original features without quantilization which means that it would be possible to improve the result but since the model crashed when using a big feature vector means that there are some limitations in hardware, which could be a cause of out of memory on the driver memory. Additional changes on the Spark parameter settings could be performed in order to run the model correctly. Future work may include refining and analyzing the dataset further, improving feature crossing to follow the theory and implementing an embedding for better performance are two steps that should be taken to obtain a value that can match reality.

\section{Discussion}
One of the reasons that the model could have performed poorly is that rows that contain null are not discarded, instead of numerical values they are replaced with the median and for categorical values they get unique value as well in the one-hot-encoding. This is due to the fact that the amount of rows containing bad data (missing values/null) would account for almost the full set of data if not removed. In order to handle this some columns containing a lot of missing data is discarded and the rest is still applied. Replacing the original position features improved the efficiency of preprocessing, training time and reduced the feature vector. During preprocessing it noticeable that a feature crossing is not possible to do with one-hot-vector. This is due to that the vector size will be too big for limited hardware to handle and therefor take a lot of time to compute. Alternative to implementing one-hot or embedding, \textit{Advanced analytics with spark: patterns for learning from data at scale} \cite{ryza2017advanced} suggests a method called FeatureHashing that specifies a given length and from there creates unique combinations of data instead of creating a unique feature for each unique class.
\end{document}
