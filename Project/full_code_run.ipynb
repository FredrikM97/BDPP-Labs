{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: [('spark.driver.extraClassPath', '/opt/conda/lib/python3.7/site-packages/sparkmonitor/listener.jar'), ('SPARKMONITOR_UI_HOST', '192.168.1.228'), ('spark.executor.id', 'driver'), ('spark.driver.host', 'localhost'), ('spark.app.name', 'Spark Project'), ('spark.executor.cores', '4'), ('executor-memory', '6'), ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'), ('spark.app.id', 'local-1588644394139'), ('spark.num.executors', '6'), ('spark.kryoserializer.buffer.max', '10'), ('spark.extraListeners', 'sparkmonitor.listener.JupyterSparkMonitorListener'), ('spark.rdd.compress', 'True'), ('spark.driver.port', '45469'), ('spark.driver.memory', '8g'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.memory.fraction', '.6'), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true')]\n",
      "CA\n",
      "Label: ['Street', 'Side', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone', 'Airport_Code', 'Wind_Direction', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight'] \n",
      "Categories: ['Street', 'Side', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone', 'Airport_Code', 'Wind_Direction', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
      "Numerical: ['TMC', 'Start_Lat', 'Start_Lng', 'Distance(mi)', 'Number', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)']\n",
      "+-----+--------+-------------------+---------+-----------+------------+------+----------+----+---------------+---------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "|  TMC|Severity|         Start_Time|Start_Lat|  Start_Lng|Distance(mi)|Number|    Street|Side|           City|   County|State|Zipcode|Country|  Timezone|Airport_Code|  Weather_Timestamp|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Amenity| Bump|Crossing|Give_Way|Junction|No_Exit|Railway|Roundabout|Station| Stop|Traffic_Calming|Traffic_Signal|Turning_Loop|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|Start_Hour|Start_Month|Weather_Hour|Weather_Month|\n",
      "+-----+--------+-------------------+---------+-----------+------------+------+----------+----+---------------+---------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "|201.0|       2|2016-06-23 10:31:12|38.653061|-121.070541|         0.0|  null|Latrobe Rd|   R|El Dorado Hills|El Dorado|   CA|  95762|     US|US/Pacific|        KMHR|2016-06-23 10:46:00|          77.0|         null|       34.0|       30.02|          10.0|            SW|            3.5|             null|            Clear|  false|false|   false|   false|   false|  false|  false|     false|  false|false|          false|         false|       false|           Day|           Day|              Day|                  Day|         3|          6|           3|            6|\n",
      "+-----+--------+-------------------+---------+-----------+------------+------+----------+----+---------------+---------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "model_indexing_dir = \"log_index.md\"\n",
    "\n",
    "images_dir = \"images/ML\"\n",
    "models_dir = \"models\"\n",
    "\n",
    "\n",
    "\n",
    "file = \"data/US_Accidents_Dec19.csv\"\n",
    "\n",
    "\n",
    "df, sc,spark = setup_spark(file)\n",
    "\n",
    "\n",
    "sc\n",
    "\n",
    "# Debug tool\n",
    "\n",
    "df,_ = df.randomSplit([1.0,0.0], 1) # Speed up the program a bit\n",
    "df.count()\n",
    "\n",
    "for region in df.groupBy('State').count().orderBy('count',ascending=False).toPandas()['State']:\n",
    "    timeSignature = str(datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "    createDirectory(models_dir+\"/\"+timeSignature +\"/\")\n",
    "    logs_dir = models_dir+\"/\"+ timeSignature +\"/logs.md\"\n",
    "    printSparkConf(sc, logs_dir)\n",
    "\n",
    "    filter_state = str(region)\n",
    "    print(filter_state)\n",
    "    \n",
    "    # 1. Define variables\n",
    "\n",
    "    colLabel = [\"Severity\"]\n",
    "\n",
    "    colRem = ['ID', \n",
    "              'Source',\n",
    "              'End_Time',\n",
    "              'End_Lat',\n",
    "              'End_Lng',\n",
    "              'Description',\n",
    "            ]\n",
    "\n",
    "    df, colCat, colNum = setup_variables(df, sc, colLabel, colRem, logs_dir)  \n",
    "\n",
    "    write2file(logs_dir,\"Number of rows\", str(df.count()))\n",
    "\n",
    "    # 2. Preprocessing\n",
    "\n",
    "    from pyspark.ml.feature import VectorAssembler,OneHotEncoderEstimator,QuantileDiscretizer, StringIndexer, Imputer,StandardScaler,MinMaxScaler\n",
    "    from pyspark.ml import Pipeline\n",
    "    from numpy import argmax\n",
    "\n",
    "    ## Analyse single state\n",
    "    #This could be CA since it stands for a little less than 50% of the total set\n",
    "\n",
    "    if filter_state != '':\n",
    "        df = df.filter(df.State == filter_state) # Lowers the dataset quite a lot\n",
    "        write2file(logs_dir,\"Specified state\",str(filter_state))\n",
    "    else:\n",
    "        write2file(logs_dir,\"No state specified\",\"\")\n",
    "\n",
    "    ## Modify time\n",
    "    #Convert the timestamp into a numeric value and then into a string so that the time of day and month can be categorised.\n",
    "\n",
    "    # Convert to int then cast to string\n",
    "\n",
    "    df = df.withColumn('Start_Hour', hour(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "    df = df.withColumn('Start_Month', month(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "    df = df.withColumn('Weather_Hour', hour(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "    df = df.withColumn('Weather_Month', month(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "    df.show(1)\n",
    "    write2file(logs_dir,\"Dataset after modifying UTC timestamp\", str(df.take(1)))\n",
    "\n",
    "    ## Quantiles function\n",
    "    #Alternative solution to get position \n",
    "    #Remove City, Country, State, Zipcode, Airport_Code\n",
    "    #In order to reduce dimensionality feature crossing will be applied upon the Start_Lat and Start_Lng then fused in order to create a new set of data\n",
    "\n",
    "    # TODO - Calculate how many buckets needed (Bins)\n",
    "    discretizer_Lat = QuantileDiscretizer(numBuckets=100, inputCol=\"Start_Lat\", outputCol=\"Start_Lat_disc\")\n",
    "    discretizer_Lng = QuantileDiscretizer(numBuckets=100, inputCol=\"Start_Lng\", outputCol=\"Start_Lng_disc\")\n",
    "\n",
    "    # Into categorical values\n",
    "    indexer_cord = [StringIndexer(inputCol=c + \"_disc\", outputCol=c+\"_IDX\") for c in [\"Start_Lat\",\"Start_Lng\"]]\n",
    "\n",
    "    # One-hot (feature crossing)\n",
    "    encoder_cord = OneHotEncoderEstimator(inputCols=[indexer.getOutputCol() for indexer in indexer_cord], outputCols=[\"{0}_vec\".format(indexer.getOutputCol()) for indexer in indexer_cord])\n",
    "\n",
    "    pipeline_cord = Pipeline(stages=[discretizer_Lat,discretizer_Lng, *indexer_cord, encoder_cord])\n",
    "    preprocessed_cord = pipeline_cord.fit(df).transform(df)\n",
    "\n",
    "    position = VectorAssembler(inputCols=[\"Start_Lat_IDX_vec\",\"Start_Lng_IDX_vec\"], outputCol=\"position\")\n",
    "\n",
    "    df = position.transform(preprocessed_cord)\n",
    "\n",
    "    df.show(1)\n",
    "    write2file(logs_dir,\"Dataset after adding quantiles\",str(df.take(1)))\n",
    "\n",
    "    ## Cast data \n",
    "    #Cast all the datacolumns into correct format so they will be sorted to numerical or categorical values. Data removed caused to big feature vector, contained too many NaN values or were converted into another form.\n",
    "\n",
    "    df = df.select(\n",
    "            #col('TMC').cast('string'),  Lot of missing values\n",
    "            col('Severity').cast('int'),\n",
    "            col('Start_Hour').cast('string'),\n",
    "            col('Start_Month').cast('string'),\n",
    "            col('Weather_Hour').cast('string'),\n",
    "            col('Weather_Month').cast('string'),\n",
    "            col('position'), # already preprocessed\n",
    "            col('Distance(mi)').cast('double'),\n",
    "            col('Side').cast('string'),   \n",
    "            col('Temperature(F)').cast('double'),\n",
    "            col('Wind_Chill(F)').cast('double'), #  Lot of missing values\n",
    "            col('Humidity(%)').cast('double'),\n",
    "            col('Pressure(in)').cast('double'),\n",
    "            col('Visibility(mi)').cast('double'),\n",
    "            col('Wind_Direction').cast('string'),\n",
    "            col('Wind_Speed(mph)').cast('double'),  #Lot of missing values\n",
    "            col('Weather_Condition').cast('string'),\n",
    "            col('Amenity').cast('string'),\n",
    "            col('Bump').cast('string'),\n",
    "            col('Crossing').cast('string'),\n",
    "            col('Give_Way').cast('string'),\n",
    "            col('Junction').cast('string'),\n",
    "            col('No_Exit').cast('string'),\n",
    "            col('Railway').cast('string'),\n",
    "            col('Roundabout').cast('string'),\n",
    "            col('Station').cast('string'),\n",
    "            col('Stop').cast('string'),\n",
    "            col('Traffic_Calming').cast('string'),\n",
    "            col('Traffic_Signal').cast('string'),\n",
    "            col('Turning_Loop').cast('string'),\n",
    "            col('Sunrise_Sunset').cast('string'),\n",
    "            col('Civil_Twilight').cast('string'),\n",
    "            col('Nautical_Twilight').cast('string'),\n",
    "            col('Astronomical_Twilight').cast('string')\n",
    "        ) \n",
    "\n",
    "    # Since the last check can be unorganised we recreate a new list that contains all data\n",
    "    #df.cache()  # TODO does this work? - Heatbeat crash otherwise\n",
    "    colLabel = [\"Severity\"]\n",
    "\n",
    "    colCat, colNum = createNewClasses(df, sc, colLabel, logs_dir)\n",
    "\n",
    "    ## Clean data\n",
    "\n",
    "    ### Recheck the missing values\n",
    "    #Check so that the output contains 0 missing values\n",
    "\n",
    "    printMissingValues(df, logs_dir)\n",
    "\n",
    "    ### Drop NaN\n",
    "    #Cant drop NaN since imputer should take care of the most of them! However we can drop values from the categorical set if we want!\n",
    "\n",
    "    #old = df.count()\n",
    "    #df = df.na.drop()\n",
    "    #print(\"Rows removed:\",old-df.count())\n",
    "\n",
    "    ### Recheck the missing values\n",
    "    #Check so that the output contains 0 missing values\n",
    "\n",
    "    printMissingValues(df, logs_dir)\n",
    "\n",
    "    ### Remove data with occurance less than 1%\n",
    "    #Based on information from analysis. With further analysis lower procentage can be used to find better results.\n",
    "\n",
    "    original_rows = df.count()\n",
    "    n = int(df.count()*0.001) # Limit the plot to ignore conditions below an limit\n",
    "\n",
    "    weather_freq = df.groupBy('Weather_Condition').count().orderBy('count',ascending=False)\n",
    "    df_filtered = weather_freq.filter(weather_freq['count'] > n)\n",
    "    filtered_conditions = df_filtered.select(\"Weather_Condition\").rdd.flatMap(lambda x: x).collect()\n",
    "    df = df.filter(df['Weather_Condition'].isin(*filtered_conditions))\n",
    "\n",
    "    df_filtered.show()\n",
    "    print(\"Rows removed:\",original_rows - df.count())\n",
    "\n",
    "    write2file(logs_dir,\"Weather condition\", str(df_filtered.take(df_filtered.count())) + \"\\nRows removed: \" + str(original_rows - df.count()))\n",
    "\n",
    "    ## Prepare Pipeline\n",
    "\n",
    "    imputer = Imputer(inputCols=colNum, outputCols=colNum)\n",
    "    imputer.setStrategy(\"median\")\n",
    "\n",
    "    num_assembler = VectorAssembler(inputCols=colNum, outputCol=\"num_features\",handleInvalid=\"skip\")\n",
    "    scaler = MinMaxScaler(min=0.0, max=1.0, inputCol=\"num_features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "    indexers = [StringIndexer(inputCol = c, outputCol = c +'_IDX', handleInvalid='skip') for c in colCat]\n",
    "\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[indexer.getOutputCol() for indexer in indexers], outputCols=[\"{0}_vec\".format(indexer.getOutputCol()) for indexer in indexers],handleInvalid=\"skip\")\n",
    "\n",
    "    ## Preprocessing - Pipeline\n",
    "\n",
    "    imputer = Imputer(inputCols=colNum, outputCols=colNum)\n",
    "    imputer.setStrategy(\"median\")\n",
    "\n",
    "\n",
    "    num_assembler = VectorAssembler(inputCols=colNum, outputCol=\"num_features\",handleInvalid=\"skip\")\n",
    "    #scaler = StandardScaler(inputCol=\"num_features\", withMean=True, withStd=True, outputCol=\"scaledFeatures\")\n",
    "    scaler = MinMaxScaler(min=0.0, max=1.0, inputCol=\"num_features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "    indexers = [StringIndexer(inputCol = c, outputCol = c +'_IDX', handleInvalid='skip') for c in colCat]\n",
    "\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[indexer.getOutputCol() for indexer in indexers], outputCols=[\"{0}_vec\".format(indexer.getOutputCol()) for indexer in indexers],handleInvalid=\"skip\")\n",
    "\n",
    "    # Categorical values\n",
    "\n",
    "    numPipeline = Pipeline(stages=[imputer, num_assembler, scaler])\n",
    "    catPipeline = Pipeline(stages=[*indexers, encoder])\n",
    "\n",
    "    pipeline = Pipeline(stages=[numPipeline, catPipeline])\n",
    "    preprocessed_df = pipeline.fit(df).transform(df)\n",
    "\n",
    "    preprocessed_df.select(\"position\",\"scaledFeatures\",*[c + \"_IDX_vec\" for c in colCat]).show(1,False)\n",
    "\n",
    "    ## Combine features\n",
    "\n",
    "    va2 = VectorAssembler(inputCols=[\"position\",\"scaledFeatures\",*[c + \"_IDX_vec\" for c in colCat]], outputCol=\"final_features\")\n",
    "\n",
    "    df = va2.transform(preprocessed_df)\n",
    "\n",
    "    df = df.withColumn('label', col(\"Severity\"))\n",
    "    df_features = df.withColumn('features', df.final_features).select(\"features\",\"label\")\n",
    "    df_features.show(1, False)\n",
    "\n",
    "    write2fileModel(df_features, models_dir,\"df_features\", timeSignature)\n",
    "\n",
    "    #df_features.write.format('parquet').mode('overwrite').option(\"header\", \"true\").save(models_dir+'/df_features.parquet')\n",
    "    write2file(logs_dir,\"Feature set size\", str(df_features.count()) + \"\\n\\n__Feature vector and label:__\\n\" + str(df_features.take(1)))\n",
    "    write2file(logs_dir,\"Number of rows\", str(df_features.count()))\n",
    "    print(\"Feature set size: \",df_features.count())\n",
    "\n",
    "    ## Count categorical values\n",
    "\n",
    "    # Checking how many classes that can be used\n",
    "    tmp = [df.select(countDistinct(c).alias(c)).collect()[0] for c in [*colCat, \"position\"]] \n",
    "    print(\"Unique column values:\", tmp)\n",
    "\n",
    "    write2file(logs_dir,\"Unique column values\", str(tmp))\n",
    "\n",
    "    # 3. Feature importance\n",
    "\n",
    "    from pyspark.ml.feature import ChiSqSelector,PCA\n",
    "\n",
    "    ## PCA\n",
    "    #In order to understand how much the variance affect the dataset we check with PCA. Try to get above 90%\n",
    "\n",
    "    k=250\n",
    "    pca = PCA(k=k, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "    pca_model = pca.fit(df_features)\n",
    "    pca_df = pca_model.transform(df_features)\n",
    "    #chi_model[0].pca_features\n",
    "\n",
    "    print(\"PCA - Feature Variance:\",\"Top 50:\\n\" + str(pca_model.explainedVariance[:50]) + \"\\nNumber of items: \"+str(k)+\"\\nSum of variance: \"+ str(sum(pca_model.explainedVariance)))\n",
    "    write2file(logs_dir,\"PCA - Feature variance\", \"Top 50:\\n\" + str(pca_model.explainedVariance[:50]) + \"\\nNumber of items: \"+ str(k) +\"\\nSum of variance: \"+ str(sum(pca_model.explainedVariance)))\n",
    "    write2fileModel(pca_df, models_dir, \"pca_df\", timeSignature)\n",
    "\n",
    "    ## ChiSqSelector\n",
    "    #Check top 100 which of the values in the feature vector\n",
    "\n",
    "    selector = ChiSqSelector(numTopFeatures=k, \n",
    "                             labelCol='label', \n",
    "                             featuresCol='features', \n",
    "                             outputCol=\"selectedFeatures\",\n",
    "                             selectorType='numTopFeatures', \n",
    "                             percentile=0.1, \n",
    "                             fpr=0.05, fdr=0.05, fwe=0.05)\n",
    "\n",
    "    chi_model = selector.fit(df_features)\n",
    "    chi_df = chi_model.transform(df_features)\n",
    "\n",
    "    write2file(logs_dir,\"Top selected features according to ChiSqSelector\", str(chi_model.selectedFeatures)+ \"\\nNumber of features: \" + str(k) + \"\\nExample data:\\n\"+str(chi_df.take(5)))\n",
    "    write2fileModel(chi_df, models_dir, \"chi_df\", timeSignature)\n",
    "\n",
    "    print(\"Top selected features according to ChiSqSelector:\", chi_model.selectedFeatures)\n",
    "    print(\"Transformed selected features:\",chi_df.head().selectedFeatures)\n",
    "    print(\"Transformed feature vector:\", chi_df.show(10))\n",
    "\n",
    "    # 4. Machine learning\n",
    "\n",
    "    from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier,RandomForestClassifier\n",
    "    from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "    # Use the chi_df for training since it is reduced\n",
    "    #trainSet, testSet = df_features.randomSplit([0.8 ,0.2], 1)\n",
    "    trainSet, testSet = chi_df.randomSplit([0.8 ,0.2], 1)\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "    trainSet.show(2,False)\n",
    "\n",
    "    ## Logistic regression\n",
    "\n",
    "    lr = LogisticRegression(labelCol=\"label\", featuresCol=\"selectedFeatures\")\n",
    "\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "                    .addGrid(lr.regParam, [0.1,0.08,0.01]) \\\n",
    "                    .addGrid(lr.maxIter, [5,10.15]) \\\n",
    "                    .addGrid(lr.elasticNetParam, [0.6,0.8]) \\\n",
    "                    .build()\n",
    "    try: \n",
    "        _, _ = evaluateModel(lr, paramGrid, \"LR_Model\", trainSet, testSet, timeSignature, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logs_dir=logs_dir,models_dir=models_dir)\n",
    "    except Exception as e:\n",
    "        write2file(logs_dir,\"Error:\", str(e))\n",
    "        print(e)\n",
    "\n",
    "    ## Decision tree\n",
    "\n",
    "    dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"selectedFeatures\") \n",
    "\n",
    "    paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "    _, _ = evaluateModel(dt, paramGrid, \"DT_Model\", trainSet, testSet, timeSignature, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logs_dir=logs_dir,models_dir=models_dir)\n",
    "\n",
    "    ## Random forest\n",
    "\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"selectedFeatures\") #numTrees=10\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "                    .addGrid(rf.numTrees,[10,15,20,25]) \\\n",
    "                    .build()\n",
    "\n",
    "    _, _ = evaluateModel(rf, paramGrid, \"RF_Model\", trainSet, testSet, timeSignature, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logs_dir=logs_dir,models_dir=models_dir)\n",
    "\n",
    "    write2file(\"logs_dir\",\"Program finished!\", \"\")\n",
    "\n",
    "    write2file(model_indexing_dir,\"New model folder created\", \\\n",
    "               \"Model finished: Yes\" + \\\n",
    "               \"Folder name: \" + timeSignature + \\\n",
    "               \"State: \"+ filter_state + \\\n",
    "               \"Logs directory: \" + logs_dir +  \\\n",
    "               \"File: \" + file + \\\n",
    "               \"Note: \" \\\n",
    "              )\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
