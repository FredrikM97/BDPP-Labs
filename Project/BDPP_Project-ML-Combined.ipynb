{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20200516135437-0001\n",
      "KERNEL_ID = af72c332-d42c-4184-a6f4-b0743b52ba66\n"
     ]
    }
   ],
   "source": [
    "# The code was removed by Watson Studio for sharing.\n",
    "# Load dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since all data came in as string we convert them back\n",
    "from pyspark.sql.functions import col\n",
    "df = df.select(\n",
    "        col('TMC').cast('double'),\n",
    "        col('Severity').cast('int'),\n",
    "        col('Start_Time').cast('timestamp'),\n",
    "        col('Start_Lat').cast('double'),\n",
    "        col('Start_Lng').cast('double'),\n",
    "        col('Distance(mi)').cast('double'),\n",
    "        col('Number').cast('double'),\n",
    "        col('Street').cast('string'),\n",
    "        col('Side').cast('string'),\n",
    "        col('City').cast('string'),\n",
    "        col('County').cast('string'),\n",
    "        col('State').cast('string'),\n",
    "        col('Zipcode').cast('string'),\n",
    "        col('Country').cast('string'),\n",
    "        col('Timezone').cast('string'),\n",
    "        col('Airport_Code').cast('string'),\n",
    "        col('Weather_Timestamp').cast('timestamp'),\n",
    "        col('Temperature(F)').cast('double'),\n",
    "        col('Wind_Chill(F)').cast('double'),\n",
    "        col('Humidity(%)').cast('double'),\n",
    "        col('Pressure(in)').cast('double'),\n",
    "        col('Visibility(mi)').cast('double'),\n",
    "        col('Wind_Direction').cast('string'),\n",
    "        col('Wind_Speed(mph)').cast('double'),\n",
    "        col('Precipitation(in)').cast('double'),\n",
    "        col('Weather_Condition').cast('string'),\n",
    "        col('Amenity').cast('string'),\n",
    "        col('Bump').cast('string'),\n",
    "        col('Crossing').cast('string'),\n",
    "        col('Give_Way').cast('string'),\n",
    "        col('Junction').cast('string'),\n",
    "        col('No_Exit').cast('string'),\n",
    "        col('Railway').cast('string'),\n",
    "        col('Roundabout').cast('string'),\n",
    "        col('Station').cast('string'),\n",
    "        col('Stop').cast('string'),\n",
    "        col('Traffic_Calming').cast('string'),\n",
    "        col('Traffic_Signal').cast('string'),\n",
    "        col('Turning_Loop').cast('string'),\n",
    "        col('Sunrise_Sunset').cast('string'),\n",
    "        col('Civil_Twilight').cast('string'),\n",
    "        col('Nautical_Twilight').cast('string'),\n",
    "        col('Astronomical_Twilight').cast('string')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions used to simplify\n",
    "Some of the following functions are deprecated since they wont work on the cloud (create spark session, save logs and model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, count, col, isnan, countDistinct,from_unixtime,from_utc_timestamp, unix_timestamp,split, to_timestamp, hour, month, lit,collect_list, max\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from datetime import datetime\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "class logging():\n",
    "    def __init__(self, models_dir, logs_dir, image_dir, foldername, enabled=False):\n",
    "        self.timeSignature = str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "        foldername = foldername + \"_\" +self.timeSignature\n",
    "        self.models_dir = models_dir + \"/\" +foldername +\"/\" \n",
    "        self.logs_dir = logs_dir + \"/\" +foldername + \"/logs.md\" \n",
    "        self.image_dir = image_dir +\"/\" + foldername +\"/\" \n",
    "        \n",
    "        self.enabled = enabled\n",
    "        \n",
    "        self.createDirectory(self.logs_dir)\n",
    "        \n",
    "    \n",
    "    def createDirectory(self,folder):\n",
    "        \"\"\"\n",
    "        Creates empty folder if it does not exist\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        folder: string\n",
    "            Path to folder\n",
    "        Return\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        if self.enabled==False: return None\n",
    "        import os\n",
    "        os.makedirs(os.path.dirname(folder))\n",
    "        \n",
    "        \n",
    "    def write2file(self, title, info, logs_dir=None):\n",
    "        \"\"\"\n",
    "        Write data to filelog\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file: string\n",
    "            Path to file\n",
    "        title: string\n",
    "            Title for info\n",
    "        info: string\n",
    "            Data that shall be stored\n",
    "\n",
    "        Return\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.enabled==False: return None\n",
    "        if logs_dir == None: logs_dir = self.logs_dir\n",
    "            \n",
    "        f = open(logs_dir, \"a\")\n",
    "        f.write(f\"\\n\\n# {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} \\n__{title}:__\\n{info}\")\n",
    "        f.close()\n",
    "        \n",
    "    def saveImage(self, image, name, image_dir=None):\n",
    "        \"\"\"\n",
    "        Save file to disk\n",
    "        Parameters\n",
    "        ----------\n",
    "        image: obj\n",
    "            Image object\n",
    "        name: string\n",
    "            Name of file\n",
    "        image_dir: string, Optional, Default=None\n",
    "            Alternative path to file\n",
    "\n",
    "        Return\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        if self.enabled==False: return None\n",
    "        if image_dir == None: image_dir = self.image_dir\n",
    "        try:\n",
    "            image.figure.savefig(image_dir +\"/\" + name)\n",
    "        except:\n",
    "            image.savefig(image_dir +\"/\" + name)\n",
    "            \n",
    "    def write2fileModel(self, model, model_name, models_dir=None):\n",
    "        \"\"\"\n",
    "        Write the model to filesystem\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model: obj\n",
    "            Model object\n",
    "\n",
    "        models_dir:\n",
    "            Directory for the model\n",
    "\n",
    "        model_name: string\n",
    "            Name of the model\n",
    "\n",
    "        timeSignature: string\n",
    "            Time when program started\n",
    "\n",
    "        Return\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        if self.enabled==False: return None\n",
    "        if models_dir == None: models_dir = self.models_dir\n",
    "            \n",
    "        print(f\"Saving into: {models_dir}{model_name}\")\n",
    "        try:\n",
    "            model.write.format('parquet').mode('overwrite').option(\"header\", \"true\").save(f\"{models_dir}/{model_name}\")\n",
    "            print(\"Saving through format 1\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            model.write().overwrite().save(f\"{models_dir}/{model_name}\")\n",
    "            print(\"Saving through format 2\")\n",
    "           \n",
    "    def setLogDir(self, logs_dir):\n",
    "        self.logs_dir = logs_dir\n",
    "    \n",
    "    def setModelsDir(self, models_dir):\n",
    "        self.models_dir = models_dir\n",
    "\n",
    "def setup_spark(file):\n",
    "    \"\"\"\n",
    "    Setup spark config and server and load datafile\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file: string\n",
    "        File to be loaded\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark.close()\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        conf = SparkConf().setAppName('Spark Project')\n",
    "        conf.set('spark.driver.memory', '6g')\n",
    "        conf.set('spark.executor.memory', '5g')\n",
    "        conf.set('spark.driver.maxResultSize','2g')\n",
    "        conf.set('sql.autoBroadcastJoinThreshold','-1')\n",
    "        #conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "        #conf.set(\"spark.kryoserializer.buffer.max\", \"256m\")\n",
    "        conf.set('spark.ui.enabled', 'true')\n",
    "        conf.set('spark.ui.killEnabled', 'false')\n",
    "        conf.set(\"spark.memory.offHeap.enabled\",'true')\n",
    "        conf.set(\"spark.memory.offHeap.size\",\"4g\") \n",
    "        #conf.set('spark.serializer.objectStreamReset', '100')\n",
    "        #conf.set('spark.driver.cores', '1')\n",
    "        #conf.set('spark.executor.id', 'driver')\n",
    "        #conf.set('spark.executor.cores', '1')\n",
    "        #conf.set('spark.ui.showConsoleProgress', 'true')\n",
    "        #conf.set('spark.executor.instances', '1')\n",
    "        #conf.set('spark.rdd.compress', 'True')\n",
    "        sc = SparkContext(conf=conf)\n",
    "        \n",
    "        spark = SparkSession.builder \\\n",
    "        .appName(\"Spark Project\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "        spark.sparkContext.setLogLevel(\"WARN\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    df = spark.read.load(file,format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "    return df, sc, spark\n",
    "\n",
    "def setup_variables(df, sc, colLabel, colRem):\n",
    "    \"\"\"\n",
    "    Creates the column items for category and numerical values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Dataframe\n",
    "        Spark Dataframe\n",
    "    sc: SparkContext object\n",
    "        SparkContext object\n",
    "    colLabel: List\n",
    "        Items that considered Label\n",
    "    colRem: List\n",
    "        Items that should be removed before analysis\n",
    "    logs_dir: string\n",
    "        Directory for log file\n",
    "    Return\n",
    "    ----------\n",
    "    df: Dataframe\n",
    "        Spark Dataframe\n",
    "    colCat: List\n",
    "        Items that is considered categories\n",
    "    colNum: List\n",
    "        Items that is considered numerical values\n",
    "    \"\"\"\n",
    "    # Dropping data that cant help during model\n",
    "    df = df.drop(*colRem)\n",
    "\n",
    "    # Convert boolean to string since PCA cant handle boolean which should be a class\n",
    "    df = df.select(*[col(c[0]).cast(\"string\").alias(c[0]) if c[1] == 'boolean' else col(c[0]).alias(c[0]) for c in df.dtypes])\n",
    "\n",
    "    #renamedHousing.select([count(when(col(c).isNull(), c)).alias(c) for c in colNum]).show()\n",
    "    colCat, colNum = createNewClasses(df, sc, colLabel)\n",
    "\n",
    "    return df, colCat, colNum\n",
    "\n",
    "def createNewClasses(df, sc, colLabel):\n",
    "    \"\"\"\n",
    "    Divide the data into classes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Dataframe\n",
    "        Spark Dataframe\n",
    "    sc: SparkContext object\n",
    "        SparkContext object\n",
    "    colLabel: List\n",
    "        Items that considered Label\n",
    "        \n",
    "    logs_dir: string\n",
    "        Directory for log file\n",
    "        \n",
    "    Return\n",
    "    ----------\n",
    "    colCat: List\n",
    "        Items that is considered categories\n",
    "    colNum: List\n",
    "        Items that is considered numerical values\n",
    "    \"\"\"\n",
    "    rdd = sc.parallelize(df.dtypes)\n",
    "    \n",
    "    colCat = rdd.map(lambda i: i[0] if (i[1]=='string' or i[1]=='boolean' and i[0] not in colLabel) else None).filter(lambda i: i != None).collect()\n",
    "    colNum = rdd.map(lambda i: i[0] if (i[1]=='double' and i[0] not in colLabel) else None).filter(lambda i: i != None).collect()\n",
    "    \n",
    "    print(f\"Label: {colLabel} \\nCategories: {colCat}\\nNumerical: {colNum}\")\n",
    "    return colCat, colNum \n",
    "\n",
    "def printMissingValues(df, logger):\n",
    "    \"\"\"\n",
    "    Print missing values in Dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Dataframe\n",
    "        Spark Dataframe\n",
    "    logs_dir: string\n",
    "        Directory for log file\n",
    "        \n",
    "    Return\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    # Check data\n",
    "    df_missingVals_cols = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "\n",
    "    # Missing value in each column\n",
    "    tmp = df_missingVals_cols.collect()\n",
    "    logger.write2file(\"Missing values\", str(tmp))\n",
    "    print(\"Missing values\" + str(tmp))\n",
    "    \n",
    "def printCategoricalValues(df, colCat, logger):\n",
    "    \"\"\"\n",
    "    Print the count of unique values from categories\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Dataframe\n",
    "        Spark Dataframe\n",
    "    colCat: List\n",
    "        Items that is considered categories\n",
    "    logs_dir: string\n",
    "        Directory for log file\n",
    "        \n",
    "    Return\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    # Checking how many classes that can be used\n",
    "    tmp = [df.select(countDistinct(c).alias(c)).collect()[0] for c in colCat] \n",
    "    print(\"Unique column values:\", tmp)\n",
    "\n",
    "    logger.write2file(\"Categorical values\", str(tmp))\n",
    "\n",
    "def evaluateModel(estimator, \n",
    "                  paramGrid, \n",
    "                  modelType, \n",
    "                  trainSet, \n",
    "                  testSet, \n",
    "                  evaluator, \n",
    "                  k=10,\n",
    "                  seed=None,\n",
    "                  logger=None):\n",
    "    \"\"\"\n",
    "    Evaluate model through CrossValidator, save model, and check accuracy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator: Obj\n",
    "        Algorithm used for fitting model\n",
    "    paramGrid: paramGrid\n",
    "        Params that will be tested and evaluated\n",
    "    modelType: string\n",
    "        Name of the model\n",
    "    trainSet: Dataframe\n",
    "        Training set\n",
    "    testSet: Dataframe\n",
    "        Test set\n",
    "    timeSignature: string\n",
    "        Time when program started\n",
    "    evaluator: Obj, Optional, Default: MulticlassClassificationEvaluator\n",
    "        Evaluator\n",
    "    k: int\n",
    "        Number of folds\n",
    "    seed:\n",
    "        Seed\n",
    "    logs_dir: string\n",
    "        Directory for log file\n",
    "    models_dir:\n",
    "        Directory for the model\n",
    "    Return\n",
    "    ----------\n",
    "    model: Obj\n",
    "        Model that is fitted\n",
    "    predictions: obj\n",
    "        Predictions that is transformed based on the model and testSet\n",
    "    \"\"\"\n",
    "    cv =  CrossValidator(estimator=estimator, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=k, seed=seed) \n",
    "    \n",
    "    print(\"Fitting model..\")\n",
    "    logger.write2file(\"Fitting \"+str(modelType)+\" model..\",\"\")\n",
    "    \n",
    "    model = cv.fit(trainSet)\n",
    "    \n",
    "    print(\"Predicting on testSet..\")\n",
    "    logger.write2file(\"Predicting \"+str(modelType)+\" on testSet..\",\"\")\n",
    "    \n",
    "    predictions = model.transform(testSet)\n",
    "    samplePredict = predictions.select(\"prediction\", \"label\", \"features\")\n",
    "    \n",
    "    print(\"Evaluating predictions..\")\n",
    "    logger.write2file(\"Evaluating \"+ str(modelType) +\" predictions..\",\"\")\n",
    "    \n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    params = model.getEstimatorParamMaps()[argmax(model.avgMetrics)]\n",
    "    \n",
    "    logger.write2file(\"Evaluating \"+ str(modelType),\"Display 5 datapoints:\\n\" + str(samplePredict.take(5)) + \"\\nAccuracy: \" + str(accuracy)+\"\\nParameters:\\n\" + str(params))\n",
    "    logger.write2fileModel(model, str(modelType))\n",
    "    samplePredict.show(5)\n",
    "    print(\"Accuracy: \",accuracy, \"\\nParameters\\n\",params)\n",
    "\n",
    "    y_true = predictions.select(['label']).collect()\n",
    "    y_pred = predictions.select(['prediction']).collect()\n",
    "    try:\n",
    "        print(f\"Classification report:\\n{classification_report(y_true, y_pred)}\")\n",
    "        print(f\"Confusion matrix:\\n{confusion_matrix(y_true, y_pred)}\")\n",
    "    except:\n",
    "        print('The cloud does not support Classification report or confusion matrix')\n",
    "   \n",
    "    return model, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler,OneHotEncoderEstimator,QuantileDiscretizer, StringIndexer, Imputer,StandardScaler,MinMaxScaler\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import ChiSqSelector,PCA\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier,RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=spark://jkg-deployment-af72c332-d42c-4184-a6f4-b0743b52ba66-7f657fn9cs9:7077) created by getOrCreate at /opt/ibm/kernelScript/python/shell.py:65 \n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'spark' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fe5776999281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-3b45ba76f899>\u001b[0m in \u001b[0;36msetup_spark\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'spark' referenced before assignment"
     ]
    }
   ],
   "source": [
    "model_indexing_dir = \"log_index.md\"\n",
    "images_dir = \"analysis\"\n",
    "models_dir = \"models\"\n",
    "logs_dir = \"models\" \n",
    "filter_state = 'CA' # ALL if all states\n",
    "model_Note = ''\n",
    "log_mode=False # Logs data to files, Dont use on massive dataset (debug_mode is recommended while on)\n",
    "debug_mode=False # Reduce number of rows collected for faster processing\n",
    "enable_plots = True # Dont use unless debug_mode is on, or datasize is small\n",
    "logger = logging(models_dir, logs_dir, \"\", filter_state, enabled=log_mode)\n",
    "\n",
    "file = \"data/US_Accidents_Dec19.csv\"\n",
    "\n",
    "\n",
    "df, sc,spark = setup_spark(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colLabel = [\"Severity\"]\n",
    "\n",
    "colRem = ['ID', \n",
    "          'Source',\n",
    "          'End_Time',\n",
    "          'End_Lat',\n",
    "          'End_Lng',\n",
    "          'Description',\n",
    "        ]\n",
    "\n",
    "df, colCat, colNum = setup_variables(df, sc, colLabel, colRem)  \n",
    "\n",
    "logger.write2file(\"Number of rows\", str(df.count()))\n",
    "logger.write2file(\"Categorical groups\",\"Defined Label:\\n\" + str(colLabel) + \"\\nDefined Categories:\\n\" + str(colCat) + \"\\nDefined Numerical:\\n\" +str(colNum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sc._conf.getAll()\n",
    "logger.write2file(\"New Spark session\", str(tmp))\n",
    "print(\"Config:\",tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Categories:\\n \\\n",
    "Labels: {len(colLabel)}\\n \\\n",
    "Classes: {len(colCat)}\\n \\\n",
    "Removed: {len(colRem)}\\n \\\n",
    "Numerical: {len(colNum)}\") \n",
    "\n",
    "info = f\"Rows: {df.count()}\\nColumns {len(df.columns)}\"\n",
    "print(info)\n",
    "df.printSchema()\n",
    "df.take(1)\n",
    "\n",
    "logger.write2file(\"Data analysis\",info +\"\\n\"+ str(df._jdf.schema().treeString()) + str(df.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(colNum).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMissingValues(df, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.persist()\n",
    "tmp = [df.select(countDistinct(c).alias(c)).collect()[0] for c in [*colCat]] \n",
    "df.unpersist()\n",
    "print(\"Unique column values:\", tmp)\n",
    "\n",
    "logger.write2file(\"Unique column values\", str(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([*colNum, *colLabel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=colNum,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid = \"skip\")\n",
    "\n",
    "df_attributes = assembler.transform(df)\n",
    "df_attributes.select(\"features\").show(1,False)\n",
    "\n",
    "\n",
    "\n",
    "r1 = Correlation.corr(df_attributes, \"features\").head()\n",
    "\n",
    "print(\"correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "logger.write2file(\"Correlation matrix\", str(r1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_plots == True:\n",
    "    sns.set(style=\"white\")\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    b = sns.heatmap(r1[0].toArray().tolist(), annot=True, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5},xticklabels=colNum,yticklabels=colNum,ax=ax )\n",
    "    ax.set_title(\"Correlation between numerical features\")\n",
    "\n",
    "    logger.saveImage(b,\"feature_corr_matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_freq = df.groupBy('Severity').count().orderBy('count',ascending=False)\n",
    "severity_freq.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_plots == True:\n",
    "    pd_severity = severity_freq.toPandas()\n",
    "\n",
    "    # Plot data\n",
    "    fig,ax = plt.subplots(figsize=(16,10))\n",
    "    b = sns.barplot(pd_severity['Severity'],pd_severity['count'], color='blue')\n",
    "    b.axes.set_title(\"Severity distribution\",fontsize=20)\n",
    "    b.set_xlabel(\"Number of Accidents\",fontsize=15)\n",
    "    b.set_ylabel(\"Severity\",fontsize=15)\n",
    "    b.tick_params(labelsize=10)\n",
    "\n",
    "    logger.saveImage(b,\"severity_dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_plots == True:\n",
    "    df_sev = df.groupby('Severity').count().toPandas()\n",
    "    fig = df_sev.plot.pie(y='count', labels=df_sev['Severity'], figsize=(10, 10), autopct='%1.0f%%',title=\"Pie plot - Severity distribution\",fontsize=15)\n",
    "    logger.saveImage(fig,\"pie_severity_dist\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of severity and state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_freq = df.groupBy('State').count().orderBy('count',ascending=False)\n",
    "state_freq.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_plots == True:\n",
    "    pd_states = state_freq.toPandas()\n",
    "\n",
    "    # Plot data\n",
    "    fig,ax = plt.subplots(figsize=(16,10))\n",
    "    b = sns.barplot(pd_states['State'],pd_states['count'], color='blue')\n",
    "    b.axes.set_title(\"Severity distribution for each state\",fontsize=20)\n",
    "    b.set_xlabel(\"Number of Accidents\",fontsize=15)\n",
    "    b.set_ylabel(\"State\",fontsize=15)\n",
    "    b.tick_params(labelsize=10)\n",
    "\n",
    "    logger.saveImage(b,\"severity_dist_state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_severity_freq = df.groupBy('State','Severity').count().orderBy('count',ascending=False)\n",
    "state_severity_freq.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_plots == True:\n",
    "    pd_state_severity = state_severity_freq.toPandas()\n",
    "\n",
    "    # Plot data\n",
    "    fig,ax = plt.subplots(figsize=(16,10))\n",
    "    b = sns.barplot(x=\"State\", y=\"count\", hue=\"Severity\", data=pd_state_severity)\n",
    "    #b = sns.barplot(pd_state_severity['State', 'Severity'],pd_state_severity['count'])\n",
    "    b.axes.set_title(\"Severity distribution for each state\",fontsize=20)\n",
    "    b.set_xlabel(\"Number of Accidents\",fontsize=15)\n",
    "    b.set_ylabel(\"State\",fontsize=15)\n",
    "    b.tick_params(labelsize=10)\n",
    "\n",
    "    logger.saveImage(b,\"severity_dist_class_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_freq = df.groupBy('Weather_Condition').count().orderBy('count',ascending=False)\n",
    "weather_freq.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = int(df.count()*0.001) # Limit the plot to ignore conditions below an limit\n",
    "\n",
    "rdd_filtered = weather_freq.rdd.filter(lambda x: x['count'] > n)\n",
    "print(rdd_filtered.collect())\n",
    "\n",
    "if enable_plots == True:\n",
    "    fig, ax=plt.subplots(figsize=(16,25))\n",
    "    pd_weather = rdd_filtered.toDF().toPandas()\n",
    "\n",
    "    b = sns.barplot(pd_weather['count'][:],pd_weather['Weather_Condition'][:], color=\"blue\")\n",
    "\n",
    "    b.axes.set_title(\"Weather Condition for accidents above 1% of total set\",fontsize=20)\n",
    "    b.set_xlabel(\"Number of Accidents\",fontsize=15)\n",
    "    b.set_ylabel(\"Weather_Condition\",fontsize=15)\n",
    "    b.tick_params(labelsize=10)\n",
    "\n",
    "    logger.saveImage(b,\"weather_cond_dist\")\n",
    "    logger.write2file(\"Weather condition distribution\", str(pd_weather))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time when accidents occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = df.selectExpr(\"hour(to_timestamp(from_utc_timestamp(Start_Time, Timezone), 'yyyy-MM-dd HH:mm:ss')) as Start_Time\")\n",
    "time_freq = df_time.groupBy('Start_Time').count().orderBy('count',ascending=False)\n",
    "time_freq.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_plots == True:\n",
    "    pd_time = time_freq.toPandas()\n",
    " \n",
    "    # Plot data\n",
    "    fig,ax = plt.subplots(figsize=(16,10))\n",
    "\n",
    "    b = sns.barplot(pd_time['Start_Time'],pd_time['count'], color='blue')\n",
    "    b.axes.set_title(\"Daytime for accidents\",fontsize=20)\n",
    "    b.set_xlabel(\"Time (Hours)\",fontsize=15)\n",
    "    b.set_ylabel(\"Number of Accidents\",fontsize=15)\n",
    "    b.tick_params(labelsize=10)\n",
    "\n",
    "    logger.saveImage(b,\"accident_hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development of accidents on Month basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = df.selectExpr(\"month(to_timestamp(from_utc_timestamp(Start_Time, Timezone), 'yyyy-MM-dd HH:mm:ss')) as Start_Time\")\n",
    "time_freq = df_time.groupBy('Start_Time').count().orderBy('count',ascending=False)\n",
    "time_freq.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_plots == True:\n",
    "    pd_time = time_freq.toPandas()\n",
    "\n",
    "    # Plot data\n",
    "    fig,ax = plt.subplots(figsize=(16,10))\n",
    "\n",
    "    b = sns.barplot(pd_time['Start_Time'],pd_time['count'], color='blue')\n",
    "    b.axes.set_title(\"Montly distribution for accidents\",fontsize=20)\n",
    "    b.set_xlabel(\"Time (Month)\",fontsize=15)\n",
    "    b.set_ylabel(\"Number of Accidents\",fontsize=15)\n",
    "    b.tick_params(labelsize=10)\n",
    "\n",
    "    logger.saveImage(b,\"accudents_months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_plots == True:\n",
    "    b = sns.jointplot(x=df.select(collect_list('Start_Lat')).first()[0],y=df.select(collect_list('Start_Lng')).first()[0],height=10)\n",
    "\n",
    "    b.set_axis_labels('Start_Lat','Start_Lng')\n",
    "    b.fig.suptitle(\"Map distribution of accidents\")\n",
    "    logger.saveImage(b,\"accident_map_dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.write2file(\"New Analysis created!\", \\\n",
    "               \"Model finished: Yes\" + \\\n",
    "               \"\\nFolder name: \" + logger.timeSignature + \\\n",
    "               \"\\nState: \"+ \"Not defined\" + \\\n",
    "               \"\\nLogs directory: \" + logger.logs_dir +  \\\n",
    "               \"\\nFile: \" + file + \\\n",
    "               \"\\nNote: \" + model_Note, \\\n",
    "               logs_dir=model_indexing_dir \\\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse single state\n",
    "#This could be CA since it stands for a little less than 50% of the total set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_state != 'ALL':\n",
    "    df = df.filter(df.State == filter_state) # Lowers the dataset quite a lot\n",
    "    logger.write2file(\"Specified state\",str(filter_state))\n",
    "else:\n",
    "    logger.write2file(\"No state specified\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify time\n",
    "#Convert the timestamp into a numeric value and then into a string so that the time of day and month can be categorised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to int then cast to string\n",
    "\n",
    "df = df.withColumn('Start_Hour', hour(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "df = df.withColumn('Start_Month', month(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "df = df.withColumn('Weather_Hour', hour(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "df = df.withColumn('Weather_Month', month(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "df.show(1)\n",
    "logger.write2file(\"Dataset after modifying UTC timestamp\", str(df.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster quantiles to reduce the feature vector for position\n",
    "#Since feature crossing wont work due to the feature vector is to big (regarding to the given resources) we hope that Clustering might reduce them and keep as much data as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters= 400\n",
    "pos_assembler = VectorAssembler(inputCols=['Start_Lat', 'Start_Lng'], outputCol=\"position_features\")\n",
    "preprocessed_cord = pos_assembler.transform(df)\n",
    "\n",
    "trainSet, testSet = preprocessed_cord.randomSplit([0.9 ,0.1], 1)\n",
    "kmeans = KMeans(k=number_of_clusters, featuresCol='position_features', predictionCol='position')\n",
    "model = kmeans.fit(trainSet)\n",
    "predictions = model.transform(testSet)\n",
    "\n",
    "evaluator = ClusteringEvaluator(featuresCol='position_features', predictionCol='position')\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Transform the hole dataset\n",
    "df = model.transform(preprocessed_cord)\n",
    "df.select('position').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(1)\n",
    "logger.write2file(\"Dataset after adding quantiles\",str(df.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast data \n",
    "#Cast all the datacolumns into correct format so they will be sorted to numerical or categorical values. Data removed caused to big feature vector, contained too #many NaN values or were converted into another form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could be handles as separated lists and then sent to a UDF for processing but since low number of features and easier to visualize this select is kept.\n",
    "df = df.select(\n",
    "        col('Severity').cast('int'),\n",
    "        col('Start_Hour').cast('string'),\n",
    "        col('Start_Month').cast('string'),\n",
    "        col('Weather_Hour').cast('string'),\n",
    "        col('Weather_Month').cast('string'),\n",
    "        col('position').cast('string'),\n",
    "        col('Distance(mi)').cast('double'),\n",
    "        col('Side').cast('string'),   \n",
    "        col('Temperature(F)').cast('double'),\n",
    "        col('Wind_Chill(F)').cast('double'), \n",
    "        col('Humidity(%)').cast('double'),\n",
    "        col('Pressure(in)').cast('double'),\n",
    "        col('Visibility(mi)').cast('double'),\n",
    "        col('Wind_Direction').cast('string'),\n",
    "        col('Wind_Speed(mph)').cast('double'), \n",
    "        col('Weather_Condition').cast('string'),\n",
    "        col('Amenity').cast('string'),\n",
    "        col('Bump').cast('string'),\n",
    "        col('Crossing').cast('string'),\n",
    "        col('Give_Way').cast('string'),\n",
    "        col('Junction').cast('string'),\n",
    "        col('No_Exit').cast('string'),\n",
    "        col('Railway').cast('string'),\n",
    "        col('Roundabout').cast('string'),\n",
    "        col('Station').cast('string'),\n",
    "        col('Stop').cast('string'),\n",
    "        col('Traffic_Calming').cast('string'),\n",
    "        col('Traffic_Signal').cast('string'),\n",
    "        col('Turning_Loop').cast('string'),\n",
    "        col('Sunrise_Sunset').cast('string'),\n",
    "        col('Civil_Twilight').cast('string'),\n",
    "        col('Nautical_Twilight').cast('string'),\n",
    "        col('Astronomical_Twilight').cast('string')\n",
    "    ) \n",
    "\n",
    "colLabel = [\"Severity\"]\n",
    "\n",
    "colCat, colNum = createNewClasses(df, sc, colLabel)\n",
    "logger.write2file(\"Categorical groups\",\"Defined Label:\\n\" + str(colLabel) + \"\\nDefined Categories:\\n\" + str(colCat) + \"\\nDefined Numerical:\\n\" +str(colNum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.persist()\n",
    "tmp = [df.select(countDistinct(c).alias(c)).collect()[0] for c in [*colCat]] \n",
    "df.unpersist()\n",
    "print(\"Unique column values:\", tmp)\n",
    "\n",
    "logger.write2file(\"Unique column values\", str(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recheck the missing values\n",
    "#Check so that the output contains 0 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "printMissingValues(df,logger)\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove data with occurance less than 1%\n",
    "#Based on information from analysis. With further analysis lower procentage can be used to find better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.persist()\n",
    "original_rows = df.count()\n",
    "n = int(df.count()*0.001) # Limit the plot to ignore conditions below an limit\n",
    "\n",
    "weather_freq = df.groupBy('Weather_Condition').count().orderBy('count',ascending=False)\n",
    "df_filtered = weather_freq.filter(weather_freq['count'] > n)\n",
    "filtered_conditions = df_filtered.select(\"Weather_Condition\").rdd.flatMap(lambda x: x).collect()\n",
    "df = df.filter(df['Weather_Condition'].isin(*filtered_conditions))\n",
    "\n",
    "df_filtered.show()\n",
    "removed_rows = original_rows - df.count()\n",
    "\n",
    "df.unpersist()\n",
    "print(\"Rows removed:\",removed_rows)\n",
    "\n",
    "logger.write2file(\"Weather condition\", str(df_filtered.take(df_filtered.count())) + \"\\nRows removed: \" + str(removed_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove if distict values in category equals 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.persist()\n",
    "tmp = [c for c in [*colCat] if df.select(countDistinct(c)).collect()[0][0] <= 1] \n",
    "df.unpersist()\n",
    "\n",
    "[colCat.remove(c) for c in tmp]\n",
    "print(\"Dropping columns with 1 class:\", tmp)\n",
    "logger.write2file(\"Dropping columns with 1 class\", str(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(inputCols=colNum, outputCols=colNum)\n",
    "imputer.setStrategy(\"median\")\n",
    "\n",
    "num_assembler = VectorAssembler(inputCols=colNum, outputCol=\"num_features\")\n",
    "scaler = MinMaxScaler(min=0.0, max=1.0, inputCol=\"num_features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "indexers = [StringIndexer(inputCol = c, outputCol = c +'_IDX', handleInvalid='skip') for c in colCat]\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[indexer.getOutputCol() for indexer in indexers], outputCols=[\"{0}_vec\".format(indexer.getOutputCol()) for indexer in indexers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical values\n",
    "\n",
    "numPipeline = Pipeline(stages=[imputer, num_assembler, scaler])\n",
    "catPipeline = Pipeline(stages=[*indexers, encoder])\n",
    "\n",
    "pipeline = Pipeline(stages=[numPipeline, catPipeline])\n",
    "\n",
    "preprocessed_df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df.select(\"scaledFeatures\",*[c + \"_IDX_vec\" for c in colCat]).show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va2 = VectorAssembler(inputCols=[\"scaledFeatures\", *[c + \"_IDX_vec\" for c in colCat]], outputCol=\"final_features\")\n",
    "\n",
    "df = va2.transform(preprocessed_df)\n",
    "df.persist()\n",
    "df = df.withColumn('label', col(\"Severity\"))\n",
    "df = df.withColumn('features', df.final_features).select(\"features\",\"label\")\n",
    "df.unpersist()\n",
    "\n",
    "\n",
    "logger.write2fileModel(df, \"df_features\")\n",
    "\n",
    "logger.write2file(\"Feature set size\", str(df.count()) + \"\\n\\n__Feature vector and label:__\\n\" + str(df.take(1)))\n",
    "logger.write2file(\"Number of rows\", str(df.count()))\n",
    "print(\"Feature set size: \",df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how many classes that can be used\n",
    "df.persist()\n",
    "tmp = [df.select(countDistinct(c).alias(c)).collect()[0] for c in [*colCat, \"position\"]] \n",
    "df.unpersist()\n",
    "print(\"Unique column values:\", tmp)\n",
    "\n",
    "#logger.write2file(\"Unique column values\", str(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "#After editing Quantilization PCA started to crash by giving error: spark.driver.maxResultSize\n",
    "#In order to understand how much the variance affect the dataset we check with PCA. Try to get above 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_feature_vec = 250\n",
    "pca = PCA(k=length_of_feature_vec, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df)\n",
    "pca_df = pca_model.transform(df)\n",
    "\n",
    "\n",
    "print(\"PCA - Feature Variance:\",\"Top 50:\\n\" + str(pca_model.explainedVariance[:50]) + \"\\nNumber of items: \"+str(k)+\"\\nSum of variance: \"+ str(sum(pca_model.explainedVariance)))\n",
    "logger.write2file(\"PCA - Feature variance\", \"Top 50:\\n\" + str(pca_model.explainedVariance[:50]) + \"\\nNumber of items: \"+ str(k) +\"\\nSum of variance: \"+ str(sum(pca_model.explainedVariance)))\n",
    "logger.write2fileModel(pca_df, \"pca_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChiSqSelector\n",
    "#Check top 100 which of the values in the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ChiSqSelector(numTopFeatures=length_of_feature_vec, \n",
    "                         labelCol='label', \n",
    "                         featuresCol='features', \n",
    "                         outputCol=\"selectedFeatures\",\n",
    "                         selectorType='numTopFeatures', \n",
    "                         percentile=0.1, \n",
    "                         fpr=0.05, fdr=0.05, fwe=0.05)\n",
    "\n",
    "chi_model = selector.fit(df)\n",
    "chi_df = chi_model.transform(df)\n",
    "\n",
    "logger.write2file(\"Top selected features according to ChiSqSelector\", str(chi_model.selectedFeatures)+ \"\\nNumber of features: \" + str(k) + \"\\nExample data:\\n\"+str(chi_df.take(5)))\n",
    "logger.write2fileModel(chi_df, \"chi_df\")\n",
    "\n",
    "print(\"Top selected features according to ChiSqSelector:\", chi_model.selectedFeatures)\n",
    "print(\"Transformed selected features:\",chi_df.head().selectedFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, testSet = chi_df.randomSplit([0.8 ,0.2], 1)\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet.show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"selectedFeatures\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.1,0.01]) \\\n",
    "                .addGrid(lr.maxIter, [10]) \\\n",
    "                .addGrid(lr.elasticNetParam, [0.6]) \\\n",
    "                .build()\n",
    "try: \n",
    "    model, _ = evaluateModel(lr, paramGrid, \"LR_Model\", trainSet, testSet, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logger=logger)\n",
    "except Exception as e:\n",
    "    logger.write2file(\"Error:\", str(e))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"selectedFeatures\") \n",
    "\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "_, _ = evaluateModel(dt, paramGrid, \"DT_Model\", trainSet, testSet, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"selectedFeatures\") #numTrees=10\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(rf.numTrees,[15]) \\\n",
    "                .build()\n",
    "\n",
    "_, _ = evaluateModel(rf, paramGrid, \"RF_Model\", trainSet, testSet, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.write2file(\"Program finished!\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.write2file(\"New model folder created\", \\\n",
    "               \"Model finished: Yes\" + \\\n",
    "               \"\\nFolder name: \" + logger.timeSignature + \\\n",
    "               \"\\nState: \"+ filter_state + \\\n",
    "               \"\\nLogs directory: \" + logger.logs_dir +  \\\n",
    "               \"\\nFile: \" + file + \\\n",
    "               \"\\nNote: \" + model_Note, \\\n",
    "               logs_dir=model_indexing_dir \\\n",
    "              )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
