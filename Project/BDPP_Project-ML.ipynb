{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport ibmos2spark\\n# @hidden_cell\\ncredentials = {\\n    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\\n    'service_id': 'iam-ServiceId-1d71bac5-a341-449f-9ca7-b29de95e771a',\\n    'iam_service_endpoint': 'https://iam.eu-gb.bluemix.net/oidc/token',\\n    'api_key': 'ejujKveBRS3Bk7l3pzjemEQDhTSaOKmqCg6x6osXWkx4'\\n}\\n\\nconfiguration_name = 'os_2d970470a6354234a1716fe9f4db519b_configs'\\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\\n\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder.getOrCreate()\\n\\ndf = spark.read  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')  .option('header', 'true')  .load(cos.url('US_Accidents_Dec19.csv', 'bdppproject-donotdelete-pr-nejualq57kuqe4'))\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-1d71bac5-a341-449f-9ca7-b29de95e771a',\n",
    "    'iam_service_endpoint': 'https://iam.eu-gb.bluemix.net/oidc/token',\n",
    "    'api_key': 'ejujKveBRS3Bk7l3pzjemEQDhTSaOKmqCg6x6osXWkx4'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_2d970470a6354234a1716fe9f4db519b_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('US_Accidents_Dec19.csv', 'bdppproject-donotdelete-pr-nejualq57kuqe4'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "try:\n",
    "    spark.close()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    conf = SparkConf().setAppName('Spark Project')\n",
    "    sc = SparkContext(conf=conf)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "#.config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#SparkContext.setSystemProperty('spark.executor.memory', '8g')\n",
    "#spark = SparkSession \\\n",
    "#    .builder \\\n",
    "#    .appName(\"Spark Project\") \\\n",
    "#    .config(\"spark.executor.memory\", '5g') \\\n",
    "#    .config(\"spark.driver.memory\", '5g') \\\n",
    "#    .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Project\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"70g\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\",True) \\\n",
    "     .config(\"spark.memory.offHeap.size\",\"16g\")  \\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\",200) \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.default.parallelism\", \"6\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.load(\"data/US_Accidents_Dec19.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ae0d30820d9f:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Spark Project>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.memory', '12g'),\n",
       " ('spark.default.parallelism', '6'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.executor.memory', '70g'),\n",
       " ('spark.app.name', 'Spark Project'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
       " ('spark.driver.host', 'ae0d30820d9f'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.port', '44931'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1588194261072'),\n",
       " ('spark.memory.offHeap.enabled', 'True'),\n",
       " ('spark.yarn.executor.memoryOverhead', '200'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.memory.offHeap.size', '16g')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, count, col, isnan, countDistinct,from_unixtime,from_utc_timestamp, unix_timestamp,split, to_timestamp, hour, month, lit,collect_list\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Due to problem in spark 3.0.0\n",
    "#spark.conf.set(\"spark.sql.legacy.utcTimestampFunc.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Street', 'Side', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone', 'Airport_Code', 'Wind_Direction', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
      "['TMC', 'Start_Lat', 'Start_Lng', 'Distance(mi)', 'Number', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)']\n"
     ]
    }
   ],
   "source": [
    "# Define data information\n",
    "\n",
    "# Choose directory\n",
    "#projDir = \"C:\\Users\\fredr\\Documents\\github-projects\\BDPP-Labs\\Project\\Models\"\n",
    "\n",
    "\n",
    "# Numerical values\n",
    "\n",
    "colLabel = [\"Severity\"]\n",
    "\n",
    "colRem = ['ID', \n",
    "          'Source',\n",
    "          'End_Time',\n",
    "          'End_Lat',\n",
    "          'End_Lng',\n",
    "          'Description',\n",
    "        ]\n",
    "\n",
    "# Dropping data that cant help during model\n",
    "df = df.drop(*colRem)\n",
    "\n",
    "# Convert boolean to string since PCA cant handle boolean which should be a class\n",
    "df = df.select(*[col(c[0]).cast(\"string\").alias(c[0]) if c[1] == 'boolean' else col(c[0]).alias(c[0]) for c in df.dtypes])\n",
    "\n",
    "#renamedHousing.select([count(when(col(c).isNull(), c)).alias(c) for c in colNum]).show()\n",
    "rdd = sc.parallelize(df.dtypes)\n",
    "\n",
    "colCat = rdd.map(lambda i: i[0] if (i[1]=='string' or i[1]=='boolean' and i[0]) else None).filter(lambda i: i != None).collect()\n",
    "colNum = rdd.map(lambda i: i[0] if (i[1]=='double' and i[0]) else None).filter(lambda i: i != None).collect()\n",
    "\n",
    "print(colCat)\n",
    "print(colNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse single city\n",
    "This could be CA since it stands for a little less than 50% of the total set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.State == \"CA\") # Lowers the dataset quite a lot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify time\n",
    "Convert the timestamp into a numeric value and then into a string so that the time of day and month can be categorised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+---------+-----------+------------+------+-----------+----+-------+------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "|  TMC|Severity|         Start_Time|Start_Lat|  Start_Lng|Distance(mi)|Number|     Street|Side|   City|County|State|Zipcode|Country|  Timezone|Airport_Code|  Weather_Timestamp|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Amenity| Bump|Crossing|Give_Way|Junction|No_Exit|Railway|Roundabout|Station| Stop|Traffic_Calming|Traffic_Signal|Turning_Loop|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|Start_Hour|Start_Month|Weather_Hour|Weather_Month|\n",
      "+-----+--------+-------------------+---------+-----------+------------+------+-----------+----+-------+------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "|201.0|       3|2016-06-21 10:34:40|  38.0853|-122.233017|         0.0|  null|Magazine St|   R|Vallejo|Solano|   CA|  94591|     US|US/Pacific|        KAPC|2016-06-21 10:54:00|          75.0|         null|       48.0|        30.0|          10.0|      Variable|            5.8|             null|            Clear|  false|false|   false|   false|   false|  false|  false|     false|  false|false|          false|         false|       false|           Day|           Day|              Day|                  Day|         3|          6|           3|            6|\n",
      "+-----+--------+-------------------+---------+-----------+------------+------+-----------+----+-------+------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to int then cast to string\n",
    "\n",
    "df = df.withColumn('Start_Hour', hour(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "df = df.withColumn('Start_Month', month(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "df = df.withColumn('Weather_Hour', hour(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "df = df.withColumn('Weather_Month', month(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast data\n",
    "Cast all the datacolumns into correct format so they will be sorted to numerical or categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from string to integer values\n",
    "# TODO: Does not work since data contains Nan/Null?\n",
    "df = df.select(\n",
    "        col('TMC').cast('string'),\n",
    "        col('Severity').cast('int'),\n",
    "        #col('Start_Time').cast('int'),\n",
    "        col('Start_Hour').cast('string'),\n",
    "        col('Start_Month').cast('string'),\n",
    "        col('Weather_Hour').cast('string'),\n",
    "        col('Weather_Month').cast('string'),\n",
    "        #col('Start_Lat').cast('string'),\n",
    "        #col('Start_Lng').cast('string'),\n",
    "        col('Distance(mi)').cast('double'),\n",
    "        col('Street').cast('string'),       # Remove eventually?\n",
    "        col('Side').cast('string'),         # Remove eventually?\n",
    "        col('City').cast('string'),         # Remove eventually?\n",
    "        col('State').cast('string'),        # Remove eventually?\n",
    "        #col('Zipcode').cast('string'),      # Remove eventually?\n",
    "        #col('Country').cast('string'),      # Remove eventually?\n",
    "        #col('Airport_Code').cast('string'), # Remove eventually?\n",
    "      #  col('Weather_Timestamp').cast('int'),\n",
    "        col('Temperature(F)').cast('double'),\n",
    "        col('Wind_Chill(F)').cast('double'),\n",
    "        col('Humidity(%)').cast('double'),\n",
    "        col('Pressure(in)').cast('double'),\n",
    "        col('Visibility(mi)').cast('double'),\n",
    "        col('Wind_Direction').cast('string'),\n",
    "        col('Wind_Speed(mph)').cast('double'),\n",
    "        col('Weather_Condition').cast('string'),\n",
    "        col('Amenity').cast('string'),\n",
    "        col('Bump').cast('string'),\n",
    "        col('Crossing').cast('string'),\n",
    "        col('Give_Way').cast('string'),\n",
    "        col('Junction').cast('string'),\n",
    "        col('No_Exit').cast('string'),\n",
    "        col('Railway').cast('string'),\n",
    "        col('Roundabout').cast('string'),\n",
    "        col('Station').cast('string'),\n",
    "        col('Stop').cast('string'),\n",
    "        col('Traffic_Calming').cast('string'),\n",
    "        col('Traffic_Signal').cast('string'),\n",
    "        col('Turning_Loop').cast('string'),\n",
    "        col('Sunrise_Sunset').cast('string'),\n",
    "        col('Civil_Twilight').cast('string'),\n",
    "        col('Nautical_Twilight').cast('string'),\n",
    "        col('Astronomical_Twilight').cast('string')\n",
    "    ) \n",
    "\n",
    "# Since the last check can be unorganised we recreate a new list that contains all data\n",
    "colLabel = [\"Severity\"]\n",
    "\n",
    "rdd = sc.parallelize(df.dtypes)\n",
    "colCat = rdd.map(lambda i: i[0] if (i[1]=='string' or i[1]=='boolean' and i[0] not in colLabel) else None).filter(lambda i: i != None).collect()\n",
    "colNum = rdd.map(lambda i: i[0] if (i[1]=='double' and i[0] not in colLabel) else None).filter(lambda i: i != None).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Severity']\n",
      "['TMC', 'Start_Hour', 'Start_Month', 'Weather_Hour', 'Weather_Month', 'Street', 'Side', 'City', 'State', 'Wind_Direction', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
      "['Distance(mi)', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)']\n"
     ]
    }
   ],
   "source": [
    "print(colLabel)\n",
    "print(colCat)\n",
    "print(colNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove data containing NaN (Drop)\n",
    "Since NaN and Null occurs we drop these rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89563"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n",
    "df = df.na.drop()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recheck the missing values\n",
    "Check so that the output contains 0 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TMC=0, Severity=0, Start_Hour=0, Start_Month=0, Weather_Hour=0, Weather_Month=0, Distance(mi)=0, Street=0, Side=0, City=0, State=0, Temperature(F)=0, Wind_Chill(F)=0, Humidity(%)=0, Pressure(in)=0, Visibility(mi)=0, Wind_Direction=0, Wind_Speed(mph)=0, Weather_Condition=0, Amenity=0, Bump=0, Crossing=0, Give_Way=0, Junction=0, No_Exit=0, Railway=0, Roundabout=0, Station=0, Stop=0, Traffic_Calming=0, Traffic_Signal=0, Turning_Loop=0, Sunrise_Sunset=0, Civil_Twilight=0, Nautical_Twilight=0, Astronomical_Twilight=0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_missingVals_cols = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "# Missing value in each column\n",
    "df_missingVals_cols.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove data with occurance less than 1%\n",
    "Based on information from analysis. With further analysis lower procentage can be used to find better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   Weather_Condition|count|\n",
      "+--------------------+-----+\n",
      "|                Fair|42320|\n",
      "|              Cloudy|13231|\n",
      "|       Partly Cloudy| 9610|\n",
      "|       Mostly Cloudy| 8372|\n",
      "|               Clear| 5317|\n",
      "|          Light Rain| 2626|\n",
      "|                Haze| 2381|\n",
      "|                 Fog| 1351|\n",
      "|            Overcast| 1128|\n",
      "|                Rain|  833|\n",
      "|        Fair / Windy|  523|\n",
      "|    Scattered Clouds|  463|\n",
      "|          Heavy Rain|  283|\n",
      "|               Smoke|  189|\n",
      "|          Light Snow|  125|\n",
      "|Partly Cloudy / W...|  110|\n",
      "|Mostly Cloudy / W...|   93|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "89563"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_freq = df.groupBy('Weather_Condition').count().orderBy('count',ascending=False)\n",
    "\n",
    "n = int(df.count()*0.001) # Limit the plot to ignore conditions below an limit\n",
    "\n",
    "df_filtered = weather_freq.filter(weather_freq['count'] > n)\n",
    "df_filtered.show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fair', 'Cloudy', 'Partly Cloudy', 'Mostly Cloudy', 'Clear', 'Light Rain', 'Haze', 'Fog', 'Overcast', 'Rain', 'Fair / Windy', 'Scattered Clouds', 'Heavy Rain', 'Smoke', 'Light Snow', 'Partly Cloudy / Windy', 'Mostly Cloudy / Windy']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88955"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_conditions = df_filtered.select(\"Weather_Condition\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "print(filtered_conditions)\n",
    "\n",
    "test1 = df.filter(df['Weather_Condition'].isin(*filtered_conditions))\n",
    "\n",
    "df = test1 # Adding the filtered data to dataframe (Hopefully we lose less than n% of the data) but reduce the dimension of feature vector\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantiles function\n",
    "Alternative solution to get position (Work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndscretizer_Lat = QuantileDiscretizer(numBuckets=50, inputCol=\"Start_Lat\", outputCol=\"Start_Lat\")\\ndscretizer_Lng = QuantileDiscretizer(numBuckets=50, inputCol=\"Start_Lng\", outputCol=\"Start_Lng\")\\n\\n# Convert into categorical values (two blocks)\\nquant_Lat_model = quantiles_Lat.fit(df_features)\\nquant_Lng_model = quantiles_Lng.fit(df_features) \\n\\nquanted_Lat = quant_Lat_model.transform(df_features)\\nquanted_Lng = quant_Lng_model.transform(df_features)\\n\\n# Into categorical values\\nstringIndexer = StringIndexer(inputCol=[\"Start_Lat\",\"Start_Lng\"], outputCol=[\"Start_Lat\",\"Start_Lng\"] + \"_num\")\\nindexer_model = stringIndexer.fit(df_features) # Change into Quanted_Lat, Quanted_Lng\\nindexed = indexer_model.transform(df_features)\\n\\n# One-hot \\nencoder = OneHotEncoder(inputCols=[\"Start_Lat_num\",\"Start_Lng_num\"], outputCols=[s + \"_vec\" for s in colCat])\\n\\nencoded = encoder.fit(indexed)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove City, Country, State, Zipcode, Airport_Code\n",
    "# In order to reduce dimensionality a kernel function will be applied upon the Start_Lat and Start_Lng then fused in order to create a new set of data\n",
    "# Start_Lat\n",
    "# Start_Lng\n",
    "\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\"\"\"\n",
    "dscretizer_Lat = QuantileDiscretizer(numBuckets=50, inputCol=\"Start_Lat\", outputCol=\"Start_Lat\")\n",
    "dscretizer_Lng = QuantileDiscretizer(numBuckets=50, inputCol=\"Start_Lng\", outputCol=\"Start_Lng\")\n",
    "\n",
    "# Convert into categorical values (two blocks)\n",
    "quant_Lat_model = quantiles_Lat.fit(df_features)\n",
    "quant_Lng_model = quantiles_Lng.fit(df_features) \n",
    "\n",
    "quanted_Lat = quant_Lat_model.transform(df_features)\n",
    "quanted_Lng = quant_Lng_model.transform(df_features)\n",
    "\n",
    "# Into categorical values\n",
    "stringIndexer = StringIndexer(inputCol=[\"Start_Lat\",\"Start_Lng\"], outputCol=[\"Start_Lat\",\"Start_Lng\"] + \"_num\")\n",
    "indexer_model = stringIndexer.fit(df_features) # Change into Quanted_Lat, Quanted_Lng\n",
    "indexed = indexer_model.transform(df_features)\n",
    "\n",
    "# One-hot \n",
    "encoder = OneHotEncoder(inputCols=[\"Start_Lat_num\",\"Start_Lng_num\"], outputCols=[s + \"_vec\" for s in colCat])\n",
    "\n",
    "encoded = encoder.fit(indexed)\n",
    "\"\"\"\n",
    "\n",
    "# Do multiplication into a final feature vector (Kernel trick)\n",
    "\n",
    "# Remove City, Country, State, Zipcode, Airport_Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imputer_f98403d7fc3d"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probobly a bad Idea but why not?\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "#imputer = Imputer(inputCols=['Wind_Chill(F)', 'Wind_Speed(mph)'], outputCols=['Wind_Chill(F)', 'Wind_Speed(mph)'])\n",
    "imputer = Imputer(inputCols=colNum, outputCols=colNum)\n",
    "imputer.setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler_ffc797dacdb9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "#Create a vector\n",
    "num_assembler = VectorAssembler(inputCols=colNum, outputCol=\"num_features\",handleInvalid=\"keep\")\n",
    "scaler = StandardScaler(inputCol=\"num_features\", withMean=True, withStd=True, outputCol=\"scaledFeatures\")\n",
    "scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "#colCat\n",
    "#col_assembler = VectorAssembler(inputCols=colCat, outputCol=\"cat_features\",handleInvalid=\"keep\")\n",
    "#indexers = StringIndexer(inputCol=\"cat_features\", outputCol=\"indexedFeatures\",handleInvalid=\"keep\")\n",
    "\n",
    "indexers = [StringIndexer(inputCol = c, outputCol = c +'_IDX', handleInvalid='keep') for c in colCat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "#from pyspark.ml import Pipeline\n",
    "#pipeline = Pipeline(stages=indexers)\n",
    "#df_indexers = pipeline.fit(df).transform(df)\n",
    "\n",
    "#col_assembler = VectorAssembler(inputCols=[c +'_IDX' for c in colCat], outputCol=\"features\",handleInvalid=\"keep\")\n",
    "#data = col_assembler.transform(df_indexers)\n",
    "#df_features.withColumn('features', col(col_assembler.transform(df_indexers)))\n",
    "#df_indexers.select([c +'_IDX' for c in colCat]).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator \n",
    "\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[indexer.getOutputCol() for indexer in indexers], outputCols=[\"{0}_vec\".format(indexer.getOutputCol()) for indexer in indexers],handleInvalid=\"keep\")\n",
    "\n",
    "#encoder = OneHotEncoder(inputCols=[c + \"_IDX\" for c in colCat], outputCols=[c + \"_IDX_vec\" for c in colCat],handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "#from pyspark.ml import Pipeline \n",
    "#pipeline = Pipeline(stages=[*indexers,*encoder])\n",
    "#df_indexers = pipeline.fit(df).transform(df)\n",
    "#df_indexers.select([c +'_IDX_vec' for c in colCat]).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count categorical values\n",
    "Confirm the amount of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o986.showString.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2142764a32e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountDistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolCat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.collect()[0].asDict()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o986.showString.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\n"
     ]
    }
   ],
   "source": [
    "df.agg(*(countDistinct(col(c)).alias(c) for c in colCat)).show()#.collect()[0].asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical values\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "numPipeline = Pipeline(stages=[imputer, num_assembler, scaler])\n",
    "catPipeline = Pipeline(stages=[*indexers, encoder])\n",
    "\n",
    "#.select(\"scaledFeatures\", *[c +'_IDX_vec' for c in colCat])\n",
    "pipeline = Pipeline(stages=[numPipeline, catPipeline])\n",
    "preprocessed_df = pipeline.fit(df).transform(df)\n",
    "#print(\"Saving model\")\n",
    "#preprocessed_df.save(projDir + \"\\pipeline\")\n",
    "#print(\"Saved complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed_df = preprocessed_df.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#catPipeline.fit(df).transform(df).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df.select(\"scaledFeatures\",*[c + \"_IDX_vec\" for c in colCat]).show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va2 = VectorAssembler(inputCols=[\"scaledFeatures\",*[c + \"_IDX_vec\" for c in colCat]], outputCol=\"final_features\")\n",
    "\n",
    "temp1 = va2.transform(preprocessed_df)\n",
    "temp1.show(1)\n",
    "temp1 = temp1.withColumn('label', col(\"Severity\"))\n",
    "dataset = temp1.withColumn('features', temp1.final_features).select(\"features\",\"label\")\n",
    "dataset.show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChiSqSelector\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=40, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "\n",
    "result = selector.fit(dataset).transform(dataset)\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, testSet = dataset.randomSplit([0.8 ,0.2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet.show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logisitic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features',labelCol=\"label\",maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(trainSet)\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficientMatrix))\n",
    "#print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "#print(\"RMSE: %s\" % str(trainingSummary.rootMeanSquaredError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients: %s\" % str(lrModel.coefficientMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lrModel.coefficients)\n",
    "plt.ylabel('Beta Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "algo = RandomForestClassifier(featuresCol='features', labelCol='label')\n",
    "model = algo.fit(trainSet)\n",
    "\n",
    "predictions = model.transform(trainSet)\n",
    "predictions.select(['label','prediction', 'probability']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderROC')\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = predictions.select(['label']).collect()\n",
    "y_pred = predictions.select(['prediction']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainingSummary.accuracy)\n",
    "print(trainingSummary.falsePositiveRateByLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
