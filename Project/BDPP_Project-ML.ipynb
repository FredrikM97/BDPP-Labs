{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport ibmos2spark\\n# @hidden_cell\\ncredentials = {\\n    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\\n    'service_id': 'iam-ServiceId-1d71bac5-a341-449f-9ca7-b29de95e771a',\\n    'iam_service_endpoint': 'https://iam.eu-gb.bluemix.net/oidc/token',\\n    'api_key': 'ejujKveBRS3Bk7l3pzjemEQDhTSaOKmqCg6x6osXWkx4'\\n}\\n\\nconfiguration_name = 'os_2d970470a6354234a1716fe9f4db519b_configs'\\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\\n\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder.getOrCreate()\\n\\ndf = spark.read  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')  .option('header', 'true')  .load(cos.url('US_Accidents_Dec19.csv', 'bdppproject-donotdelete-pr-nejualq57kuqe4'))\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-1d71bac5-a341-449f-9ca7-b29de95e771a',\n",
    "    'iam_service_endpoint': 'https://iam.eu-gb.bluemix.net/oidc/token',\n",
    "    'api_key': 'ejujKveBRS3Bk7l3pzjemEQDhTSaOKmqCg6x6osXWkx4'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_2d970470a6354234a1716fe9f4db519b_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('US_Accidents_Dec19.csv', 'bdppproject-donotdelete-pr-nejualq57kuqe4'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler,OneHotEncoderEstimator,QuantileDiscretizer, StringIndexer, Imputer,StandardScaler,MinMaxScaler\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import ChiSqSelector,PCA\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier,RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_indexing_dir = \"log_index.md\"\n",
    "models_dir = \"models\"\n",
    "logs_dir = \"models\" \n",
    "filter_state = 'CA' # ALL if all states\n",
    "model_Note = ''\n",
    "log_mode=True # Logs data to files, Dont use on massive dataset (debug_mode is recommended while on)\n",
    "debug_mode=False # Reduce number of rows collected for faster processing\n",
    "enable_plots = True # Dont use unless debug_mode is on, or datasize is small\n",
    "logger = logging(models_dir, logs_dir, \"\", filter_state, enabled=log_mode)\n",
    "\n",
    "file = \"data/US_Accidents_Dec19.csv\"\n",
    "\n",
    "\n",
    "df, sc,spark = setup_spark(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4082e07eadd9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Spark Project>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: [('spark.memory.offHeap.size', '4g'), ('spark.driver.port', '44161'), ('spark.app.id', 'local-1589634946008'), ('spark.executor.memory', '4g'), ('spark.driver.host', '4082e07eadd9'), ('spark.ui.enabled', 'true'), ('spark.executor.id', 'driver'), ('spark.app.name', 'Spark Project'), ('spark.memory.offHeap.enabled', 'true'), ('spark.ui.killEnabled', 'false'), ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'), ('spark.driver.memory', '6g'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.executor.instances', '1'), ('spark.kryoserializer.buffer.max', '256m'), ('spark.master', 'local[*]'), ('spark.executor.cores', '1'), ('spark.submit.deployMode', 'client'), ('spark.driver.maxResultSize', '3g'), ('spark.ui.showConsoleProgress', 'true'), ('spark.driver.cores', '1')]\n"
     ]
    }
   ],
   "source": [
    "tmp = sc._conf.getAll()\n",
    "logger.write2file(\"New Spark session\", str(tmp))\n",
    "print(\"Config:\",tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2925212"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if debug_mode == True: \n",
    "    df.createOrReplaceTempView(\"records\")\n",
    "    reviewData = spark.sql(\"SELECT * FROM records LIMIT 100000\") \n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: ['Severity'] \n",
      "Categories: ['Street', 'Side', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone', 'Airport_Code', 'Wind_Direction', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
      "Numerical: ['TMC', 'Start_Lat', 'Start_Lng', 'Distance(mi)', 'Number', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)']\n"
     ]
    }
   ],
   "source": [
    "colLabel = [\"Severity\"]\n",
    "\n",
    "colRem = ['ID', \n",
    "          'Source',\n",
    "          'End_Time',\n",
    "          'End_Lat',\n",
    "          'End_Lng',\n",
    "          'Description',\n",
    "        ]\n",
    "\n",
    "df, colCat, colNum = setup_variables(df, sc, colLabel, colRem)  \n",
    "\n",
    "logger.write2file(\"Number of rows\", str(df.count()))\n",
    "logger.write2file(\"Categorical groups\",\"Defined Label:\\n\" + str(colLabel) + \"\\nDefined Categories:\\n\" + str(colCat) + \"\\nDefined Numerical:\\n\" +str(colNum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse single state\n",
    "#Create a model for separates states instead of all states at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_state != 'ALL':\n",
    "    df = df.filter(df.State == filter_state) # Lowers the dataset quite a lot\n",
    "    logger.write2file(\"Specified state\",str(filter_state))\n",
    "else:\n",
    "    logger.write2file(\"No state specified\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify time\n",
    "#Convert the timestamp into a numeric value and then into a string so that the time of day and month can be categorised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+---------+-----------+------------+------+-----------+----+-------+------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "|  TMC|Severity|         Start_Time|Start_Lat|  Start_Lng|Distance(mi)|Number|     Street|Side|   City|County|State|Zipcode|Country|  Timezone|Airport_Code|  Weather_Timestamp|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Amenity| Bump|Crossing|Give_Way|Junction|No_Exit|Railway|Roundabout|Station| Stop|Traffic_Calming|Traffic_Signal|Turning_Loop|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|Start_Hour|Start_Month|Weather_Hour|Weather_Month|\n",
      "+-----+--------+-------------------+---------+-----------+------------+------+-----------+----+-------+------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "|201.0|       3|2016-06-21 10:34:40|  38.0853|-122.233017|         0.0|  null|Magazine St|   R|Vallejo|Solano|   CA|  94591|     US|US/Pacific|        KAPC|2016-06-21 10:54:00|          75.0|         null|       48.0|        30.0|          10.0|      Variable|            5.8|             null|            Clear|  false|false|   false|   false|   false|  false|  false|     false|  false|false|          false|         false|       false|           Day|           Day|              Day|                  Day|         3|          6|           3|            6|\n",
      "+-----+--------+-------------------+---------+-----------+------------+------+-----------+----+-------+------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to int then cast to string\n",
    "\n",
    "df = df.withColumn('Start_Hour', hour(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "df = df.withColumn('Start_Month', month(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "df = df.withColumn('Weather_Hour', hour(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "df = df.withColumn('Weather_Month', month(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "df.show(1)\n",
    "logger.write2file(\"Dataset after modifying UTC timestamp\", str(df.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantiles function\n",
    "#Alternative solution to get position \n",
    "#Remove City, Country, State, Zipcode, Airport_Code\n",
    "#In order to reduce dimensionality feature crossing will be applied upon the Start_Lat and Start_Lng then fused in order to create a new set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_feature_cross = False\n",
    "if use_feature_cross: # Depricated due to limited amount of hardware (not enough performance in RAM) to do a correct feature cross!\n",
    "    def cross_func(x,y): \n",
    "        \n",
    "        \"\"\"\n",
    "        Feature crossing of data\n",
    "        \"\"\"\n",
    "        position = np.outer(np.array(x), np.array(y)).flatten()\n",
    "        nonzero = np.nonzero(position)[0]\n",
    "        return SparseVector(position.size, nonzero, position[nonzero])\n",
    "\n",
    "\n",
    "    cross_udf = udf(cross_func, VectorUDT())\n",
    "\n",
    "    # Calculate how many buckets needed (Bins)\n",
    "    discretizer_Lat = QuantileDiscretizer(numBuckets=20, inputCol=\"Start_Lat\", outputCol=\"Start_Lat_disc\")\n",
    "    discretizer_Lng = QuantileDiscretizer(numBuckets=20, inputCol=\"Start_Lng\", outputCol=\"Start_Lng_disc\")\n",
    "\n",
    "    # Into categorical values\n",
    "    indexer_cord = [StringIndexer(inputCol=c + \"_disc\", outputCol=c+\"_IDX\") for c in [\"Start_Lat\",\"Start_Lng\"]]\n",
    "\n",
    "    # One-hot (feature crossing)\n",
    "    encoder_cord = OneHotEncoderEstimator(inputCols=[indexer.getOutputCol() for indexer in indexer_cord], outputCols=[\"{0}_vec\".format(indexer.getOutputCol()) for indexer in indexer_cord])\n",
    "\n",
    "    cord_stages = [discretizer_Lat,discretizer_Lng, *indexer_cord, encoder_cord]\n",
    "    #pipeline_cord = Pipeline(stages=[])\n",
    "    #preprocessed_cord = pipeline_cord.fit(df).transform(df)\n",
    "\n",
    "    # Feature crossing - Since One-hot-encoding works very bad with this it is sadly disabled. It works but the time to calculate the vector is to great with One-hot encoding\n",
    "    \n",
    "    # Following row of code is used instead to give an example of how it would work.\n",
    "    #position = VectorAssembler(inputCols=[\"Start_Lat_IDX_vec\",\"Start_Lng_IDX_vec\"], outputCol=\"position\")\n",
    "    #f = position.transform(preprocessed_cord)\n",
    "    #preprocessed_cord.select('position').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster quantiles to reduce the feature vector for position\n",
    "#Since feature crossing wont work due to the feature vector is to big (regarding to the given resources) we hope that Clustering might reduce them and keep as much data as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.601691964398246\n",
      "+--------+\n",
      "|position|\n",
      "+--------+\n",
      "|     126|\n",
      "|     128|\n",
      "|     105|\n",
      "|     240|\n",
      "|       2|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "number_of_clusters= 400\n",
    "pos_assembler = VectorAssembler(inputCols=['Start_Lat', 'Start_Lng'], outputCol=\"position_features\")\n",
    "preprocessed_cord = pos_assembler.transform(df)\n",
    "\n",
    "trainSet, testSet = preprocessed_cord.randomSplit([0.9 ,0.1], 1)\n",
    "kmeans = KMeans(k=number_of_clusters, featuresCol='position_features', predictionCol='position')\n",
    "model = kmeans.fit(trainSet)\n",
    "predictions = model.transform(testSet)\n",
    "\n",
    "evaluator = ClusteringEvaluator(featuresCol='position_features', predictionCol='position')\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Transform the hole dataset\n",
    "df = model.transform(preprocessed_cord)\n",
    "df.select('position').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+---------+-----------+------------+------+-----------+----+-------+------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+--------------------+--------+\n",
      "|  TMC|Severity|         Start_Time|Start_Lat|  Start_Lng|Distance(mi)|Number|     Street|Side|   City|County|State|Zipcode|Country|  Timezone|Airport_Code|  Weather_Timestamp|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Amenity| Bump|Crossing|Give_Way|Junction|No_Exit|Railway|Roundabout|Station| Stop|Traffic_Calming|Traffic_Signal|Turning_Loop|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|Start_Hour|Start_Month|Weather_Hour|Weather_Month|   position_features|position|\n",
      "+-----+--------+-------------------+---------+-----------+------------+------+-----------+----+-------+------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+--------------------+--------+\n",
      "|201.0|       3|2016-06-21 10:34:40|  38.0853|-122.233017|         0.0|  null|Magazine St|   R|Vallejo|Solano|   CA|  94591|     US|US/Pacific|        KAPC|2016-06-21 10:54:00|          75.0|         null|       48.0|        30.0|          10.0|      Variable|            5.8|             null|            Clear|  false|false|   false|   false|   false|  false|  false|     false|  false|false|          false|         false|       false|           Day|           Day|              Day|                  Day|         3|          6|           3|            6|[38.0853,-122.233...|     126|\n",
      "+-----+--------+-------------------+---------+-----------+------------+------+-----------+----+-------+------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+--------------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)\n",
    "logger.write2file(\"Dataset after adding quantiles\",str(df.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast data \n",
    "#Cast all the datacolumns into correct format so they will be sorted to numerical or categorical values. Data removed caused to big feature vector, contained too #many NaN values or were converted into another form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: ['Severity'] \n",
      "Categories: ['Start_Hour', 'Start_Month', 'Weather_Hour', 'Weather_Month', 'position', 'Side', 'Wind_Direction', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
      "Numerical: ['Distance(mi)', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)']\n"
     ]
    }
   ],
   "source": [
    "# Could be handles as separated lists and then sent to a UDF for processing but since low number of features and easier to visualize this select is kept.\n",
    "df = df.select(\n",
    "        col('Severity').cast('int'),\n",
    "        col('Start_Hour').cast('string'),\n",
    "        col('Start_Month').cast('string'),\n",
    "        col('Weather_Hour').cast('string'),\n",
    "        col('Weather_Month').cast('string'),\n",
    "        col('position').cast('string'),\n",
    "        col('Distance(mi)').cast('double'),\n",
    "        col('Side').cast('string'),   \n",
    "        col('Temperature(F)').cast('double'),\n",
    "        col('Wind_Chill(F)').cast('double'), \n",
    "        col('Humidity(%)').cast('double'),\n",
    "        col('Pressure(in)').cast('double'),\n",
    "        col('Visibility(mi)').cast('double'),\n",
    "        col('Wind_Direction').cast('string'),\n",
    "        col('Wind_Speed(mph)').cast('double'), \n",
    "        col('Weather_Condition').cast('string'),\n",
    "        col('Amenity').cast('string'),\n",
    "        col('Bump').cast('string'),\n",
    "        col('Crossing').cast('string'),\n",
    "        col('Give_Way').cast('string'),\n",
    "        col('Junction').cast('string'),\n",
    "        col('No_Exit').cast('string'),\n",
    "        col('Railway').cast('string'),\n",
    "        col('Roundabout').cast('string'),\n",
    "        col('Station').cast('string'),\n",
    "        col('Stop').cast('string'),\n",
    "        col('Traffic_Calming').cast('string'),\n",
    "        col('Traffic_Signal').cast('string'),\n",
    "        col('Turning_Loop').cast('string'),\n",
    "        col('Sunrise_Sunset').cast('string'),\n",
    "        col('Civil_Twilight').cast('string'),\n",
    "        col('Nautical_Twilight').cast('string'),\n",
    "        col('Astronomical_Twilight').cast('string')\n",
    "    ) \n",
    "\n",
    "colLabel = [\"Severity\"]\n",
    "\n",
    "colCat, colNum = createNewClasses(df, sc, colLabel)\n",
    "logger.write2file(\"Categorical groups\",\"Defined Label:\\n\" + str(colLabel) + \"\\nDefined Categories:\\n\" + str(colCat) + \"\\nDefined Numerical:\\n\" +str(colNum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique column values: [Row(Start_Hour=24), Row(Start_Month=12), Row(Weather_Hour=24), Row(Weather_Month=12), Row(position=400), Row(Side=3), Row(Wind_Direction=24), Row(Weather_Condition=69), Row(Amenity=2), Row(Bump=2), Row(Crossing=2), Row(Give_Way=2), Row(Junction=2), Row(No_Exit=2), Row(Railway=2), Row(Roundabout=2), Row(Station=2), Row(Stop=2), Row(Traffic_Calming=2), Row(Traffic_Signal=2), Row(Turning_Loop=1), Row(Sunrise_Sunset=2), Row(Civil_Twilight=2), Row(Nautical_Twilight=2), Row(Astronomical_Twilight=2)]\n"
     ]
    }
   ],
   "source": [
    "df.persist()\n",
    "tmp = [df.select(countDistinct(c).alias(c)).collect()[0] for c in [*colCat]] \n",
    "df.unpersist()\n",
    "print(\"Unique column values:\", tmp)\n",
    "\n",
    "logger.write2file(\"Unique column values\", str(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recheck the missing values\n",
    "#Check so that the output contains 0 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing valuesroot\n",
      " |-- Severity: long (nullable = false)\n",
      " |-- Start_Hour: long (nullable = false)\n",
      " |-- Start_Month: long (nullable = false)\n",
      " |-- Weather_Hour: long (nullable = false)\n",
      " |-- Weather_Month: long (nullable = false)\n",
      " |-- position: long (nullable = false)\n",
      " |-- Distance(mi): long (nullable = false)\n",
      " |-- Side: long (nullable = false)\n",
      " |-- Temperature(F): long (nullable = false)\n",
      " |-- Wind_Chill(F): long (nullable = false)\n",
      " |-- Humidity(%): long (nullable = false)\n",
      " |-- Pressure(in): long (nullable = false)\n",
      " |-- Visibility(mi): long (nullable = false)\n",
      " |-- Wind_Direction: long (nullable = false)\n",
      " |-- Wind_Speed(mph): long (nullable = false)\n",
      " |-- Weather_Condition: long (nullable = false)\n",
      " |-- Amenity: long (nullable = false)\n",
      " |-- Bump: long (nullable = false)\n",
      " |-- Crossing: long (nullable = false)\n",
      " |-- Give_Way: long (nullable = false)\n",
      " |-- Junction: long (nullable = false)\n",
      " |-- No_Exit: long (nullable = false)\n",
      " |-- Railway: long (nullable = false)\n",
      " |-- Roundabout: long (nullable = false)\n",
      " |-- Station: long (nullable = false)\n",
      " |-- Stop: long (nullable = false)\n",
      " |-- Traffic_Calming: long (nullable = false)\n",
      " |-- Traffic_Signal: long (nullable = false)\n",
      " |-- Turning_Loop: long (nullable = false)\n",
      " |-- Sunrise_Sunset: long (nullable = false)\n",
      " |-- Civil_Twilight: long (nullable = false)\n",
      " |-- Nautical_Twilight: long (nullable = false)\n",
      " |-- Astronomical_Twilight: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Severity: int, Start_Hour: string, Start_Month: string, Weather_Hour: string, Weather_Month: string, position: string, Distance(mi): double, Side: string, Temperature(F): double, Wind_Chill(F): double, Humidity(%): double, Pressure(in): double, Visibility(mi): double, Wind_Direction: string, Wind_Speed(mph): double, Weather_Condition: string, Amenity: string, Bump: string, Crossing: string, Give_Way: string, Junction: string, No_Exit: string, Railway: string, Roundabout: string, Station: string, Stop: string, Traffic_Calming: string, Traffic_Signal: string, Turning_Loop: string, Sunrise_Sunset: string, Civil_Twilight: string, Nautical_Twilight: string, Astronomical_Twilight: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "printMissingValues(df,logger)\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove data with occurance less than 1%\n",
    "#Based on information from analysis. With further analysis lower procentage can be used to find better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|Weather_Condition| count|\n",
      "+-----------------+------+\n",
      "|            Clear|253070|\n",
      "|             Fair| 96006|\n",
      "|    Mostly Cloudy| 59929|\n",
      "|    Partly Cloudy| 58523|\n",
      "|         Overcast| 58472|\n",
      "| Scattered Clouds| 29356|\n",
      "|           Cloudy| 24063|\n",
      "|             Haze| 21561|\n",
      "|       Light Rain| 20421|\n",
      "|             null| 12730|\n",
      "|             Rain|  6438|\n",
      "|              Fog|  4100|\n",
      "|       Heavy Rain|  2246|\n",
      "|            Smoke|  1921|\n",
      "|     Fair / Windy|  1179|\n",
      "+-----------------+------+\n",
      "\n",
      "Rows removed: 15887\n"
     ]
    }
   ],
   "source": [
    "df.persist()\n",
    "original_rows = df.count()\n",
    "n = int(df.count()*0.001) # Limit the plot to ignore conditions below an limit\n",
    "\n",
    "weather_freq = df.groupBy('Weather_Condition').count().orderBy('count',ascending=False)\n",
    "df_filtered = weather_freq.filter(weather_freq['count'] > n)\n",
    "filtered_conditions = df_filtered.select(\"Weather_Condition\").rdd.flatMap(lambda x: x).collect()\n",
    "df = df.filter(df['Weather_Condition'].isin(*filtered_conditions))\n",
    "\n",
    "df_filtered.show()\n",
    "removed_rows = original_rows - df.count()\n",
    "\n",
    "df.unpersist()\n",
    "print(\"Rows removed:\",removed_rows)\n",
    "\n",
    "logger.write2file(\"Weather condition\", str(df_filtered.take(df_filtered.count())) + \"\\nRows removed: \" + str(removed_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove if distict values in category equals 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns with 1 class: ['Turning_Loop']\n"
     ]
    }
   ],
   "source": [
    "df.persist()\n",
    "tmp = [c for c in [*colCat] if df.select(countDistinct(c)).collect()[0][0] <= 1] \n",
    "df.unpersist()\n",
    "\n",
    "[colCat.remove(c) for c in tmp]\n",
    "print(\"Dropping columns with 1 class:\", tmp)\n",
    "logger.write2file(\"Dropping columns with 1 class\", str(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(inputCols=colNum, outputCols=colNum)\n",
    "imputer.setStrategy(\"median\")\n",
    "\n",
    "num_assembler = VectorAssembler(inputCols=colNum, outputCol=\"num_features\")\n",
    "scaler = MinMaxScaler(min=0.0, max=1.0, inputCol=\"num_features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "indexers = [StringIndexer(inputCol = c, outputCol = c +'_IDX', handleInvalid='skip') for c in colCat]\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[indexer.getOutputCol() for indexer in indexers], outputCols=[\"{0}_vec\".format(indexer.getOutputCol()) for indexer in indexers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical values\n",
    "\n",
    "numPipeline = Pipeline(stages=[imputer, num_assembler, scaler])\n",
    "catPipeline = Pipeline(stages=[*indexers, encoder])\n",
    "\n",
    "pipeline = Pipeline(stages=[numPipeline, catPipeline])\n",
    "\n",
    "preprocessed_df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+--------------------+---------------------+-----------------+-------------+----------------------+-------------------------+---------------+-------------+----------------+----------------+----------------+---------------+---------------+------------------+---------------+-------------+-----------------------+----------------------+----------------------+----------------------+-------------------------+-----------------------------+\n",
      "|scaledFeatures                                                                                                             |Start_Hour_IDX_vec|Start_Month_IDX_vec|Weather_Hour_IDX_vec|Weather_Month_IDX_vec|position_IDX_vec |Side_IDX_vec |Wind_Direction_IDX_vec|Weather_Condition_IDX_vec|Amenity_IDX_vec|Bump_IDX_vec |Crossing_IDX_vec|Give_Way_IDX_vec|Junction_IDX_vec|No_Exit_IDX_vec|Railway_IDX_vec|Roundabout_IDX_vec|Station_IDX_vec|Stop_IDX_vec |Traffic_Calming_IDX_vec|Traffic_Signal_IDX_vec|Sunrise_Sunset_IDX_vec|Civil_Twilight_IDX_vec|Nautical_Twilight_IDX_vec|Astronomical_Twilight_IDX_vec|\n",
      "+---------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+--------------------+---------------------+-----------------+-------------+----------------------+-------------------------+---------------+-------------+----------------+----------------+----------------+---------------+---------------+------------------+---------------+-------------+-----------------------+----------------------+----------------------+----------------------+-------------------------+-----------------------------+\n",
      "|[0.0,0.6668785759694851,0.6379044684129429,0.47474747474747475,0.9071472205253511,0.07142857142857142,0.007049100631988332]|(23,[10],[1.0])   |(11,[7],[1.0])     |(23,[8],[1.0])      |(11,[7],[1.0])       |(399,[113],[1.0])|(2,[0],[1.0])|(23,[7],[1.0])        |(13,[0],[1.0])           |(1,[0],[1.0])  |(1,[0],[1.0])|(1,[0],[1.0])   |(1,[0],[1.0])   |(1,[0],[1.0])   |(1,[0],[1.0])  |(1,[0],[1.0])  |(1,[0],[1.0])     |(1,[0],[1.0])  |(1,[0],[1.0])|(1,[0],[1.0])          |(1,[0],[1.0])         |(1,[0],[1.0])         |(1,[0],[1.0])         |(1,[0],[1.0])            |(1,[0],[1.0])                |\n",
      "+---------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+--------------------+---------------------+-----------------+-------------+----------------------+-------------------------+---------------+-------------+----------------+----------------+----------------+---------------+---------------+------------------+---------------+-------------+-----------------------+----------------------+----------------------+----------------------+-------------------------+-----------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df.select(\"scaledFeatures\",*[c + \"_IDX_vec\" for c in colCat]).show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving into: models/CA_20200516131544/df_features\n",
      "Saving through format 1\n",
      "Feature set size:  635496\n"
     ]
    }
   ],
   "source": [
    "va2 = VectorAssembler(inputCols=[\"scaledFeatures\", *[c + \"_IDX_vec\" for c in colCat]], outputCol=\"final_features\")\n",
    "\n",
    "df = va2.transform(preprocessed_df)\n",
    "df.persist()\n",
    "df = df.withColumn('label', col(\"Severity\"))\n",
    "df = df.withColumn('features', df.final_features).select(\"features\",\"label\")\n",
    "df.unpersist()\n",
    "#df_features.show(1, False)\n",
    "#df = df_features\n",
    "\n",
    "logger.write2fileModel(df, \"df_features\")\n",
    "\n",
    "logger.write2file(\"Feature set size\", str(df.count()) + \"\\n\\n__Feature vector and label:__\\n\" + str(df.take(1)))\n",
    "logger.write2file(\"Number of rows\", str(df.count()))\n",
    "print(\"Feature set size: \",df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique column values: ['Turning_Loop']\n"
     ]
    }
   ],
   "source": [
    "# Checking how many classes that can be used\n",
    "#df.persist()\n",
    "#tmp = [df.select(countDistinct(c).alias(c)).collect()[0] for c in [*colCat, \"position\"]] \n",
    "#df.unpersist()\n",
    "print(\"Unique column values:\", tmp)\n",
    "\n",
    "#logger.write2file(\"Unique column values\", str(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "#In order to understand how much the variance affect the dataset we check with PCA. Try to get above 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA - Feature Variance: Top 50:\n",
      "[0.09517194 0.04232475 0.03187734 0.02992612 0.02626072 0.02584163\n",
      " 0.02481872 0.02431837 0.021698   0.0210608  0.01971418 0.019116\n",
      " 0.01864003 0.01732142 0.01655527 0.01550796 0.01488046 0.01471253\n",
      " 0.0138168  0.01375283 0.01310388 0.01297739 0.0125252  0.01215968\n",
      " 0.01195906 0.0116045  0.01129448 0.01100461 0.01055348 0.01033876\n",
      " 0.00993654 0.00958048 0.00940869 0.00864292 0.00853628 0.00799536\n",
      " 0.00739191 0.00730536 0.0070218  0.00665225 0.00655656 0.00617407\n",
      " 0.00588477 0.00584203 0.00550042 0.00536951 0.0048586  0.0047834\n",
      " 0.00463187 0.00446751]\n",
      "Number of items: 377\n",
      "Sum of variance: 0.9928885632516312\n",
      "Saving into: models/CA_20200516131544/pca_df\n",
      "Saving through format 1\n"
     ]
    }
   ],
   "source": [
    "k=377#k=250\n",
    "pca = PCA(k=k, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df)\n",
    "pca_df = pca_model.transform(df)\n",
    "\n",
    "\n",
    "print(\"PCA - Feature Variance:\",\"Top 50:\\n\" + str(pca_model.explainedVariance[:50]) + \"\\nNumber of items: \"+str(k)+\"\\nSum of variance: \"+ str(sum(pca_model.explainedVariance)))\n",
    "logger.write2file(\"PCA - Feature variance\", \"Top 50:\\n\" + str(pca_model.explainedVariance[:50]) + \"\\nNumber of items: \"+ str(k) +\"\\nSum of variance: \"+ str(sum(pca_model.explainedVariance)))\n",
    "logger.write2fileModel(pca_df, \"pca_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChiSqSelector\n",
    "#Check top 100 which of the values in the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving into: models/CA_20200516131544/chi_df\n",
      "Saving through format 1\n",
      "Top selected features according to ChiSqSelector: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 16, 17, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 49, 51, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 96, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 113, 115, 116, 117, 118, 119, 120, 122, 125, 127, 128, 129, 131, 132, 133, 134, 135, 137, 138, 139, 141, 142, 144, 146, 147, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 196, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213, 214, 215, 217, 219, 221, 222, 223, 224, 226, 227, 229, 230, 231, 232, 233, 234, 235, 236, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 307, 308, 309, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 337, 338, 339, 340, 341, 342, 343, 346, 347, 348, 350, 351, 352, 354, 356, 359, 362, 363, 364, 365, 366, 368, 369, 371, 372, 373, 374, 375, 377, 378, 379, 380, 381, 382, 383, 384, 385, 389, 390, 391, 392, 393, 396, 398, 399, 401, 402, 405, 406, 408, 409, 411, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 430, 433, 436, 438, 448, 456, 464, 473, 474, 475, 476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489, 491, 492, 493, 495, 498]\n",
      "Transformed selected features: (377,[1,2,3,4,5,6,14,32,40,57,151,357,365],[0.6668785759694851,0.6379044684129429,0.47474747474747475,0.9071472205253511,0.07142857142857142,0.007049100631988332,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
      "+--------------------+-----+--------------------+\n",
      "|            features|label|    selectedFeatures|\n",
      "+--------------------+-----+--------------------+\n",
      "|(528,[1,2,3,4,5,6...|    3|(377,[1,2,3,4,5,6...|\n",
      "|(528,[1,2,3,4,5,6...|    3|(377,[1,2,3,4,5,6...|\n",
      "|(528,[1,2,3,4,5,6...|    2|(377,[1,2,3,4,5,6...|\n",
      "|(528,[1,2,3,4,5,6...|    3|(377,[1,2,3,4,5,6...|\n",
      "|(528,[1,2,3,4,5,6...|    2|(377,[1,2,3,4,5,6...|\n",
      "|(528,[1,2,3,4,5,6...|    3|(377,[1,2,3,4,5,6...|\n",
      "|(528,[1,2,3,4,5,6...|    3|(377,[1,2,3,4,5,6...|\n",
      "|(528,[1,2,3,4,5,6...|    3|(377,[1,2,3,4,5,6...|\n",
      "|(528,[1,2,3,4,5,6...|    2|(377,[1,2,3,4,5,6...|\n",
      "|(528,[1,2,3,4,5,6...|    3|(377,[1,2,3,4,5,6...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Transformed feature vector: None\n"
     ]
    }
   ],
   "source": [
    "selector = ChiSqSelector(numTopFeatures=k, \n",
    "                         labelCol='label', \n",
    "                         featuresCol='features', \n",
    "                         outputCol=\"selectedFeatures\",\n",
    "                         selectorType='numTopFeatures', \n",
    "                         percentile=0.1, \n",
    "                         fpr=0.05, fdr=0.05, fwe=0.05)\n",
    "\n",
    "chi_model = selector.fit(df)\n",
    "chi_df = chi_model.transform(df)\n",
    "\n",
    "logger.write2file(\"Top selected features according to ChiSqSelector\", str(chi_model.selectedFeatures)+ \"\\nNumber of features: \" + str(k) + \"\\nExample data:\\n\"+str(chi_df.take(5)))\n",
    "logger.write2fileModel(chi_df, \"chi_df\")\n",
    "\n",
    "print(\"Top selected features according to ChiSqSelector:\", chi_model.selectedFeatures)\n",
    "print(\"Transformed selected features:\",chi_df.head().selectedFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the chi_df for training since it is reduced\n",
    "#trainSet, testSet = df_features.randomSplit([0.8 ,0.2], 1)\n",
    "trainSet, testSet = chi_df.randomSplit([0.8 ,0.2], 1)\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                    |label|selectedFeatures                                                                                                                                                                                                    |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(528,[0,1,2,3,4,5,6,7,30,41,64,75,474,482,508,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527],[4.4012146052121174E-5,0.5848696757787667,0.6379044684129429,0.9595959595959596,0.9050091631032375,0.012857142857142857,0.00838599902771026,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|3    |(377,[0,1,2,3,4,5,6,7,25,36,51,61,357,364],[4.4012146052121174E-5,0.5848696757787667,0.6379044684129429,0.9595959595959596,0.9050091631032375,0.012857142857142857,0.00838599902771026,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(528,[0,1,2,3,4,5,6,7,30,41,64,75,474,482,508,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527],[4.4012146052121174E-5,0.5848696757787667,0.6379044684129429,0.9595959595959596,0.9050091631032375,0.012857142857142857,0.00838599902771026,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|3    |(377,[0,1,2,3,4,5,6,7,25,36,51,61,357,364],[4.4012146052121174E-5,0.5848696757787667,0.6379044684129429,0.9595959595959596,0.9050091631032375,0.012857142857142857,0.00838599902771026,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainSet.show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model..\n",
      "Predicting on testSet..\n",
      "Evaluating predictions..\n",
      "Saving into: models/CA_20200516131544/LR_Model\n",
      "'function' object has no attribute 'format'\n",
      "Saving through format 2\n",
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       2.0|    2|(528,[0,1,2,3,4,5...|\n",
      "|       2.0|    2|(528,[0,1,2,3,4,5...|\n",
      "|       3.0|    3|(528,[0,1,2,3,4,5...|\n",
      "|       2.0|    2|(528,[0,1,2,3,4,5...|\n",
      "|       2.0|    3|(528,[0,1,2,3,4,5...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Accuracy:  0.6883025028570805 \n",
      "Parameters\n",
      " {Param(parent='LogisticRegression_f235d06c17ee', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent='LogisticRegression_f235d06c17ee', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='LogisticRegression_f235d06c17ee', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        50\n",
      "           2       0.73      0.94      0.82     85204\n",
      "           3       0.70      0.31      0.43     40853\n",
      "           4       0.00      0.00      0.00      1127\n",
      "\n",
      "    accuracy                           0.73    127234\n",
      "   macro avg       0.36      0.31      0.31    127234\n",
      "weighted avg       0.71      0.73      0.69    127234\n",
      "\n",
      "Confusion matrix:\n",
      "[[    0    46     4     0]\n",
      " [    0 79844  5360     0]\n",
      " [    0 28184 12669     0]\n",
      " [    0   988   139     0]]\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"selectedFeatures\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.1,0.01]) \\\n",
    "                .addGrid(lr.maxIter, [10]) \\\n",
    "                .addGrid(lr.elasticNetParam, [0.6]) \\\n",
    "                .build()\n",
    "try: \n",
    "    model, _ = evaluateModel(lr, paramGrid, \"LR_Model\", trainSet, testSet, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logger=logger)\n",
    "except Exception as e:\n",
    "    logger.write2file(\"Error:\", str(e))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model..\n",
      "Predicting on testSet..\n",
      "Evaluating predictions..\n",
      "Saving into: models/CA_20200516131544/DT_Model\n",
      "'function' object has no attribute 'format'\n",
      "Saving through format 2\n",
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       2.0|    2|(528,[0,1,2,3,4,5...|\n",
      "|       2.0|    2|(528,[0,1,2,3,4,5...|\n",
      "|       3.0|    3|(528,[0,1,2,3,4,5...|\n",
      "|       2.0|    2|(528,[0,1,2,3,4,5...|\n",
      "|       2.0|    3|(528,[0,1,2,3,4,5...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Accuracy:  0.5498715797285845 \n",
      "Parameters\n",
      " {}\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        50\n",
      "           2       0.67      1.00      0.80     85204\n",
      "           3       0.77      0.02      0.04     40853\n",
      "           4       1.00      0.00      0.00      1127\n",
      "\n",
      "    accuracy                           0.67    127234\n",
      "   macro avg       0.61      0.25      0.21    127234\n",
      "weighted avg       0.71      0.67      0.55    127234\n",
      "\n",
      "Confusion matrix:\n",
      "[[    0    50     0     0]\n",
      " [    0 85036   168     0]\n",
      " [    0 40116   737     0]\n",
      " [    0  1077    49     1]]\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"selectedFeatures\") \n",
    "\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "_, _ = evaluateModel(dt, paramGrid, \"DT_Model\", trainSet, testSet, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model..\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"selectedFeatures\") #numTrees=10\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(rf.numTrees,[15]) \\\n",
    "                .build()\n",
    "\n",
    "_, _ = evaluateModel(rf, paramGrid, \"RF_Model\", trainSet, testSet, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.write2file(\"Program finished!\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.write2file(\"New model folder created\", \\\n",
    "               \"Model finished: Yes\" + \\\n",
    "               \"\\nFolder name: \" + logger.timeSignature + \\\n",
    "               \"\\nState: \"+ filter_state + \\\n",
    "               \"\\nLogs directory: \" + logger.logs_dir +  \\\n",
    "               \"\\nFile: \" + file + \\\n",
    "               \"\\nNote: \" + model_Note, \\\n",
    "               logs_dir=model_indexing_dir \\\n",
    "              )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
