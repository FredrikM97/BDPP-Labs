{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport ibmos2spark\\n# @hidden_cell\\ncredentials = {\\n    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\\n    'service_id': 'iam-ServiceId-1d71bac5-a341-449f-9ca7-b29de95e771a',\\n    'iam_service_endpoint': 'https://iam.eu-gb.bluemix.net/oidc/token',\\n    'api_key': 'ejujKveBRS3Bk7l3pzjemEQDhTSaOKmqCg6x6osXWkx4'\\n}\\n\\nconfiguration_name = 'os_2d970470a6354234a1716fe9f4db519b_configs'\\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\\n\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder.getOrCreate()\\n\\ndf = spark.read  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')  .option('header', 'true')  .load(cos.url('US_Accidents_Dec19.csv', 'bdppproject-donotdelete-pr-nejualq57kuqe4'))\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-1d71bac5-a341-449f-9ca7-b29de95e771a',\n",
    "    'iam_service_endpoint': 'https://iam.eu-gb.bluemix.net/oidc/token',\n",
    "    'api_key': 'ejujKveBRS3Bk7l3pzjemEQDhTSaOKmqCg6x6osXWkx4'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_2d970470a6354234a1716fe9f4db519b_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('US_Accidents_Dec19.csv', 'bdppproject-donotdelete-pr-nejualq57kuqe4'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler,OneHotEncoderEstimator,QuantileDiscretizer, StringIndexer, Imputer,StandardScaler,MinMaxScaler\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import ChiSqSelector,PCA\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier,RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_indexing_dir = \"log_index.md\"\n",
    "models_dir = \"models\"\n",
    "logs_dir = \"models\" \n",
    "filter_state = 'SC' # ALL if all states\n",
    "model_Note = ''\n",
    "log_mode=True\n",
    "debug_mode=True\n",
    "logger = logging(models_dir, logs_dir, \"\", filter_state, enabled=log_mode)\n",
    "\n",
    "file = \"data/US_Accidents_Dec19.csv\"\n",
    "\n",
    "\n",
    "df, sc,spark = setup_spark(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://97901d576ca5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Spark Project>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: [('spark.driver.memory', '4g'), ('spark.executor.memory', '4g'), ('spark.ui.enabled', 'true'), ('spark.executor.id', 'driver'), ('spark.app.name', 'Spark Project'), ('spark.ui.killEnabled', 'false'), ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'), ('spark.driver.host', '97901d576ca5'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.app.id', 'local-1589251920243'), ('spark.executor.instances', '1'), ('spark.master', 'local[*]'), ('spark.executor.cores', '1'), ('spark.submit.deployMode', 'client'), ('spark.driver.port', '37213'), ('spark.kryoserializer.buffer.max', '15'), ('spark.ui.showConsoleProgress', 'true'), ('spark.driver.cores', '1')]\n"
     ]
    }
   ],
   "source": [
    "tmp = sc._conf.getAll()\n",
    "logger.write2file(\"New Spark session\", str(tmp))\n",
    "print(\"Config:\",tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2925212"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if debug_mode == True: \n",
    "    df.createOrReplaceTempView(\"records\")\n",
    "    reviewData = spark.sql(\"SELECT * FROM records LIMIT 100000\") \n",
    "\n",
    "#df,_ = df.randomSplit([1.0,0.0], 1) # Speed up the program a bit\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: ['Severity'] \n",
      "Categories: ['Street', 'Side', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone', 'Airport_Code', 'Wind_Direction', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
      "Numerical: ['TMC', 'Start_Lat', 'Start_Lng', 'Distance(mi)', 'Number', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)']\n"
     ]
    }
   ],
   "source": [
    "colLabel = [\"Severity\"]\n",
    "\n",
    "colRem = ['ID', \n",
    "          'Source',\n",
    "          'End_Time',\n",
    "          'End_Lat',\n",
    "          'End_Lng',\n",
    "          'Description',\n",
    "        ]\n",
    "\n",
    "df, colCat, colNum = setup_variables(df, sc, colLabel, colRem)  \n",
    "\n",
    "logger.write2file(\"Number of rows\", str(df.count()))\n",
    "logger.write2file(\"Categorical groups\",\"Defined Label:\\n\" + str(colLabel) + \"\\nDefined Categories:\\n\" + str(colCat) + \"\\nDefined Numerical:\\n\" +str(colNum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse single state\n",
    "#Create a model for separates states instead of all states at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_state != 'ALL':\n",
    "    df = df.filter(df.State == filter_state) # Lowers the dataset quite a lot\n",
    "    logger.write2file(\"Specified state\",str(filter_state))\n",
    "else:\n",
    "    logger.write2file(\"No state specified\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify time\n",
    "#Convert the timestamp into a numeric value and then into a string so that the time of day and month can be categorised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+---------+----------+------------+------+---------+----+----+--------+-----+----------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "|  TMC|Severity|         Start_Time|Start_Lat| Start_Lng|Distance(mi)|Number|   Street|Side|City|  County|State|   Zipcode|Country|  Timezone|Airport_Code|  Weather_Timestamp|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Amenity| Bump|Crossing|Give_Way|Junction|No_Exit|Railway|Roundabout|Station| Stop|Traffic_Calming|Traffic_Signal|Turning_Loop|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|Start_Hour|Start_Month|Weather_Hour|Weather_Month|\n",
      "+-----+--------+-------------------+---------+----------+------------+------+---------+----+----+--------+-----+----------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "|201.0|       2|2016-11-30 16:19:05|34.318562|-82.663651|        0.01| 100.0|Ashley St|   L| Iva|Anderson|   SC|29655-9004|     US|US/Eastern|        KAND|2016-11-30 16:21:00|          64.9|         null|       90.0|       29.75|           0.8|          West|           17.3|             0.47|       Heavy Rain|  false|false|   false|   false|   false|  false|  false|     false|  false|false|          false|         false|       false|           Day|           Day|              Day|                  Day|        11|         11|          11|           11|\n",
      "+-----+--------+-------------------+---------+----------+------------+------+---------+----+----+--------+-----+----------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to int then cast to string\n",
    "\n",
    "df = df.withColumn('Start_Hour', hour(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "df = df.withColumn('Start_Month', month(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "df = df.withColumn('Weather_Hour', hour(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "df = df.withColumn('Weather_Month', month(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "df.show(1)\n",
    "logger.write2file(\"Dataset after modifying UTC timestamp\", str(df.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantiles function\n",
    "#Alternative solution to get position \n",
    "#Remove City, Country, State, Zipcode, Airport_Code\n",
    "#In order to reduce dimensionality feature crossing will be applied upon the Start_Lat and Start_Lng then fused in order to create a new set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Calculate how many buckets needed (Bins)\n",
    "discretizer_Lat = QuantileDiscretizer(numBuckets=100, inputCol=\"Start_Lat\", outputCol=\"Start_Lat_disc\")\n",
    "discretizer_Lng = QuantileDiscretizer(numBuckets=100, inputCol=\"Start_Lng\", outputCol=\"Start_Lng_disc\")\n",
    "\n",
    "# Into categorical values\n",
    "indexer_cord = [StringIndexer(inputCol=c + \"_disc\", outputCol=c+\"_IDX\") for c in [\"Start_Lat\",\"Start_Lng\"]]\n",
    "\n",
    "# One-hot (feature crossing)\n",
    "encoder_cord = OneHotEncoderEstimator(inputCols=[indexer.getOutputCol() for indexer in indexer_cord], outputCols=[\"{0}_vec\".format(indexer.getOutputCol()) for indexer in indexer_cord])\n",
    "\n",
    "pipeline_cord = Pipeline(stages=[discretizer_Lat,discretizer_Lng, *indexer_cord, encoder_cord])\n",
    "preprocessed_cord = pipeline_cord.fit(df).transform(df)\n",
    "\n",
    "position = VectorAssembler(inputCols=[\"Start_Lat_IDX_vec\",\"Start_Lng_IDX_vec\"], outputCol=\"position\")\n",
    "\n",
    "df = position.transform(preprocessed_cord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+---------+----------+------------+------+---------+----+----+--------+-----+----------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+--------------+--------------+-------------+-------------+-----------------+-----------------+--------------------+\n",
      "|  TMC|Severity|         Start_Time|Start_Lat| Start_Lng|Distance(mi)|Number|   Street|Side|City|  County|State|   Zipcode|Country|  Timezone|Airport_Code|  Weather_Timestamp|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Amenity| Bump|Crossing|Give_Way|Junction|No_Exit|Railway|Roundabout|Station| Stop|Traffic_Calming|Traffic_Signal|Turning_Loop|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|Start_Hour|Start_Month|Weather_Hour|Weather_Month|Start_Lat_disc|Start_Lng_disc|Start_Lat_IDX|Start_Lng_IDX|Start_Lat_IDX_vec|Start_Lng_IDX_vec|            position|\n",
      "+-----+--------+-------------------+---------+----------+------------+------+---------+----+----+--------+-----+----------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+--------------+--------------+-------------+-------------+-----------------+-----------------+--------------------+\n",
      "|201.0|       2|2016-11-30 16:19:05|34.318562|-82.663651|        0.01| 100.0|Ashley St|   L| Iva|Anderson|   SC|29655-9004|     US|US/Eastern|        KAND|2016-11-30 16:21:00|          64.9|         null|       90.0|       29.75|           0.8|          West|           17.3|             0.47|       Heavy Rain|  false|false|   false|   false|   false|  false|  false|     false|  false|false|          false|         false|       false|           Day|           Day|              Day|                  Day|        11|         11|          11|           11|          55.0|           4.0|         95.0|         71.0|  (99,[95],[1.0])|  (99,[71],[1.0])|(198,[95,170],[1....|\n",
      "+-----+--------+-------------------+---------+----------+------------+------+---------+----+----+--------+-----+----------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+--------------+--------------+-------------+-------------+-----------------+-----------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)\n",
    "logger.write2file(\"Dataset after adding quantiles\",str(df.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast data \n",
    "#Cast all the datacolumns into correct format so they will be sorted to numerical or categorical values. Data removed caused to big feature vector, contained too #many NaN values or were converted into another form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: ['Severity'] \n",
      "Categories: ['Start_Hour', 'Start_Month', 'Weather_Hour', 'Weather_Month', 'Side', 'Wind_Direction', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
      "Numerical: ['Distance(mi)', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)']\n"
     ]
    }
   ],
   "source": [
    "# Could be handles as separated lists and then sent to a UDF for processing but since low number of features and easier to visualize this select is kept.\n",
    "df = df.select(\n",
    "        col('Severity').cast('int'),\n",
    "        col('Start_Hour').cast('string'),\n",
    "        col('Start_Month').cast('string'),\n",
    "        col('Weather_Hour').cast('string'),\n",
    "        col('Weather_Month').cast('string'),\n",
    "        col('position'),\n",
    "        col('Distance(mi)').cast('double'),\n",
    "        col('Side').cast('string'),   \n",
    "        col('Temperature(F)').cast('double'),\n",
    "        col('Wind_Chill(F)').cast('double'), \n",
    "        col('Humidity(%)').cast('double'),\n",
    "        col('Pressure(in)').cast('double'),\n",
    "        col('Visibility(mi)').cast('double'),\n",
    "        col('Wind_Direction').cast('string'),\n",
    "        col('Wind_Speed(mph)').cast('double'), \n",
    "        col('Weather_Condition').cast('string'),\n",
    "        col('Amenity').cast('string'),\n",
    "        col('Bump').cast('string'),\n",
    "        col('Crossing').cast('string'),\n",
    "        col('Give_Way').cast('string'),\n",
    "        col('Junction').cast('string'),\n",
    "        col('No_Exit').cast('string'),\n",
    "        col('Railway').cast('string'),\n",
    "        col('Roundabout').cast('string'),\n",
    "        col('Station').cast('string'),\n",
    "        col('Stop').cast('string'),\n",
    "        col('Traffic_Calming').cast('string'),\n",
    "        col('Traffic_Signal').cast('string'),\n",
    "        col('Turning_Loop').cast('string'),\n",
    "        col('Sunrise_Sunset').cast('string'),\n",
    "        col('Civil_Twilight').cast('string'),\n",
    "        col('Nautical_Twilight').cast('string'),\n",
    "        col('Astronomical_Twilight').cast('string')\n",
    "    ) \n",
    "\n",
    "colLabel = [\"Severity\"]\n",
    "\n",
    "colCat, colNum = createNewClasses(df, sc, colLabel)\n",
    "logger.write2file(\"Categorical groups\",\"Defined Label:\\n\" + str(colLabel) + \"\\nDefined Categories:\\n\" + str(colCat) + \"\\nDefined Numerical:\\n\" +str(colNum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recheck the missing values\n",
    "#Check so that the output contains 0 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values[Row(Severity=0, Start_Hour=1391, Start_Month=1391, Weather_Hour=2257, Weather_Month=2257, position=0, Distance(mi)=0, Side=0, Temperature(F)=2836, Wind_Chill(F)=89759, Humidity(%)=2865, Pressure(in)=2494, Visibility(mi)=3137, Wind_Direction=2535, Wind_Speed(mph)=25585, Weather_Condition=3079, Amenity=0, Bump=0, Crossing=0, Give_Way=0, Junction=0, No_Exit=0, Railway=0, Roundabout=0, Station=0, Stop=0, Traffic_Calming=0, Traffic_Signal=0, Turning_Loop=0, Sunrise_Sunset=0, Civil_Twilight=0, Nautical_Twilight=0, Astronomical_Twilight=0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Severity: int, Start_Hour: string, Start_Month: string, Weather_Hour: string, Weather_Month: string, position: vector, Distance(mi): double, Side: string, Temperature(F): double, Wind_Chill(F): double, Humidity(%): double, Pressure(in): double, Visibility(mi): double, Wind_Direction: string, Wind_Speed(mph): double, Weather_Condition: string, Amenity: string, Bump: string, Crossing: string, Give_Way: string, Junction: string, No_Exit: string, Railway: string, Roundabout: string, Station: string, Stop: string, Traffic_Calming: string, Traffic_Signal: string, Turning_Loop: string, Sunrise_Sunset: string, Civil_Twilight: string, Nautical_Twilight: string, Astronomical_Twilight: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.persist()\n",
    "printMissingValues(df,logger)\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove data with occurance less than 1%\n",
    "#Based on information from analysis. With further analysis lower procentage can be used to find better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   Weather_Condition|count|\n",
      "+--------------------+-----+\n",
      "|               Clear|44274|\n",
      "|                Fair|25503|\n",
      "|            Overcast|15699|\n",
      "|       Mostly Cloudy|15139|\n",
      "|       Partly Cloudy|10433|\n",
      "|    Scattered Clouds| 9870|\n",
      "|          Light Rain| 7139|\n",
      "|              Cloudy| 5475|\n",
      "|                null| 3079|\n",
      "|                Rain| 2650|\n",
      "|                 Fog| 1661|\n",
      "|          Heavy Rain| 1008|\n",
      "|                Haze|  780|\n",
      "|       Light Drizzle|  610|\n",
      "|             Drizzle|  317|\n",
      "|Thunder in the Vi...|  312|\n",
      "|      Patches of Fog|  303|\n",
      "|        Thunderstorm|  271|\n",
      "|Light Thunderstor...|  207|\n",
      "|             T-Storm|  199|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Rows removed: 4152\n"
     ]
    }
   ],
   "source": [
    "df.persist()\n",
    "original_rows = df.count()\n",
    "n = int(df.count()*0.001) # Limit the plot to ignore conditions below an limit\n",
    "\n",
    "weather_freq = df.groupBy('Weather_Condition').count().orderBy('count',ascending=False)\n",
    "df_filtered = weather_freq.filter(weather_freq['count'] > n)\n",
    "filtered_conditions = df_filtered.select(\"Weather_Condition\").rdd.flatMap(lambda x: x).collect()\n",
    "df = df.filter(df['Weather_Condition'].isin(*filtered_conditions))\n",
    "\n",
    "df_filtered.show()\n",
    "removed_rows = original_rows - df.count()\n",
    "\n",
    "df.unpersist()\n",
    "print(\"Rows removed:\",removed_rows)\n",
    "\n",
    "logger.write2file(\"Weather condition\", str(df_filtered.take(df_filtered.count())) + \"\\nRows removed: \" + str(removed_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove if distict values in category equals 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns with 1 class: ['Turning_Loop']\n"
     ]
    }
   ],
   "source": [
    "df.persist()\n",
    "tmp = [c for c in [*colCat, \"position\"] if df.select(countDistinct(c)).collect()[0][0] <= 1] \n",
    "df.unpersist()\n",
    "\n",
    "[colCat.remove(c) for c in tmp]\n",
    "print(\"Dropping columns with 1 class:\", tmp)\n",
    "logger.write2file(\"Dropping columns with 1 class\", str(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(inputCols=colNum, outputCols=colNum)\n",
    "imputer.setStrategy(\"median\")\n",
    "\n",
    "num_assembler = VectorAssembler(inputCols=colNum, outputCol=\"num_features\")\n",
    "scaler = MinMaxScaler(min=0.0, max=1.0, inputCol=\"num_features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "indexers = [StringIndexer(inputCol = c, outputCol = c +'_IDX', handleInvalid='skip') for c in colCat]\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[indexer.getOutputCol() for indexer in indexers], outputCols=[\"{0}_vec\".format(indexer.getOutputCol()) for indexer in indexers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical values\n",
    "\n",
    "numPipeline = Pipeline(stages=[imputer, num_assembler, scaler])\n",
    "catPipeline = Pipeline(stages=[*indexers, encoder])\n",
    "\n",
    "pipeline = Pipeline(stages=[numPipeline, catPipeline])\n",
    "\n",
    "preprocessed_df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+--------------------+---------------------+------------+----------------------+-------------------------+---------------+-------------+----------------+----------------+----------------+---------------+---------------+------------------+---------------+-------------+-----------------------+----------------------+----------------------+----------------------+-------------------------+-----------------------------+\n",
      "|position                |scaledFeatures                                                                                                              |Start_Hour_IDX_vec|Start_Month_IDX_vec|Weather_Hour_IDX_vec|Weather_Month_IDX_vec|Side_IDX_vec|Wind_Direction_IDX_vec|Weather_Condition_IDX_vec|Amenity_IDX_vec|Bump_IDX_vec |Crossing_IDX_vec|Give_Way_IDX_vec|Junction_IDX_vec|No_Exit_IDX_vec|Railway_IDX_vec|Roundabout_IDX_vec|Station_IDX_vec|Stop_IDX_vec |Traffic_Calming_IDX_vec|Traffic_Signal_IDX_vec|Sunrise_Sunset_IDX_vec|Civil_Twilight_IDX_vec|Nautical_Twilight_IDX_vec|Astronomical_Twilight_IDX_vec|\n",
      "+------------------------+----------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+--------------------+---------------------+------------+----------------------+-------------------------+---------------+-------------+----------------+----------------+----------------+---------------+---------------+------------------+---------------+-------------+-----------------------+----------------------+----------------------+----------------------+-------------------------+-----------------------------+\n",
      "|(198,[95,170],[1.0,1.0])|[2.9973323303180955E-5,0.7251851851851853,0.7870967741935484,0.8958333333333334,0.9655955858487505,0.08,0.07058343533251735]|(23,[3],[1.0])    |(11,[1],[1.0])     |(23,[2],[1.0])      |(11,[1],[1.0])       |(1,[],[])   |(23,[8],[1.0])        |(20,[10],[1.0])          |(1,[0],[1.0])  |(1,[0],[1.0])|(1,[0],[1.0])   |(1,[0],[1.0])   |(1,[0],[1.0])   |(1,[0],[1.0])  |(1,[0],[1.0])  |(1,[0],[1.0])     |(1,[0],[1.0])  |(1,[0],[1.0])|(1,[0],[1.0])          |(1,[0],[1.0])         |(1,[0],[1.0])         |(1,[0],[1.0])         |(1,[0],[1.0])            |(1,[0],[1.0])                |\n",
      "+------------------------+----------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+--------------------+---------------------+------------+----------------------+-------------------------+---------------+-------------+----------------+----------------+----------------+---------------+---------------+------------------+---------------+-------------+-----------------------+----------------------+----------------------+----------------------+-------------------------+-----------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df.select(\"position\",\"scaledFeatures\",*[c + \"_IDX_vec\" for c in colCat]).show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                       |label|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|(333,[95,170,198,199,200,201,202,203,204,208,229,241,263,282,307,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332],[1.0,1.0,2.9973323303180955E-5,0.7251851851851853,0.7870967741935484,0.8958333333333334,0.9655955858487505,0.08,0.07058343533251735,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|2    |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "Saving into: models/SC_20200512025158/df_features\n",
      "Saving through format 1\n",
      "Feature set size:  141982\n"
     ]
    }
   ],
   "source": [
    "va2 = VectorAssembler(inputCols=[\"position\",\"scaledFeatures\",*[c + \"_IDX_vec\" for c in colCat]], outputCol=\"final_features\")\n",
    "\n",
    "df = va2.transform(preprocessed_df)\n",
    "df.persist()\n",
    "df = df.withColumn('label', col(\"Severity\"))\n",
    "df_features = df.withColumn('features', df.final_features).select(\"features\",\"label\")\n",
    "df.unpersist()\n",
    "df_features.show(1, False)\n",
    "\n",
    "print(type(df_features))\n",
    "\n",
    "logger.write2fileModel(df_features, \"df_features\")\n",
    "\n",
    "logger.write2file(\"Feature set size\", str(df_features.count()) + \"\\n\\n__Feature vector and label:__\\n\" + str(df_features.take(1)))\n",
    "logger.write2file(\"Number of rows\", str(df_features.count()))\n",
    "print(\"Feature set size: \",df_features.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique column values: [Row(Start_Hour=24), Row(Start_Month=12), Row(Weather_Hour=24), Row(Weather_Month=12), Row(Side=2), Row(Wind_Direction=24), Row(Weather_Condition=21), Row(Amenity=2), Row(Bump=2), Row(Crossing=2), Row(Give_Way=2), Row(Junction=2), Row(No_Exit=2), Row(Railway=2), Row(Roundabout=2), Row(Station=2), Row(Stop=2), Row(Traffic_Calming=2), Row(Traffic_Signal=2), Row(Sunrise_Sunset=2), Row(Civil_Twilight=2), Row(Nautical_Twilight=2), Row(Astronomical_Twilight=2), Row(position=5538)]\n"
     ]
    }
   ],
   "source": [
    "# Checking how many classes that can be used\n",
    "df.persist()\n",
    "tmp = [df.select(countDistinct(c).alias(c)).collect()[0] for c in [*colCat, \"position\"]] \n",
    "df.unpersist()\n",
    "print(\"Unique column values:\", tmp)\n",
    "\n",
    "logger.write2file(\"Unique column values\", str(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "#In order to understand how much the variance affect the dataset we check with PCA. Try to get above 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA - Feature Variance: Top 50:\n",
      "[0.05766229 0.03530102 0.02757442 0.0256291  0.02532432 0.02461963\n",
      " 0.02438295 0.02273854 0.02122041 0.01998778 0.01843186 0.01785771\n",
      " 0.01713528 0.0167578  0.01634119 0.01592616 0.01538732 0.01529416\n",
      " 0.01491239 0.0142619  0.0137218  0.01306225 0.01261786 0.01210671\n",
      " 0.01160655 0.01121158 0.01078696 0.01064676 0.01004623 0.0098263\n",
      " 0.00884415 0.0086728  0.0082641  0.00813522 0.00739017 0.00713196\n",
      " 0.00700715 0.00687807 0.006521   0.00578096 0.00523802 0.00515462\n",
      " 0.00501049 0.00495673 0.00472208 0.00464995 0.00458716 0.0044969\n",
      " 0.00439275 0.00420986]\n",
      "Number of items: 250\n",
      "Sum of variance: 0.9734968859053912\n",
      "Saving into: models/SC_20200512025158/pca_df\n",
      "Saving through format 1\n"
     ]
    }
   ],
   "source": [
    "k=250\n",
    "pca = PCA(k=k, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df_features)\n",
    "pca_df = pca_model.transform(df_features)\n",
    "#chi_model[0].pca_features\n",
    "\n",
    "print(\"PCA - Feature Variance:\",\"Top 50:\\n\" + str(pca_model.explainedVariance[:50]) + \"\\nNumber of items: \"+str(k)+\"\\nSum of variance: \"+ str(sum(pca_model.explainedVariance)))\n",
    "logger.write2file(\"PCA - Feature variance\", \"Top 50:\\n\" + str(pca_model.explainedVariance[:50]) + \"\\nNumber of items: \"+ str(k) +\"\\nSum of variance: \"+ str(sum(pca_model.explainedVariance)))\n",
    "logger.write2fileModel(pca_df, \"pca_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChiSqSelector\n",
    "#Check top 100 which of the values in the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving into: models/SC_20200512025158/chi_df\n",
      "Saving through format 1\n",
      "Top selected features according to ChiSqSelector: [4, 7, 8, 13, 15, 16, 19, 23, 25, 27, 28, 29, 32, 38, 45, 47, 53, 58, 59, 61, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 76, 77, 79, 81, 84, 86, 88, 91, 93, 99, 101, 102, 104, 105, 107, 109, 114, 115, 120, 121, 124, 128, 129, 130, 132, 133, 134, 137, 142, 143, 146, 148, 149, 151, 152, 153, 155, 157, 158, 159, 160, 166, 169, 170, 172, 173, 174, 179, 180, 182, 183, 188, 193, 194, 195, 196, 198, 199, 200, 202, 203, 204, 224, 225, 226, 227, 258, 259, 260, 261, 273, 275, 298, 300, 301, 302, 317, 319, 321, 323, 325, 326, 328, 329, 330, 331, 332, 305, 110, 57, 122, 222, 87, 156, 201, 223, 136, 54, 106, 184, 255, 82, 126, 64, 307, 0, 144, 197, 312, 154, 140, 244, 211, 257, 304, 50, 62, 89, 34, 1, 178, 119, 44, 308, 108, 167, 56, 112, 22, 43, 74, 164, 9, 147, 254, 296, 90, 138, 139, 294, 297, 267, 233, 250, 246, 221, 218, 186, 98, 215, 209, 55, 36, 287, 12, 249, 220, 192, 285, 219, 31, 175, 240, 232, 266, 96, 30, 75, 252, 239, 161, 247, 189, 214, 42, 256, 314, 216, 40, 295, 303, 35, 210, 205, 116, 60, 83, 52, 292, 263, 229, 191, 208, 253, 17, 306, 316, 127, 207, 14, 85, 78, 125, 245, 315, 282, 272, 37, 238, 118, 322, 111, 6, 283, 251, 268, 26, 234, 299, 320]\n",
      "Transformed selected features: (250,[135,156,157,158,159,160,161,162,165,182,206,213,231,237,238,239,240,241,242,243,244,245,246,247,248,249],[1.0,2.9973323303180955e-05,0.7251851851851853,0.7870967741935484,0.8958333333333334,0.9655955858487505,0.08,0.07058343533251735,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
      "+--------------------+-----+--------------------+\n",
      "|            features|label|    selectedFeatures|\n",
      "+--------------------+-----+--------------------+\n",
      "|(333,[95,170,198,...|    2|(250,[135,156,157...|\n",
      "|(333,[81,181,198,...|    2|(250,[64,156,157,...|\n",
      "|(333,[11,137,198,...|    3|(250,[109,156,157...|\n",
      "|(333,[81,158,198,...|    2|(250,[64,127,156,...|\n",
      "|(333,[78,176,198,...|    2|(250,[62,156,157,...|\n",
      "|(333,[81,181,198,...|    2|(250,[64,156,157,...|\n",
      "|(333,[37,191,199,...|    2|(250,[27,149,157,...|\n",
      "|(333,[49,168,199,...|    2|(250,[157,158,159...|\n",
      "|(333,[3,161,198,1...|    3|(250,[130,156,157...|\n",
      "|(333,[13,125,199,...|    2|(250,[8,99,157,15...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Transformed feature vector: None\n"
     ]
    }
   ],
   "source": [
    "selector = ChiSqSelector(numTopFeatures=k, \n",
    "                         labelCol='label', \n",
    "                         featuresCol='features', \n",
    "                         outputCol=\"selectedFeatures\",\n",
    "                         selectorType='numTopFeatures', \n",
    "                         percentile=0.1, \n",
    "                         fpr=0.05, fdr=0.05, fwe=0.05)\n",
    "\n",
    "chi_model = selector.fit(df_features)\n",
    "chi_df = chi_model.transform(df_features)\n",
    "\n",
    "logger.write2file(\"Top selected features according to ChiSqSelector\", str(chi_model.selectedFeatures)+ \"\\nNumber of features: \" + str(k) + \"\\nExample data:\\n\"+str(chi_df.take(5)))\n",
    "logger.write2fileModel(chi_df, \"chi_df\")\n",
    "\n",
    "print(\"Top selected features according to ChiSqSelector:\", chi_model.selectedFeatures)\n",
    "print(\"Transformed selected features:\",chi_df.head().selectedFeatures)\n",
    "print(\"Transformed feature vector:\", chi_df.show(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the chi_df for training since it is reduced\n",
    "#trainSet, testSet = df_features.randomSplit([0.8 ,0.2], 1)\n",
    "trainSet, testSet = chi_df.randomSplit([0.8 ,0.2], 1)\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                                                                                                                                                    |label|selectedFeatures                                                                                                                                                                                                                                                                                   |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(333,[0,106,199,200,201,202,203,204,215,237,250,271,273,282,297,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332],[1.0,1.0,0.9251851851851852,0.7870967741935484,0.3958333333333333,0.9753326841934438,1.0,0.037535699714402286,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|2    |(250,[0,83,157,158,159,160,161,162,170,194,211,213,221,237,238,239,240,241,242,243,244,245,246,247,248,249],[1.0,1.0,0.9251851851851852,0.7870967741935484,0.3958333333333333,0.9753326841934438,1.0,0.037535699714402286,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(333,[0,107,198,199,200,201,202,203,204,218,230,252,264,273,295,306,317,318,319,320,321,322,323,324,325,326,327,328,331,332],[1.0,1.0,2.9973323303180955E-5,0.6518518518518519,0.7032258064516129,1.0,0.9594287568971114,0.05,0.012239902080783354,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])    |2    |(250,[0,84,156,157,158,159,160,161,162,172,196,211,219,230,237,238,239,240,241,242,243,244,245,248,249],[1.0,1.0,2.9973323303180955E-5,0.6518518518518519,0.7032258064516129,1.0,0.9594287568971114,0.05,0.012239902080783354,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])    |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainSet.show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model..\n",
      "Predicting on testSet..\n",
      "Evaluating predictions..\n",
      "Saving into: models/SC_20200512025158/LR_Model\n",
      "'function' object has no attribute 'format'\n",
      "Saving through format 2\n",
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       2.0|    2|(333,[0,113,198,1...|\n",
      "|       2.0|    2|(333,[0,113,199,2...|\n",
      "|       2.0|    2|(333,[0,163,199,2...|\n",
      "|       2.0|    2|(333,[0,185,199,2...|\n",
      "|       2.0|    2|(333,[3,112,199,2...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Accuracy:  0.7612227624648832 \n",
      "Parameters\n",
      " {Param(parent='LogisticRegression_3ee5d4c96c80', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent='LogisticRegression_3ee5d4c96c80', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='LogisticRegression_3ee5d4c96c80', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.81      0.98      0.89     22104\n",
      "           3       0.73      0.21      0.33      6210\n",
      "           4       0.00      0.00      0.00       175\n",
      "\n",
      "    accuracy                           0.81     28495\n",
      "   macro avg       0.39      0.30      0.30     28495\n",
      "weighted avg       0.79      0.81      0.76     28495\n",
      "\n",
      "Confusion matrix:\n",
      "[[    0     6     0     0]\n",
      " [    0 21665   439     0]\n",
      " [    0  4880  1330     0]\n",
      " [    0   130    45     0]]\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"selectedFeatures\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.1,0.01]) \\\n",
    "                .addGrid(lr.maxIter, [10]) \\\n",
    "                .addGrid(lr.elasticNetParam, [0.6]) \\\n",
    "                .build()\n",
    "try: \n",
    "    model, _ = evaluateModel(lr, paramGrid, \"LR_Model\", trainSet, testSet, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logger=logger)\n",
    "except Exception as e:\n",
    "    logger.write2file(\"Error:\", str(e))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model..\n",
      "Predicting on testSet..\n",
      "Evaluating predictions..\n",
      "Saving into: models/SC_20200512025158/DT_Model\n",
      "'function' object has no attribute 'format'\n",
      "Saving through format 2\n",
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       2.0|    2|(333,[0,113,198,1...|\n",
      "|       2.0|    2|(333,[0,113,199,2...|\n",
      "|       2.0|    2|(333,[0,163,199,2...|\n",
      "|       2.0|    2|(333,[0,185,199,2...|\n",
      "|       2.0|    2|(333,[3,112,199,2...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Accuracy:  0.7510268211255935 \n",
      "Parameters\n",
      " {}\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.80      0.99      0.89     22104\n",
      "           3       0.81      0.18      0.29      6210\n",
      "           4       0.00      0.00      0.00       175\n",
      "\n",
      "    accuracy                           0.80     28495\n",
      "   macro avg       0.40      0.29      0.29     28495\n",
      "weighted avg       0.80      0.80      0.75     28495\n",
      "\n",
      "Confusion matrix:\n",
      "[[    0     6     0     0]\n",
      " [    0 21850   253     1]\n",
      " [    0  5122  1088     0]\n",
      " [    0   171     4     0]]\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"selectedFeatures\") \n",
    "\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "_, _ = evaluateModel(dt, paramGrid, \"DT_Model\", trainSet, testSet, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model..\n",
      "Predicting on testSet..\n",
      "Evaluating predictions..\n",
      "Saving into: models/SC_20200512025158/RF_Model\n",
      "'function' object has no attribute 'format'\n",
      "Saving through format 2\n",
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       2.0|    2|(333,[0,113,198,1...|\n",
      "|       2.0|    2|(333,[0,113,199,2...|\n",
      "|       2.0|    2|(333,[0,163,199,2...|\n",
      "|       2.0|    2|(333,[0,185,199,2...|\n",
      "|       2.0|    2|(333,[3,112,199,2...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Accuracy:  0.6777369194606085 \n",
      "Parameters\n",
      " {Param(parent='RandomForestClassifier_00d454841bb0', name='numTrees', doc='Number of trees to train (>= 1).'): 15}\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.78      1.00      0.87     22104\n",
      "           3       0.00      0.00      0.00      6210\n",
      "           4       0.00      0.00      0.00       175\n",
      "\n",
      "    accuracy                           0.78     28495\n",
      "   macro avg       0.19      0.25      0.22     28495\n",
      "weighted avg       0.60      0.78      0.68     28495\n",
      "\n",
      "Confusion matrix:\n",
      "[[    0     6     0     0]\n",
      " [    0 22104     0     0]\n",
      " [    0  6210     0     0]\n",
      " [    0   175     0     0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"selectedFeatures\") #numTrees=10\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(rf.numTrees,[15]) \\\n",
    "                .build()\n",
    "\n",
    "_, _ = evaluateModel(rf, paramGrid, \"RF_Model\", trainSet, testSet, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.write2file(\"Program finished!\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.write2file(\"New model folder created\", \\\n",
    "               \"Model finished: Yes\" + \\\n",
    "               \"\\nFolder name: \" + logger.timeSignature + \\\n",
    "               \"\\nState: \"+ filter_state + \\\n",
    "               \"\\nLogs directory: \" + logger.logs_dir +  \\\n",
    "               \"\\nFile: \" + file + \\\n",
    "               \"\\nNote: \" + model_Note, \\\n",
    "               logs_dir=model_indexing_dir \\\n",
    "              )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
