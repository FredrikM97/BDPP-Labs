{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport ibmos2spark\\n# @hidden_cell\\ncredentials = {\\n    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\\n    'service_id': 'iam-ServiceId-1d71bac5-a341-449f-9ca7-b29de95e771a',\\n    'iam_service_endpoint': 'https://iam.eu-gb.bluemix.net/oidc/token',\\n    'api_key': 'ejujKveBRS3Bk7l3pzjemEQDhTSaOKmqCg6x6osXWkx4'\\n}\\n\\nconfiguration_name = 'os_2d970470a6354234a1716fe9f4db519b_configs'\\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\\n\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder.getOrCreate()\\n\\ndf = spark.read  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')  .option('header', 'true')  .load(cos.url('US_Accidents_Dec19.csv', 'bdppproject-donotdelete-pr-nejualq57kuqe4'))\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-1d71bac5-a341-449f-9ca7-b29de95e771a',\n",
    "    'iam_service_endpoint': 'https://iam.eu-gb.bluemix.net/oidc/token',\n",
    "    'api_key': 'ejujKveBRS3Bk7l3pzjemEQDhTSaOKmqCg6x6osXWkx4'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_2d970470a6354234a1716fe9f4db519b_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('US_Accidents_Dec19.csv', 'bdppproject-donotdelete-pr-nejualq57kuqe4'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    .config(\"spark.memory.fraction\",\"0.6\")     .config(\"spark.memory.storageFraction\",\"0.5\")    .config(\"spark.memory.offHeap.enabled\",True)     .config(\"spark.memory.offHeap.size\",\"4g\")      .config(\"spark.executor.heartbeatInterval\", \"10000s\")      .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")     .config(\"spark.executor.extraJavaOptions\",\"                 -XX:+UseG1GC                 -XX:+PrintFlagsFinal                 -XX:+PrintReferenceGC                 -verbose:gc                 -XX:+PrintGCDetails                 -XX:+PrintGCTimeStamps                 -XX:+PrintAdaptiveSizePolicy                 -XX:+UnlockDiagnosticVMOptions                 -XX:+G1SummarizeConcMark                 -Xms13g                 -Xmx13g                 -XX:InitiatingHeapOccupancyPercent=35                 -XX:ConcGCThread=20                 \") '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    .config(\"spark.memory.fraction\",\"0.6\") \\\n",
    "    .config(\"spark.memory.storageFraction\",\"0.5\")\\\n",
    "    .config(\"spark.memory.offHeap.enabled\",True) \\\n",
    "    .config(\"spark.memory.offHeap.size\",\"4g\")  \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"10000s\")  \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\",\" \\\n",
    "                -XX:+UseG1GC \\\n",
    "                -XX:+PrintFlagsFinal \\\n",
    "                -XX:+PrintReferenceGC \\\n",
    "                -verbose:gc \\\n",
    "                -XX:+PrintGCDetails \\\n",
    "                -XX:+PrintGCTimeStamps \\\n",
    "                -XX:+PrintAdaptiveSizePolicy \\\n",
    "                -XX:+UnlockDiagnosticVMOptions \\\n",
    "                -XX:+G1SummarizeConcMark \\\n",
    "                -Xms13g \\\n",
    "                -Xmx13g \\\n",
    "                -XX:InitiatingHeapOccupancyPercent=35 \\\n",
    "                -XX:ConcGCThread=20 \\\n",
    "                \") \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "try:\n",
    "    spark.close()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    conf = SparkConf().setAppName('Spark Project')\n",
    "    conf.set('executor-memory','6')\n",
    "    conf.set('spark.driver.memory', '8g')\n",
    "    conf.set('spark.executor.cores', '4')\n",
    "    conf.set(\"spark.num.executors\",\"6\")\n",
    "    conf.set('spark.memory.fraction','.6')\n",
    "    conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "#.config(\"spark.executor.cores\", \"4\") \\\n",
    "#.config(\"spark.default.parallelism\", \"6\") \\ # No more than availible Cores\n",
    "#.config(\"spark.executor.memoryOverhead\",200) \\\n",
    "# spark.num.executors is set since we dont have enough ram for 8 \n",
    "# Note: 6 running processes each having 2gb => 12 gb ram used\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Project\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "df = spark.read.load(\"data/US_Accidents_Dec19.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://97901d576ca5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Spark Project>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', 'localhost'),\n",
       " ('spark.app.id', 'local-1588449089354'),\n",
       " ('spark.app.name', 'Spark Project'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('executor-memory', '6'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
       " ('spark.num.executors', '6'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.memory', '8g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.memory.fraction', '.6'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '46099')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, count, col, isnan, countDistinct,from_unixtime,from_utc_timestamp, unix_timestamp,split, to_timestamp, hour, month, lit,collect_list\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Street', 'Side', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone', 'Airport_Code', 'Wind_Direction', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
      "['TMC', 'Start_Lat', 'Start_Lng', 'Distance(mi)', 'Number', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)']\n"
     ]
    }
   ],
   "source": [
    "colLabel = [\"Severity\"]\n",
    "\n",
    "colRem = ['ID', \n",
    "          'Source',\n",
    "          'End_Time',\n",
    "          'End_Lat',\n",
    "          'End_Lng',\n",
    "          'Description',\n",
    "        ]\n",
    "\n",
    "# Dropping data that cant help during model\n",
    "df = df.drop(*colRem)\n",
    "\n",
    "# Convert boolean to string since PCA cant handle boolean which should be a class\n",
    "df = df.select(*[col(c[0]).cast(\"string\").alias(c[0]) if c[1] == 'boolean' else col(c[0]).alias(c[0]) for c in df.dtypes])\n",
    "\n",
    "#renamedHousing.select([count(when(col(c).isNull(), c)).alias(c) for c in colNum]).show()\n",
    "rdd = sc.parallelize(df.dtypes)\n",
    "\n",
    "colCat = rdd.map(lambda i: i[0] if (i[1]=='string' or i[1]=='boolean' and i[0]) else None).filter(lambda i: i != None).collect()\n",
    "colNum = rdd.map(lambda i: i[0] if (i[1]=='double' and i[0]) else None).filter(lambda i: i != None).collect()\n",
    "\n",
    "print(colCat)\n",
    "print(colNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler,OneHotEncoderEstimator,QuantileDiscretizer, StringIndexer, Imputer,StandardScaler,MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse single city\n",
    "This could be CA since it stands for a little less than 50% of the total set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.filter(df.State == \"CA\") # Lowers the dataset quite a lot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify time\n",
    "Convert the timestamp into a numeric value and then into a string so that the time of day and month can be categorised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+---------+----------+------------+------+------+----+------+----------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "|  TMC|Severity|         Start_Time|Start_Lat| Start_Lng|Distance(mi)|Number|Street|Side|  City|    County|State|Zipcode|Country|  Timezone|Airport_Code|  Weather_Timestamp|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Amenity| Bump|Crossing|Give_Way|Junction|No_Exit|Railway|Roundabout|Station| Stop|Traffic_Calming|Traffic_Signal|Turning_Loop|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|Start_Hour|Start_Month|Weather_Hour|Weather_Month|\n",
      "+-----+--------+-------------------+---------+----------+------------+------+------+----+------+----------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "|201.0|       3|2016-02-08 05:46:00|39.865147|-84.058723|        0.01|  null|I-70 E|   R|Dayton|Montgomery|   OH|  45424|     US|US/Eastern|        KFFO|2016-02-08 05:58:00|          36.9|         null|       91.0|       29.68|          10.0|          Calm|           null|             0.02|       Light Rain|  false|false|   false|   false|   false|  false|  false|     false|  false|false|          false|         false|       false|         Night|         Night|            Night|                Night|         0|          2|           0|            2|\n",
      "+-----+--------+-------------------+---------+----------+------------+------+------+----+------+----------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to int then cast to string\n",
    "\n",
    "df = df.withColumn('Start_Hour', hour(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "df = df.withColumn('Start_Month', month(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "df = df.withColumn('Weather_Hour', hour(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "df = df.withColumn('Weather_Month', month(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantiles function\n",
    "Alternative solution to get position \n",
    "Remove City, Country, State, Zipcode, Airport_Code\n",
    "In order to reduce dimensionality feature crossing will be applied upon the Start_Lat and Start_Lng then fused in order to create a new set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Calculate how many buckets needed (Bins)\n",
    "discretizer_Lat = QuantileDiscretizer(numBuckets=100, inputCol=\"Start_Lat\", outputCol=\"Start_Lat_disc\")\n",
    "discretizer_Lng = QuantileDiscretizer(numBuckets=100, inputCol=\"Start_Lng\", outputCol=\"Start_Lng_disc\")\n",
    "\n",
    "# Into categorical values\n",
    "indexer_cord = [StringIndexer(inputCol=c + \"_disc\", outputCol=c+\"_IDX\") for c in [\"Start_Lat\",\"Start_Lng\"]]\n",
    "\n",
    "# One-hot (feature crossing)\n",
    "encoder_cord = OneHotEncoderEstimator(inputCols=[indexer.getOutputCol() for indexer in indexer_cord], outputCols=[\"{0}_vec\".format(indexer.getOutputCol()) for indexer in indexer_cord])\n",
    "\n",
    "pipeline_cord = Pipeline(stages=[discretizer_Lat,discretizer_Lng, *indexer_cord, encoder_cord])\n",
    "preprocessed_cord = pipeline_cord.fit(df).transform(df)\n",
    "\n",
    "position = VectorAssembler(inputCols=[\"Start_Lat_IDX_vec\",\"Start_Lng_IDX_vec\"], outputCol=\"position\")\n",
    "\n",
    "df = position.transform(preprocessed_cord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+---------+----------+------------+------+------+----+------+----------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+--------------+--------------+-------------+-------------+-----------------+-----------------+--------------------+\n",
      "|  TMC|Severity|         Start_Time|Start_Lat| Start_Lng|Distance(mi)|Number|Street|Side|  City|    County|State|Zipcode|Country|  Timezone|Airport_Code|  Weather_Timestamp|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Amenity| Bump|Crossing|Give_Way|Junction|No_Exit|Railway|Roundabout|Station| Stop|Traffic_Calming|Traffic_Signal|Turning_Loop|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|Start_Hour|Start_Month|Weather_Hour|Weather_Month|Start_Lat_disc|Start_Lng_disc|Start_Lat_IDX|Start_Lng_IDX|Start_Lat_IDX_vec|Start_Lng_IDX_vec|            position|\n",
      "+-----+--------+-------------------+---------+----------+------------+------+------+----+------+----------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+--------------+--------------+-------------+-------------+-----------------+-----------------+--------------------+\n",
      "|201.0|       3|2016-02-08 05:46:00|39.865147|-84.058723|        0.01|  null|I-70 E|   R|Dayton|Montgomery|   OH|  45424|     US|US/Eastern|        KFFO|2016-02-08 05:58:00|          36.9|         null|       91.0|       29.68|          10.0|          Calm|           null|             0.02|       Light Rain|  false|false|   false|   false|   false|  false|  false|     false|  false|false|          false|         false|       false|         Night|         Night|            Night|                Night|         0|          2|           0|            2|          71.0|          62.0|         38.0|         47.0|  (99,[38],[1.0])|  (99,[47],[1.0])|(198,[38,146],[1....|\n",
      "+-----+--------+-------------------+---------+----------+------------+------+------+----+------+----------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+--------------+--------------+-------------+-------------+-----------------+-----------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast data \n",
    "Cast all the datacolumns into correct format so they will be sorted to numerical or categorical values. Data removed caused to big feature vector, contained too many NaN values or were converted into another form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\n",
    "        #col('TMC').cast('string'),  Lot of missing values!!\n",
    "        col('Severity').cast('int'),\n",
    "        #col('Start_Time').cast('int'),\n",
    "        col('Start_Hour').cast('string'),\n",
    "        col('Start_Month').cast('string'),\n",
    "        col('Weather_Hour').cast('string'),\n",
    "        col('Weather_Month').cast('string'),\n",
    "        col('position'), # already preprocessed\n",
    "        col('Distance(mi)').cast('double'),\n",
    "        col('Side').cast('string'),         # Remove eventually?\n",
    "        col('Temperature(F)').cast('double'),\n",
    "        col('Wind_Chill(F)').cast('double'), #  Lot of missing values!!\n",
    "        col('Humidity(%)').cast('double'),\n",
    "        col('Pressure(in)').cast('double'),\n",
    "        col('Visibility(mi)').cast('double'),\n",
    "        col('Wind_Direction').cast('string'),\n",
    "        col('Wind_Speed(mph)').cast('double'),  #Lot of missing values!!\n",
    "        col('Weather_Condition').cast('string'),\n",
    "        col('Amenity').cast('string'),\n",
    "        col('Bump').cast('string'),\n",
    "        col('Crossing').cast('string'),\n",
    "        col('Give_Way').cast('string'),\n",
    "        col('Junction').cast('string'),\n",
    "        col('No_Exit').cast('string'),\n",
    "        col('Railway').cast('string'),\n",
    "        col('Roundabout').cast('string'),\n",
    "        col('Station').cast('string'),\n",
    "        col('Stop').cast('string'),\n",
    "        col('Traffic_Calming').cast('string'),\n",
    "        col('Traffic_Signal').cast('string'),\n",
    "        col('Turning_Loop').cast('string'),\n",
    "        col('Sunrise_Sunset').cast('string'),\n",
    "        col('Civil_Twilight').cast('string'),\n",
    "        col('Nautical_Twilight').cast('string'),\n",
    "        col('Astronomical_Twilight').cast('string')\n",
    "    ) \n",
    "\n",
    "# Since the last check can be unorganised we recreate a new list that contains all data\n",
    "#df.cache()  # TODO does this work? - Heatbeat crash otherwise\n",
    "colLabel = [\"Severity\"]\n",
    "\n",
    "rdd = sc.parallelize(df.dtypes)\n",
    "colCat = rdd.map(lambda i: i[0] if (i[1]=='string' or i[1]=='boolean' and i[0] not in colLabel) else None).filter(lambda i: i != None).collect()\n",
    "colNum = rdd.map(lambda i: i[0] if (i[1]=='double' and i[0] not in colLabel) else None).filter(lambda i: i != None).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Severity']\n",
      "['Start_Hour', 'Start_Month', 'Weather_Hour', 'Weather_Month', 'Side', 'Wind_Direction', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
      "['Distance(mi)', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)']\n"
     ]
    }
   ],
   "source": [
    "print(colLabel)\n",
    "print(colCat)\n",
    "print(colNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recheck the missing values\n",
    "Check so that the output contains 0 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Severity=0, Start_Hour=3163, Start_Month=3163, Weather_Hour=36705, Weather_Month=36705, position=0, Distance(mi)=0, Side=0, Temperature(F)=56063, Wind_Chill(F)=1852623, Humidity(%)=59173, Pressure(in)=48142, Visibility(mi)=65691, Wind_Direction=45101, Wind_Speed(mph)=440840, Weather_Condition=65932, Amenity=0, Bump=0, Crossing=0, Give_Way=0, Junction=0, No_Exit=0, Railway=0, Roundabout=0, Station=0, Stop=0, Traffic_Calming=0, Traffic_Signal=0, Turning_Loop=0, Sunrise_Sunset=93, Civil_Twilight=93, Nautical_Twilight=93, Astronomical_Twilight=93)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_missingVals_cols = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "# Missing value in each column\n",
    "df_missingVals_cols.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop NaN\n",
    "Cant drop NaN since imputer should take care of the most of them! However we can drop values from the categorical set if we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old = df.count()\n",
    "#df = df.na.drop()\n",
    "#print(\"Rows removed:\",old-df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recheck the missing values\n",
    "Check so that the output contains 0 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missingVals_cols = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "# Missing value in each column\n",
    "df_missingVals_cols.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove data with occurance less than 1%\n",
    "Based on information from analysis. With further analysis lower procentage can be used to find better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_rows = df.count()\n",
    "weather_freq = df.groupBy('Weather_Condition').count().orderBy('count',ascending=False)\n",
    "\n",
    "n = int(df.count()*0.001) # Limit the plot to ignore conditions below an limit\n",
    "df_filtered = weather_freq.filter(weather_freq['count'] > n)\n",
    "\n",
    "filtered_conditions = df_filtered.select(\"Weather_Condition\").rdd.flatMap(lambda x: x).collect()\n",
    "df = df.filter(df['Weather_Condition'].isin(*filtered_conditions))\n",
    "\n",
    "df_filtered.show()\n",
    "print(filtered_conditions)\n",
    "print(\"Rows removed:\",original_rows - df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(inputCols=colNum, outputCols=colNum)\n",
    "imputer.setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_assembler = VectorAssembler(inputCols=colNum, outputCol=\"num_features\",handleInvalid=\"keep\")\n",
    "#scaler = StandardScaler(inputCol=\"num_features\", withMean=True, withStd=True, outputCol=\"scaledFeatures\")\n",
    "scaler = MinMaxScaler(min=0.0, max=1.0, inputCol=\"num_features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol = c, outputCol = c +'_IDX', handleInvalid='keep') for c in colCat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoderEstimator(inputCols=[indexer.getOutputCol() for indexer in indexers], outputCols=[\"{0}_vec\".format(indexer.getOutputCol()) for indexer in indexers],handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical values\n",
    "\n",
    "numPipeline = Pipeline(stages=[imputer, num_assembler, scaler])\n",
    "catPipeline = Pipeline(stages=[*indexers, encoder])\n",
    "\n",
    "pipeline = Pipeline(stages=[numPipeline, catPipeline])\n",
    "preprocessed_df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df.select(\"position\",\"scaledFeatures\",*[c + \"_IDX_vec\" for c in colCat]).show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va2 = VectorAssembler(inputCols=[\"position\",\"scaledFeatures\",*[c + \"_IDX_vec\" for c in colCat]], outputCol=\"final_features\")\n",
    "\n",
    "df = va2.transform(preprocessed_df)\n",
    "\n",
    "df = df.withColumn('label', col(\"Severity\"))\n",
    "df_features = df.withColumn('features', df.final_features).select(\"features\",\"label\")\n",
    "df_features.show(1, False)\n",
    "print(\"Feature set size:\",df_features.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how many classes that can be used\n",
    "tmp = [df.select(countDistinct(c).alias(c)).collect()[0] for c in [*colCat, \"position\"]] \n",
    "print(\"Unique column values:\", tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.feature import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChiSqSelector\n",
    "Check top 50 which of the values in the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ChiSqSelector(numTopFeatures=50, \n",
    "                         labelCol='label', \n",
    "                         featuresCol='features', \n",
    "                         outputCol=\"selectedFeatures\",\n",
    "                         selectorType='numTopFeatures', \n",
    "                         percentile=0.1, \n",
    "                         fpr=0.05, fdr=0.05, fwe=0.05)\n",
    "\n",
    "model = selector.fit(df_features)\n",
    "chi_model = model.transform(df_features)\n",
    "chi_model.head().selectedFeatures\n",
    "\n",
    "print(\"Top selected features according to ChiSqSelector:\", model.selectedFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "In order to understand how much the variance affect the dataset we check with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=50, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "model = pca.fit(df_features)\n",
    "chi_model = model.transform(df_features)\n",
    "\n",
    "chi_model[0].pca_features\n",
    "\n",
    "model.explainedVariance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier,RandomForestClassifier,GBTClassifier,MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, testSet = df_features.randomSplit([0.8 ,0.2], 1)\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet.show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.1]) \\\n",
    "                .addGrid(lr.maxIter, [10]) \\\n",
    "                .addGrid(lr.elasticNetParam, [0.6]) \\\n",
    "                .build()\n",
    "                #.addGrid(lr.regParam, [0.1, 0.01,0.02]) \\\n",
    "                #.addGrid(lr.maxIter, [5,10,15]) \\\n",
    "                #.addGrid(lr.elasticNetParam, [0.6,0.8]) \\\n",
    "cv =  CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=10, seed=None, parallelism=8) \n",
    "cvModel_lr = cv.fit(trainSet)\n",
    "\n",
    "predictions = cvModel_lr.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "trainSummary = cvModel_lr.bestModel.summary\n",
    "# TODO: What was the best parameters?\n",
    "print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_lr.getEstimatorParamMaps()[ argmax(cvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_lr.save(\"/Models/LR_Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\") \n",
    "\n",
    "paramGrid = ParamGridBuilder() .build()\n",
    "\n",
    "cv =  CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=10, seed=None, parallelism=8) \n",
    "cvModel_dt = cv.fit(trainSet)\n",
    "\n",
    "predictions = cvModel_dt.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_dt.getEstimatorParamMaps()[ argmax(cvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\") #numTrees=10\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(rf.numTrees,[10,15,20,25]) \\\n",
    "                .build()\n",
    "\n",
    "cv =  CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=10, seed=None, parallelism=8) \n",
    "cvModel_rf = cv.fit(trainSet)\n",
    "\n",
    "predictions = cvModel_rf.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_rf.getEstimatorParamMaps()[ argmax(cvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP (Not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp = MultilayerPerceptronClassifier(maxIter=100, blockSize=128, layers=[4, 5, 4, 3], seed=1234) #Layers=[]\n",
    "#paramGrid = ParamGridBuilder().build()\n",
    "                #.addGrid(gbt.maxIter, [10,15,20]) \\\n",
    "                \n",
    "\n",
    "#cv =  CrossValidator(estimator=mlp, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=10, seed=42) \n",
    "#cvModel = cv.fit(trainSet)\n",
    "\n",
    "#predictions = cvModel.transform(testSet)\n",
    "#predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "#accuracy = evaluator.evaluate(predictions)\n",
    "#print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning of best model\n",
    "\n",
    "Since there is no predefined way to test multiple algorithms this is used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = createParams(models)\n",
    "#bestModels = evaluateAlgorithm(models,trainSet)\n",
    "#finalModel = getBestModel(bestModels,testSet)\n",
    "\n",
    "#prediction = makePredictions(finalModel, testSet)\n",
    "trainingSummary = finalModel.summary\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "\n",
    "\n",
    "#trainingSummary = cvModel.bestModel.stages[-1].summary\n",
    "#selected = prediction.select(\"label\", \"probability\", \"prediction\")\n",
    "#selected.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finalModel = getBestModel(bestModels,testSet)\n",
    "#print(finalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "falsePositiveRateLabel = trainingSummary.falsePositiveRateByLabel\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "iterations = trainingSummary.totalIterations\n",
    "\n",
    "print(\"Iterations: %s\\nAccuracy: %s\\nFPR: %s\\nTPR: %s\\nFPRL: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (iterations,accuracy, falsePositiveRate, truePositiveRate, falsePositiveRateLabel, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_true = prediction.select(['label']).collect()\n",
    "y_pred = prediction.select(['prediction']).collect()\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO try streaming (online-learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
