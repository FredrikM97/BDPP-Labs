{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport ibmos2spark\\n# @hidden_cell\\ncredentials = {\\n    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\\n    'service_id': 'iam-ServiceId-1d71bac5-a341-449f-9ca7-b29de95e771a',\\n    'iam_service_endpoint': 'https://iam.eu-gb.bluemix.net/oidc/token',\\n    'api_key': 'ejujKveBRS3Bk7l3pzjemEQDhTSaOKmqCg6x6osXWkx4'\\n}\\n\\nconfiguration_name = 'os_2d970470a6354234a1716fe9f4db519b_configs'\\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\\n\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder.getOrCreate()\\n\\ndf = spark.read  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')  .option('header', 'true')  .load(cos.url('US_Accidents_Dec19.csv', 'bdppproject-donotdelete-pr-nejualq57kuqe4'))\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-1d71bac5-a341-449f-9ca7-b29de95e771a',\n",
    "    'iam_service_endpoint': 'https://iam.eu-gb.bluemix.net/oidc/token',\n",
    "    'api_key': 'ejujKveBRS3Bk7l3pzjemEQDhTSaOKmqCg6x6osXWkx4'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_2d970470a6354234a1716fe9f4db519b_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('US_Accidents_Dec19.csv', 'bdppproject-donotdelete-pr-nejualq57kuqe4'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from datetime import datetime\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler,OneHotEncoderEstimator,QuantileDiscretizer, StringIndexer, Imputer,StandardScaler,MinMaxScaler\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import ChiSqSelector,PCA\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier,RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_indexing_dir = \"log_index.md\"\n",
    "models_dir = \"models\"\n",
    "logs_dir = \"models\" \n",
    "filter_state = 'ALL' # ALL if all states\n",
    "model_Note = ''\n",
    "log_mode=True # Logs data to files, Dont use on massive dataset (debug_mode is recommended while on)\n",
    "debug_mode=True # Reduce number of rows collected for faster processing\n",
    "enable_plots = True # Dont use unless debug_mode is on, or datasize is small\n",
    "logger = logging(models_dir, logs_dir, \"\", filter_state, enabled=log_mode)\n",
    "\n",
    "file = \"data/US_Accidents_Dec19.csv\"\n",
    "\n",
    "\n",
    "df, sc,spark = setup_spark(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4082e07eadd9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Spark Project>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: [('spark.driver.memory', '4g'), ('spark.executor.memory', '4g'), ('spark.driver.host', '4082e07eadd9'), ('spark.ui.enabled', 'true'), ('spark.executor.id', 'driver'), ('spark.app.name', 'Spark Project'), ('spark.app.id', 'local-1589641872957'), ('spark.ui.killEnabled', 'false'), ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'), ('spark.driver.port', '46471'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.executor.instances', '1'), ('spark.master', 'local[*]'), ('spark.executor.cores', '1'), ('spark.submit.deployMode', 'client'), ('spark.kryoserializer.buffer.max', '15'), ('spark.ui.showConsoleProgress', 'true'), ('spark.driver.cores', '1')]\n"
     ]
    }
   ],
   "source": [
    "tmp = sc._conf.getAll()\n",
    "logger.write2file(\"New Spark session\", str(tmp))\n",
    "print(\"Config:\",tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2925212"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if debug_mode == True: \n",
    "    df.createOrReplaceTempView(\"records\")\n",
    "    reviewData = spark.sql(\"SELECT * FROM records LIMIT 100000\") \n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: ['Severity'] \n",
      "Categories: ['Street', 'Side', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone', 'Airport_Code', 'Wind_Direction', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
      "Numerical: ['TMC', 'Start_Lat', 'Start_Lng', 'Distance(mi)', 'Number', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)']\n"
     ]
    }
   ],
   "source": [
    "colLabel = [\"Severity\"]\n",
    "\n",
    "colRem = ['ID', \n",
    "          'Source',\n",
    "          'End_Time',\n",
    "          'End_Lat',\n",
    "          'End_Lng',\n",
    "          'Description',\n",
    "        ]\n",
    "\n",
    "df, colCat, colNum = setup_variables(df, sc, colLabel, colRem)  \n",
    "\n",
    "logger.write2file(\"Number of rows\", str(df.count()))\n",
    "logger.write2file(\"Categorical groups\",\"Defined Label:\\n\" + str(colLabel) + \"\\nDefined Categories:\\n\" + str(colCat) + \"\\nDefined Numerical:\\n\" +str(colNum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse single state\n",
    "#Create a model for separates states instead of all states at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_state != 'ALL':\n",
    "    df = df.filter(df.State == filter_state) # Lowers the dataset quite a lot\n",
    "    logger.write2file(\"Specified state\",str(filter_state))\n",
    "else:\n",
    "    logger.write2file(\"No state specified\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify time\n",
    "#Convert the timestamp into a numeric value and then into a string so that the time of day and month can be categorised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+---------+----------+------------+------+------+----+------+----------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "|  TMC|Severity|         Start_Time|Start_Lat| Start_Lng|Distance(mi)|Number|Street|Side|  City|    County|State|Zipcode|Country|  Timezone|Airport_Code|  Weather_Timestamp|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Amenity| Bump|Crossing|Give_Way|Junction|No_Exit|Railway|Roundabout|Station| Stop|Traffic_Calming|Traffic_Signal|Turning_Loop|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|Start_Hour|Start_Month|Weather_Hour|Weather_Month|\n",
      "+-----+--------+-------------------+---------+----------+------------+------+------+----+------+----------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "|201.0|       3|2016-02-08 05:46:00|39.865147|-84.058723|        0.01|  null|I-70 E|   R|Dayton|Montgomery|   OH|  45424|     US|US/Eastern|        KFFO|2016-02-08 05:58:00|          36.9|         null|       91.0|       29.68|          10.0|          Calm|           null|             0.02|       Light Rain|  false|false|   false|   false|   false|  false|  false|     false|  false|false|          false|         false|       false|         Night|         Night|            Night|                Night|         0|          2|           0|            2|\n",
      "+-----+--------+-------------------+---------+----------+------------+------+------+----+------+----------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to int then cast to string\n",
    "\n",
    "df = df.withColumn('Start_Hour', hour(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "df = df.withColumn('Start_Month', month(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "df = df.withColumn('Weather_Hour', hour(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "df = df.withColumn('Weather_Month', month(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "df.show(1)\n",
    "logger.write2file(\"Dataset after modifying UTC timestamp\", str(df.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster quantiles to reduce the feature vector for position\n",
    "#Since feature crossing wont work due to the feature vector is to big (regarding to the given resources) we hope that Clustering might reduce them and keep as much data as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.5938803358205699\n",
      "+--------+\n",
      "|position|\n",
      "+--------+\n",
      "|     112|\n",
      "|      38|\n",
      "|      77|\n",
      "|     112|\n",
      "|     112|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number_of_clusters= 400\n",
    "pos_assembler = VectorAssembler(inputCols=['Start_Lat', 'Start_Lng'], outputCol=\"position_features\")\n",
    "preprocessed_cord = pos_assembler.transform(df)\n",
    "\n",
    "trainSet, testSet = preprocessed_cord.randomSplit([0.9 ,0.1], 1)\n",
    "kmeans = KMeans(k=number_of_clusters, featuresCol='position_features', predictionCol='position')\n",
    "model = kmeans.fit(trainSet)\n",
    "predictions = model.transform(testSet)\n",
    "\n",
    "evaluator = ClusteringEvaluator(featuresCol='position_features', predictionCol='position')\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Transform the hole dataset\n",
    "df = model.transform(preprocessed_cord)\n",
    "df.select('position').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+---------+----------+------------+------+------+----+------+----------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+--------------------+--------+\n",
      "|  TMC|Severity|         Start_Time|Start_Lat| Start_Lng|Distance(mi)|Number|Street|Side|  City|    County|State|Zipcode|Country|  Timezone|Airport_Code|  Weather_Timestamp|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Amenity| Bump|Crossing|Give_Way|Junction|No_Exit|Railway|Roundabout|Station| Stop|Traffic_Calming|Traffic_Signal|Turning_Loop|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|Start_Hour|Start_Month|Weather_Hour|Weather_Month|   position_features|position|\n",
      "+-----+--------+-------------------+---------+----------+------------+------+------+----+------+----------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+--------------------+--------+\n",
      "|201.0|       3|2016-02-08 05:46:00|39.865147|-84.058723|        0.01|  null|I-70 E|   R|Dayton|Montgomery|   OH|  45424|     US|US/Eastern|        KFFO|2016-02-08 05:58:00|          36.9|         null|       91.0|       29.68|          10.0|          Calm|           null|             0.02|       Light Rain|  false|false|   false|   false|   false|  false|  false|     false|  false|false|          false|         false|       false|         Night|         Night|            Night|                Night|         0|          2|           0|            2|[39.865147,-84.05...|     112|\n",
      "+-----+--------+-------------------+---------+----------+------------+------+------+----+------+----------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+--------------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)\n",
    "logger.write2file(\"Dataset after adding quantiles\",str(df.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast data \n",
    "#Cast all the datacolumns into correct format so they will be sorted to numerical or categorical values. Data removed caused to big feature vector, contained too #many NaN values or were converted into another form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: ['Severity'] \n",
      "Categories: ['Start_Hour', 'Start_Month', 'Weather_Hour', 'Weather_Month', 'position', 'Side', 'Wind_Direction', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
      "Numerical: ['Distance(mi)', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)']\n"
     ]
    }
   ],
   "source": [
    "# Could be handles as separated lists and then sent to a UDF for processing but since low number of features and easier to visualize this select is kept.\n",
    "df = df.select(\n",
    "        col('Severity').cast('int'),\n",
    "        col('Start_Hour').cast('string'),\n",
    "        col('Start_Month').cast('string'),\n",
    "        col('Weather_Hour').cast('string'),\n",
    "        col('Weather_Month').cast('string'),\n",
    "        col('position').cast('string'),\n",
    "        col('Distance(mi)').cast('double'),\n",
    "        col('Side').cast('string'),   \n",
    "        col('Temperature(F)').cast('double'),\n",
    "        col('Wind_Chill(F)').cast('double'), \n",
    "        col('Humidity(%)').cast('double'),\n",
    "        col('Pressure(in)').cast('double'),\n",
    "        col('Visibility(mi)').cast('double'),\n",
    "        col('Wind_Direction').cast('string'),\n",
    "        col('Wind_Speed(mph)').cast('double'), \n",
    "        col('Weather_Condition').cast('string'),\n",
    "        col('Amenity').cast('string'),\n",
    "        col('Bump').cast('string'),\n",
    "        col('Crossing').cast('string'),\n",
    "        col('Give_Way').cast('string'),\n",
    "        col('Junction').cast('string'),\n",
    "        col('No_Exit').cast('string'),\n",
    "        col('Railway').cast('string'),\n",
    "        col('Roundabout').cast('string'),\n",
    "        col('Station').cast('string'),\n",
    "        col('Stop').cast('string'),\n",
    "        col('Traffic_Calming').cast('string'),\n",
    "        col('Traffic_Signal').cast('string'),\n",
    "        col('Turning_Loop').cast('string'),\n",
    "        col('Sunrise_Sunset').cast('string'),\n",
    "        col('Civil_Twilight').cast('string'),\n",
    "        col('Nautical_Twilight').cast('string'),\n",
    "        col('Astronomical_Twilight').cast('string')\n",
    "    ) \n",
    "\n",
    "colLabel = [\"Severity\"]\n",
    "\n",
    "colCat, colNum = createNewClasses(df, sc, colLabel)\n",
    "logger.write2file(\"Categorical groups\",\"Defined Label:\\n\" + str(colLabel) + \"\\nDefined Categories:\\n\" + str(colCat) + \"\\nDefined Numerical:\\n\" +str(colNum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique column values: [Row(Start_Hour=24), Row(Start_Month=12), Row(Weather_Hour=24), Row(Weather_Month=12), Row(position=400), Row(Side=3), Row(Wind_Direction=24), Row(Weather_Condition=119), Row(Amenity=2), Row(Bump=2), Row(Crossing=2), Row(Give_Way=2), Row(Junction=2), Row(No_Exit=2), Row(Railway=2), Row(Roundabout=2), Row(Station=2), Row(Stop=2), Row(Traffic_Calming=2), Row(Traffic_Signal=2), Row(Turning_Loop=1), Row(Sunrise_Sunset=2), Row(Civil_Twilight=2), Row(Nautical_Twilight=2), Row(Astronomical_Twilight=2)]\n"
     ]
    }
   ],
   "source": [
    "df.persist()\n",
    "tmp = [df.select(countDistinct(c).alias(c)).collect()[0] for c in [*colCat]] \n",
    "df.unpersist()\n",
    "print(\"Unique column values:\", tmp)\n",
    "\n",
    "logger.write2file(\"Unique column values\", str(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recheck the missing values\n",
    "#Check so that the output contains 0 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values[Row(Severity=0, Start_Hour=2974, Start_Month=2974, Weather_Hour=35687, Weather_Month=35687, position=0, Distance(mi)=1, Side=1, Temperature(F)=54843, Wind_Chill(F)=1850870, Humidity(%)=57920, Pressure(in)=47054, Visibility(mi)=64348, Wind_Direction=43491, Wind_Speed(mph)=439231, Weather_Condition=64557, Amenity=1, Bump=1, Crossing=1, Give_Way=1, Junction=1, No_Exit=1, Railway=1, Roundabout=1, Station=1, Stop=1, Traffic_Calming=1, Traffic_Signal=1, Turning_Loop=1, Sunrise_Sunset=94, Civil_Twilight=94, Nautical_Twilight=94, Astronomical_Twilight=94)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Severity: int, Start_Hour: string, Start_Month: string, Weather_Hour: string, Weather_Month: string, position: string, Distance(mi): double, Side: string, Temperature(F): double, Wind_Chill(F): double, Humidity(%): double, Pressure(in): double, Visibility(mi): double, Wind_Direction: string, Wind_Speed(mph): double, Weather_Condition: string, Amenity: string, Bump: string, Crossing: string, Give_Way: string, Junction: string, No_Exit: string, Railway: string, Roundabout: string, Station: string, Stop: string, Traffic_Calming: string, Traffic_Signal: string, Turning_Loop: string, Sunrise_Sunset: string, Civil_Twilight: string, Nautical_Twilight: string, Astronomical_Twilight: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.persist()\n",
    "printMissingValues(df,logger)\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove data with occurance less than 1%\n",
    "#Based on information from analysis. With further analysis lower procentage can be used to find better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|   Weather_Condition| count|\n",
      "+--------------------+------+\n",
      "|               Clear|808171|\n",
      "|       Mostly Cloudy|405136|\n",
      "|            Overcast|382479|\n",
      "|                Fair|311864|\n",
      "|       Partly Cloudy|288380|\n",
      "|    Scattered Clouds|204662|\n",
      "|          Light Rain|139622|\n",
      "|              Cloudy|109817|\n",
      "|                null| 64557|\n",
      "|          Light Snow| 42109|\n",
      "|                Haze| 33932|\n",
      "|                Rain| 32585|\n",
      "|                 Fog| 21853|\n",
      "|          Heavy Rain| 11966|\n",
      "|       Light Drizzle| 10201|\n",
      "|Light Thunderstor...|  4928|\n",
      "|                Snow|  4795|\n",
      "|        Thunderstorm|  4438|\n",
      "|               Smoke|  3558|\n",
      "|        Fair / Windy|  3547|\n",
      "+--------------------+------+\n",
      "\n",
      "Rows removed: 101169\n"
     ]
    }
   ],
   "source": [
    "df.persist()\n",
    "original_rows = df.count()\n",
    "n = int(df.count()*0.001) # Limit the plot to ignore conditions below an limit\n",
    "\n",
    "weather_freq = df.groupBy('Weather_Condition').count().orderBy('count',ascending=False)\n",
    "df_filtered = weather_freq.filter(weather_freq['count'] > n)\n",
    "filtered_conditions = df_filtered.select(\"Weather_Condition\").rdd.flatMap(lambda x: x).collect()\n",
    "df = df.filter(df['Weather_Condition'].isin(*filtered_conditions))\n",
    "\n",
    "df_filtered.show()\n",
    "removed_rows = original_rows - df.count()\n",
    "\n",
    "df.unpersist()\n",
    "print(\"Rows removed:\",removed_rows)\n",
    "\n",
    "logger.write2file(\"Weather condition\", str(df_filtered.take(df_filtered.count())) + \"\\nRows removed: \" + str(removed_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove if distict values in category equals 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns with 1 class: ['Turning_Loop']\n"
     ]
    }
   ],
   "source": [
    "df.persist()\n",
    "tmp = [c for c in [*colCat] if df.select(countDistinct(c)).collect()[0][0] <= 1] \n",
    "df.unpersist()\n",
    "\n",
    "[colCat.remove(c) for c in tmp]\n",
    "print(\"Dropping columns with 1 class:\", tmp)\n",
    "logger.write2file(\"Dropping columns with 1 class\", str(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(inputCols=colNum, outputCols=colNum)\n",
    "imputer.setStrategy(\"median\")\n",
    "\n",
    "num_assembler = VectorAssembler(inputCols=colNum, outputCol=\"num_features\")\n",
    "scaler = MinMaxScaler(min=0.0, max=1.0, inputCol=\"num_features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "indexers = [StringIndexer(inputCol = c, outputCol = c +'_IDX', handleInvalid='skip') for c in colCat]\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[indexer.getOutputCol() for indexer in indexers], outputCols=[\"{0}_vec\".format(indexer.getOutputCol()) for indexer in indexers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical values\n",
    "\n",
    "numPipeline = Pipeline(stages=[imputer, num_assembler, scaler])\n",
    "catPipeline = Pipeline(stages=[*indexers, encoder])\n",
    "\n",
    "pipeline = Pipeline(stages=[numPipeline, catPipeline])\n",
    "\n",
    "preprocessed_df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+--------------------+---------------------+----------------+-------------+----------------------+-------------------------+---------------+-------------+----------------+----------------+----------------+---------------+---------------+------------------+---------------+-------------+-----------------------+----------------------+----------------------+----------------------+-------------------------+-----------------------------+\n",
      "|scaledFeatures                                                                                                                              |Start_Hour_IDX_vec|Start_Month_IDX_vec|Weather_Hour_IDX_vec|Weather_Month_IDX_vec|position_IDX_vec|Side_IDX_vec |Wind_Direction_IDX_vec|Weather_Condition_IDX_vec|Amenity_IDX_vec|Bump_IDX_vec |Crossing_IDX_vec|Give_Way_IDX_vec|Junction_IDX_vec|No_Exit_IDX_vec|Railway_IDX_vec|Roundabout_IDX_vec|Station_IDX_vec|Stop_IDX_vec |Traffic_Calming_IDX_vec|Traffic_Signal_IDX_vec|Sunrise_Sunset_IDX_vec|Civil_Twilight_IDX_vec|Nautical_Twilight_IDX_vec|Astronomical_Twilight_IDX_vec|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+--------------------+---------------------+----------------+-------------+----------------------+-------------------------+---------------+-------------+----------------+----------------+----------------+---------------+---------------+------------------+---------------+-------------+-----------------------+----------------------+----------------------+----------------------+-------------------------+-----------------------------+\n",
      "|[2.9973323303180955E-5,0.36514719848053184,0.6517412935323383,0.9090909090909091,0.8983050847457628,0.07142857142857142,0.00850753524550316]|(23,[13],[1.0])   |(11,[10],[1.0])    |(23,[13],[1.0])     |(11,[10],[1.0])      |(399,[46],[1.0])|(2,[0],[1.0])|(23,[0],[1.0])        |(18,[6],[1.0])           |(1,[0],[1.0])  |(1,[0],[1.0])|(1,[0],[1.0])   |(1,[0],[1.0])   |(1,[0],[1.0])   |(1,[0],[1.0])  |(1,[0],[1.0])  |(1,[0],[1.0])     |(1,[0],[1.0])  |(1,[0],[1.0])|(1,[0],[1.0])          |(1,[0],[1.0])         |(1,[],[])             |(1,[],[])             |(1,[],[])                |(1,[],[])                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+--------------------+---------------------+----------------+-------------+----------------------+-------------------------+---------------+-------------+----------------+----------------+----------------+---------------+---------------+------------------+---------------+-------------+-----------------------+----------------------+----------------------+----------------------+-------------------------+-----------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df.select(\"scaledFeatures\",*[c + \"_IDX_vec\" for c in colCat]).show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving into: models/ALL_20200516151111/df_features\n",
      "Saving through format 1\n",
      "Feature set size:  2819074\n"
     ]
    }
   ],
   "source": [
    "va2 = VectorAssembler(inputCols=[\"scaledFeatures\", *[c + \"_IDX_vec\" for c in colCat]], outputCol=\"final_features\")\n",
    "\n",
    "df = va2.transform(preprocessed_df)\n",
    "df.persist()\n",
    "df = df.withColumn('label', col(\"Severity\"))\n",
    "df = df.withColumn('features', df.final_features).select(\"features\",\"label\")\n",
    "df.unpersist()\n",
    "\n",
    "\n",
    "logger.write2fileModel(df, \"df_features\")\n",
    "\n",
    "logger.write2file(\"Feature set size\", str(df.count()) + \"\\n\\n__Feature vector and label:__\\n\" + str(df.take(1)))\n",
    "logger.write2file(\"Number of rows\", str(df.count()))\n",
    "print(\"Feature set size: \",df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "#In order to understand how much the variance affect the dataset we check with PCA. Try to get above 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA - Feature Variance: Top 50:\n",
      "[0.07321439 0.03977307 0.03276837 0.02880588 0.02672684 0.02641962\n",
      " 0.02481372 0.02378435 0.02192447 0.02063203 0.01940309 0.01887571\n",
      " 0.01869002 0.01839489 0.01808785 0.01769793 0.01737013 0.01586431\n",
      " 0.0148442  0.01439132 0.01411925 0.0138601  0.01380034 0.01314934\n",
      " 0.01246885 0.01235675 0.01169773 0.01087211 0.010551   0.01039097\n",
      " 0.00992521 0.00935934 0.00911678 0.00800177 0.00789326 0.00747711\n",
      " 0.00726978 0.00712672 0.00674917 0.00647458 0.00638822 0.00636049\n",
      " 0.00625865 0.00619767 0.00615622 0.00602236 0.00562463 0.0056029\n",
      " 0.00547812 0.00523358]\n",
      "Number of items: 250\n",
      "Sum of variance: 0.9701745094361596\n",
      "Saving into: models/ALL_20200516151111/pca_df\n",
      "Saving through format 1\n"
     ]
    }
   ],
   "source": [
    "length_of_feature_vec = 250\n",
    "pca = PCA(k=length_of_feature_vec, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df)\n",
    "pca_df = pca_model.transform(df)\n",
    "\n",
    "\n",
    "print(\"PCA - Feature Variance:\",\"Top 50:\\n\" + str(pca_model.explainedVariance[:50]) + \"\\nNumber of items: \"+str(length_of_feature_vec)+\"\\nSum of variance: \"+ str(sum(pca_model.explainedVariance)))\n",
    "logger.write2file(\"PCA - Feature variance\", \"Top 50:\\n\" + str(pca_model.explainedVariance[:50]) + \"\\nNumber of items: \"+ str(length_of_feature_vec) +\"\\nSum of variance: \"+ str(sum(pca_model.explainedVariance)))\n",
    "logger.write2fileModel(pca_df, \"pca_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChiSqSelector\n",
    "#Check top 100 which of the values in the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving into: models/ALL_20200516151111/chi_df\n",
      "Saving through format 1\n",
      "Top selected features according to ChiSqSelector: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 252, 253, 254, 256, 257, 258, 259, 260, 261, 262, 263]\n",
      "Transformed selected features: (250,[0,1,2,3,4,5,6,38,70,116],[2.9973323303180955e-05,0.36514719848053184,0.6517412935323383,0.9090909090909091,0.8983050847457628,0.07142857142857142,0.00850753524550316,1.0,1.0,1.0])\n"
     ]
    }
   ],
   "source": [
    "selector = ChiSqSelector(numTopFeatures=length_of_feature_vec, \n",
    "                         labelCol='label', \n",
    "                         featuresCol='features', \n",
    "                         outputCol=\"selectedFeatures\",\n",
    "                         selectorType='numTopFeatures', \n",
    "                         percentile=0.1, \n",
    "                         fpr=0.05, fdr=0.05, fwe=0.05)\n",
    "\n",
    "chi_model = selector.fit(df)\n",
    "chi_df = chi_model.transform(df)\n",
    "\n",
    "logger.write2file(\"Top selected features according to ChiSqSelector\", str(chi_model.selectedFeatures)+ \"\\nNumber of features: \" + str(length_of_feature_vec) + \"\\nExample data:\\n\"+str(chi_df.take(5)))\n",
    "logger.write2fileModel(chi_df, \"chi_df\")\n",
    "\n",
    "print(\"Top selected features according to ChiSqSelector:\", chi_model.selectedFeatures)\n",
    "print(\"Transformed selected features:\",chi_df.head().selectedFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, testSet = chi_df.randomSplit([0.8 ,0.2], 1)\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                   |label|selectedFeatures                                                                                                                                                                                   |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(533,[0,1,2,3,4,5,6,7,30,41,64,75,474,476,499,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532],[2.9973323303180955E-5,0.5180436847103513,0.6517412935323383,0.8080808080808081,0.9098062953995157,0.06428571428571428,0.00850753524550316,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|3    |(250,[0,1,2,3,4,5,6,7,28,39,60,71],[2.9973323303180955E-5,0.5180436847103513,0.6517412935323383,0.8080808080808081,0.9098062953995157,0.06428571428571428,0.00850753524550316,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(533,[0,1,2,3,4,5,6,7,30,41,64,75,474,476,499,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532],[2.9973323303180955E-5,0.5213675213675214,0.6517412935323383,0.5959595959595959,0.912227602905569,0.06428571428571428,0.00850753524550316,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |3    |(250,[0,1,2,3,4,5,6,7,28,39,60,71],[2.9973323303180955E-5,0.5213675213675214,0.6517412935323383,0.5959595959595959,0.912227602905569,0.06428571428571428,0.00850753524550316,1.0,1.0,1.0,1.0,1.0]) |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainSet.show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model..\n",
      "Predicting on testSet..\n",
      "Evaluating predictions..\n",
      "Saving into: models/ALL_20200516151111/LR_Model\n",
      "'function' object has no attribute 'format'\n",
      "Saving through format 2\n",
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       2.0|    3|(533,[0,1,2,3,4,5...|\n",
      "|       2.0|    3|(533,[0,1,2,3,4,5...|\n",
      "|       2.0|    3|(533,[0,1,2,3,4,5...|\n",
      "|       2.0|    3|(533,[0,1,2,3,4,5...|\n",
      "|       2.0|    3|(533,[0,1,2,3,4,5...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Accuracy:  0.6008996863851584 \n",
      "Parameters\n",
      " {Param(parent='LogisticRegression_08a883edf9cb', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent='LogisticRegression_08a883edf9cb', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='LogisticRegression_08a883edf9cb', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.6}\n",
      "name 'MultiClassMetrics' is not defined\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"selectedFeatures\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.1,0.01]) \\\n",
    "                .addGrid(lr.maxIter, [10]) \\\n",
    "                .addGrid(lr.elasticNetParam, [0.6]) \\\n",
    "                .build()\n",
    "try: \n",
    "    model, predictions = evaluateModel(lr, paramGrid, \"LR_Model\", trainSet, testSet, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logger=logger)\n",
    "    \n",
    "    \n",
    "\n",
    "except Exception as e:\n",
    "    logger.write2file(\"Error:\", str(e))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6873242387228724\n",
      "Recall 0.6873242387228724\n",
      "F-1 Score 0.6873242387228724\n",
      "Weighted recall:  0.6873242387228724\n",
      "Weighted precision:  0.9236140551538071\n",
      "Weighted F-1 Score:  0.7737487910605864\n",
      "Weighted false positive rate:  0.3148863959984523\n",
      "Weighted true positive rate:  0.6873242387228724\n",
      "Confusion matrix: DenseMatrix([[3.67755e+05, 1.49417e+05, 1.35840e+04],\n",
      "             [1.02150e+04, 1.93370e+04, 2.59900e+03],\n",
      "             [3.00000e+01, 7.70000e+01, 5.30000e+01]])\n"
     ]
    }
   ],
   "source": [
    "predLabel = predictions.select([col('label').cast('float'),col('prediction').cast('float')])\n",
    "metrics = MulticlassMetrics(predLabel.rdd)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy)\n",
    "print('Recall', metrics.recall())\n",
    "print('F-1 Score', metrics.fMeasure())\n",
    "\n",
    "print(\"Weighted recall: \",metrics.weightedRecall)\n",
    "print(\"Weighted precision: \",metrics.weightedPrecision)\n",
    "print(\"Weighted F-1 Score: \",metrics.weightedFMeasure())\n",
    "print(\"Weighted false positive rate: \",metrics.weightedFalsePositiveRate)\n",
    "print(\"Weighted true positive rate: \", metrics.weightedTruePositiveRate)\n",
    "print(\"Confusion matrix:\", metrics.confusionMatrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model..\n",
      "Predicting on testSet..\n",
      "Evaluating predictions..\n",
      "Saving into: models/ALL_20200516151111/DT_Model\n",
      "'function' object has no attribute 'format'\n",
      "Saving through format 2\n",
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       3.0|    3|(533,[0,1,2,3,4,5...|\n",
      "|       3.0|    3|(533,[0,1,2,3,4,5...|\n",
      "|       3.0|    3|(533,[0,1,2,3,4,5...|\n",
      "|       3.0|    3|(533,[0,1,2,3,4,5...|\n",
      "|       3.0|    3|(533,[0,1,2,3,4,5...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Accuracy:  0.5885962528525441 \n",
      "Parameters\n",
      " {}\n",
      "Accuracy:  0.6805849477332121\n",
      "Recall 0.6805849477332121\n",
      "F-1 Score 0.6805849477332121\n",
      "Weighted recall:  0.6805849477332122\n",
      "Weighted precision:  0.9264912448740179\n",
      "Weighted F-1 Score:  0.7725736426138803\n",
      "Weighted false positive rate:  0.33909805368751733\n",
      "Weighted true positive rate:  0.6805849477332122\n",
      "Confusion matrix: DenseMatrix([[3.67828e+05, 1.53361e+05, 1.21470e+04],\n",
      "             [1.01230e+04, 1.53800e+04, 3.94800e+03],\n",
      "             [4.90000e+01, 9.00000e+01, 1.41000e+02]])\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"selectedFeatures\") \n",
    "\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "model, predictions = evaluateModel(dt, paramGrid, \"DT_Model\", trainSet, testSet, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logger=logger)\n",
    "\n",
    "predLabel = predictions.select([col('label').cast('float'),col('prediction').cast('float')])\n",
    "metrics = MulticlassMetrics(predLabel.rdd)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy)\n",
    "print('Recall', metrics.recall())\n",
    "print('F-1 Score', metrics.fMeasure())\n",
    "\n",
    "print(\"Weighted recall: \",metrics.weightedRecall)\n",
    "print(\"Weighted precision: \",metrics.weightedPrecision)\n",
    "print(\"Weighted F-1 Score: \",metrics.weightedFMeasure())\n",
    "print(\"Weighted false positive rate: \",metrics.weightedFalsePositiveRate)\n",
    "print(\"Weighted true positive rate: \", metrics.weightedTruePositiveRate)\n",
    "print(\"Confusion matrix:\", metrics.confusionMatrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"selectedFeatures\") #numTrees=10\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(rf.numTrees,[15]) \\\n",
    "                .build()\n",
    "\n",
    "_, _ = evaluateModel(rf, paramGrid, \"RF_Model\", trainSet, testSet, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.write2file(\"Program finished!\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.write2file(\"New model folder created\", \\\n",
    "               \"Model finished: Yes\" + \\\n",
    "               \"\\nFolder name: \" + logger.timeSignature + \\\n",
    "               \"\\nState: \"+ filter_state + \\\n",
    "               \"\\nLogs directory: \" + logger.logs_dir +  \\\n",
    "               \"\\nFile: \" + file + \\\n",
    "               \"\\nNote: \" + model_Note, \\\n",
    "               logs_dir=model_indexing_dir \\\n",
    "              )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
