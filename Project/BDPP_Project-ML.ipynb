{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport ibmos2spark\\n# @hidden_cell\\ncredentials = {\\n    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\\n    'service_id': 'iam-ServiceId-1d71bac5-a341-449f-9ca7-b29de95e771a',\\n    'iam_service_endpoint': 'https://iam.eu-gb.bluemix.net/oidc/token',\\n    'api_key': 'ejujKveBRS3Bk7l3pzjemEQDhTSaOKmqCg6x6osXWkx4'\\n}\\n\\nconfiguration_name = 'os_2d970470a6354234a1716fe9f4db519b_configs'\\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\\n\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder.getOrCreate()\\n\\ndf = spark.read  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')  .option('header', 'true')  .load(cos.url('US_Accidents_Dec19.csv', 'bdppproject-donotdelete-pr-nejualq57kuqe4'))\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-1d71bac5-a341-449f-9ca7-b29de95e771a',\n",
    "    'iam_service_endpoint': 'https://iam.eu-gb.bluemix.net/oidc/token',\n",
    "    'api_key': 'ejujKveBRS3Bk7l3pzjemEQDhTSaOKmqCg6x6osXWkx4'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_2d970470a6354234a1716fe9f4db519b_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('US_Accidents_Dec19.csv', 'bdppproject-donotdelete-pr-nejualq57kuqe4'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from datetime import datetime\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler,OneHotEncoderEstimator,QuantileDiscretizer, StringIndexer, Imputer,StandardScaler,MinMaxScaler\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import ChiSqSelector,PCA\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier,RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_indexing_dir = \"log_index.md\"\n",
    "models_dir = \"models\"\n",
    "logs_dir = \"models\" \n",
    "filter_state = 'CA' # ALL if all states\n",
    "model_Note = ''\n",
    "log_mode=True # Logs data to files, Dont use on massive dataset (debug_mode is recommended while on)\n",
    "debug_mode=False # Reduce number of rows collected for faster processing\n",
    "enable_plots = True # Dont use unless debug_mode is on, or datasize is small\n",
    "logger = logging(models_dir, logs_dir, \"\", filter_state, enabled=log_mode)\n",
    "\n",
    "file = \"data/US_Accidents_Dec19.csv\"\n",
    "\n",
    "\n",
    "df, sc,spark = setup_spark(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4082e07eadd9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Spark Project>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: [('spark.driver.memory', '4g'), ('spark.executor.memory', '4g'), ('spark.driver.host', '4082e07eadd9'), ('spark.ui.enabled', 'true'), ('spark.executor.id', 'driver'), ('spark.app.name', 'Spark Project'), ('spark.app.id', 'local-1589636674668'), ('spark.ui.killEnabled', 'false'), ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.executor.instances', '1'), ('spark.master', 'local[*]'), ('spark.executor.cores', '1'), ('spark.submit.deployMode', 'client'), ('spark.kryoserializer.buffer.max', '15'), ('spark.ui.showConsoleProgress', 'true'), ('spark.driver.port', '33439'), ('spark.driver.cores', '1')]\n"
     ]
    }
   ],
   "source": [
    "tmp = sc._conf.getAll()\n",
    "logger.write2file(\"New Spark session\", str(tmp))\n",
    "print(\"Config:\",tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2925212"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if debug_mode == True: \n",
    "    df.createOrReplaceTempView(\"records\")\n",
    "    reviewData = spark.sql(\"SELECT * FROM records LIMIT 100000\") \n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: ['Severity'] \n",
      "Categories: ['Street', 'Side', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone', 'Airport_Code', 'Wind_Direction', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
      "Numerical: ['TMC', 'Start_Lat', 'Start_Lng', 'Distance(mi)', 'Number', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)']\n"
     ]
    }
   ],
   "source": [
    "colLabel = [\"Severity\"]\n",
    "\n",
    "colRem = ['ID', \n",
    "          'Source',\n",
    "          'End_Time',\n",
    "          'End_Lat',\n",
    "          'End_Lng',\n",
    "          'Description',\n",
    "        ]\n",
    "\n",
    "df, colCat, colNum = setup_variables(df, sc, colLabel, colRem)  \n",
    "\n",
    "logger.write2file(\"Number of rows\", str(df.count()))\n",
    "logger.write2file(\"Categorical groups\",\"Defined Label:\\n\" + str(colLabel) + \"\\nDefined Categories:\\n\" + str(colCat) + \"\\nDefined Numerical:\\n\" +str(colNum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse single state\n",
    "#Create a model for separates states instead of all states at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_state != 'ALL':\n",
    "    df = df.filter(df.State == filter_state) # Lowers the dataset quite a lot\n",
    "    logger.write2file(\"Specified state\",str(filter_state))\n",
    "else:\n",
    "    logger.write2file(\"No state specified\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify time\n",
    "#Convert the timestamp into a numeric value and then into a string so that the time of day and month can be categorised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+---------+-----------+------------+------+-----------+----+-------+------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "|  TMC|Severity|         Start_Time|Start_Lat|  Start_Lng|Distance(mi)|Number|     Street|Side|   City|County|State|Zipcode|Country|  Timezone|Airport_Code|  Weather_Timestamp|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Amenity| Bump|Crossing|Give_Way|Junction|No_Exit|Railway|Roundabout|Station| Stop|Traffic_Calming|Traffic_Signal|Turning_Loop|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|Start_Hour|Start_Month|Weather_Hour|Weather_Month|\n",
      "+-----+--------+-------------------+---------+-----------+------------+------+-----------+----+-------+------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "|201.0|       3|2016-06-21 10:34:40|  38.0853|-122.233017|         0.0|  null|Magazine St|   R|Vallejo|Solano|   CA|  94591|     US|US/Pacific|        KAPC|2016-06-21 10:54:00|          75.0|         null|       48.0|        30.0|          10.0|      Variable|            5.8|             null|            Clear|  false|false|   false|   false|   false|  false|  false|     false|  false|false|          false|         false|       false|           Day|           Day|              Day|                  Day|         3|          6|           3|            6|\n",
      "+-----+--------+-------------------+---------+-----------+------------+------+-----------+----+-------+------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to int then cast to string\n",
    "\n",
    "df = df.withColumn('Start_Hour', hour(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "df = df.withColumn('Start_Month', month(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "df = df.withColumn('Weather_Hour', hour(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "df = df.withColumn('Weather_Month', month(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "df.show(1)\n",
    "logger.write2file(\"Dataset after modifying UTC timestamp\", str(df.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster quantiles to reduce the feature vector for position\n",
    "#Since feature crossing wont work due to the feature vector is to big (regarding to the given resources) we hope that Clustering might reduce them and keep as much data as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.5875128445613211\n",
      "+--------+\n",
      "|position|\n",
      "+--------+\n",
      "|     192|\n",
      "|     386|\n",
      "|     230|\n",
      "|     180|\n",
      "|     348|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number_of_clusters= 400\n",
    "pos_assembler = VectorAssembler(inputCols=['Start_Lat', 'Start_Lng'], outputCol=\"position_features\")\n",
    "preprocessed_cord = pos_assembler.transform(df)\n",
    "\n",
    "trainSet, testSet = preprocessed_cord.randomSplit([0.9 ,0.1], 1)\n",
    "kmeans = KMeans(k=number_of_clusters, featuresCol='position_features', predictionCol='position')\n",
    "model = kmeans.fit(trainSet)\n",
    "predictions = model.transform(testSet)\n",
    "\n",
    "evaluator = ClusteringEvaluator(featuresCol='position_features', predictionCol='position')\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Transform the hole dataset\n",
    "df = model.transform(preprocessed_cord)\n",
    "df.select('position').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+---------+-----------+------------+------+-----------+----+-------+------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+--------------------+--------+\n",
      "|  TMC|Severity|         Start_Time|Start_Lat|  Start_Lng|Distance(mi)|Number|     Street|Side|   City|County|State|Zipcode|Country|  Timezone|Airport_Code|  Weather_Timestamp|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Amenity| Bump|Crossing|Give_Way|Junction|No_Exit|Railway|Roundabout|Station| Stop|Traffic_Calming|Traffic_Signal|Turning_Loop|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|Start_Hour|Start_Month|Weather_Hour|Weather_Month|   position_features|position|\n",
      "+-----+--------+-------------------+---------+-----------+------------+------+-----------+----+-------+------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+--------------------+--------+\n",
      "|201.0|       3|2016-06-21 10:34:40|  38.0853|-122.233017|         0.0|  null|Magazine St|   R|Vallejo|Solano|   CA|  94591|     US|US/Pacific|        KAPC|2016-06-21 10:54:00|          75.0|         null|       48.0|        30.0|          10.0|      Variable|            5.8|             null|            Clear|  false|false|   false|   false|   false|  false|  false|     false|  false|false|          false|         false|       false|           Day|           Day|              Day|                  Day|         3|          6|           3|            6|[38.0853,-122.233...|     192|\n",
      "+-----+--------+-------------------+---------+-----------+------------+------+-----------+----+-------+------+-----+-------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+----------+-----------+------------+-------------+--------------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)\n",
    "logger.write2file(\"Dataset after adding quantiles\",str(df.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast data \n",
    "#Cast all the datacolumns into correct format so they will be sorted to numerical or categorical values. Data removed caused to big feature vector, contained too #many NaN values or were converted into another form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: ['Severity'] \n",
      "Categories: ['Start_Hour', 'Start_Month', 'Weather_Hour', 'Weather_Month', 'position', 'Side', 'Wind_Direction', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
      "Numerical: ['Distance(mi)', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)']\n"
     ]
    }
   ],
   "source": [
    "# Could be handles as separated lists and then sent to a UDF for processing but since low number of features and easier to visualize this select is kept.\n",
    "df = df.select(\n",
    "        col('Severity').cast('int'),\n",
    "        col('Start_Hour').cast('string'),\n",
    "        col('Start_Month').cast('string'),\n",
    "        col('Weather_Hour').cast('string'),\n",
    "        col('Weather_Month').cast('string'),\n",
    "        col('position').cast('string'),\n",
    "        col('Distance(mi)').cast('double'),\n",
    "        col('Side').cast('string'),   \n",
    "        col('Temperature(F)').cast('double'),\n",
    "        col('Wind_Chill(F)').cast('double'), \n",
    "        col('Humidity(%)').cast('double'),\n",
    "        col('Pressure(in)').cast('double'),\n",
    "        col('Visibility(mi)').cast('double'),\n",
    "        col('Wind_Direction').cast('string'),\n",
    "        col('Wind_Speed(mph)').cast('double'), \n",
    "        col('Weather_Condition').cast('string'),\n",
    "        col('Amenity').cast('string'),\n",
    "        col('Bump').cast('string'),\n",
    "        col('Crossing').cast('string'),\n",
    "        col('Give_Way').cast('string'),\n",
    "        col('Junction').cast('string'),\n",
    "        col('No_Exit').cast('string'),\n",
    "        col('Railway').cast('string'),\n",
    "        col('Roundabout').cast('string'),\n",
    "        col('Station').cast('string'),\n",
    "        col('Stop').cast('string'),\n",
    "        col('Traffic_Calming').cast('string'),\n",
    "        col('Traffic_Signal').cast('string'),\n",
    "        col('Turning_Loop').cast('string'),\n",
    "        col('Sunrise_Sunset').cast('string'),\n",
    "        col('Civil_Twilight').cast('string'),\n",
    "        col('Nautical_Twilight').cast('string'),\n",
    "        col('Astronomical_Twilight').cast('string')\n",
    "    ) \n",
    "\n",
    "colLabel = [\"Severity\"]\n",
    "\n",
    "colCat, colNum = createNewClasses(df, sc, colLabel)\n",
    "logger.write2file(\"Categorical groups\",\"Defined Label:\\n\" + str(colLabel) + \"\\nDefined Categories:\\n\" + str(colCat) + \"\\nDefined Numerical:\\n\" +str(colNum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique column values: [Row(Start_Hour=24), Row(Start_Month=12), Row(Weather_Hour=24), Row(Weather_Month=12), Row(position=400), Row(Side=3), Row(Wind_Direction=24), Row(Weather_Condition=69), Row(Amenity=2), Row(Bump=2), Row(Crossing=2), Row(Give_Way=2), Row(Junction=2), Row(No_Exit=2), Row(Railway=2), Row(Roundabout=2), Row(Station=2), Row(Stop=2), Row(Traffic_Calming=2), Row(Traffic_Signal=2), Row(Turning_Loop=1), Row(Sunrise_Sunset=2), Row(Civil_Twilight=2), Row(Nautical_Twilight=2), Row(Astronomical_Twilight=2)]\n"
     ]
    }
   ],
   "source": [
    "df.persist()\n",
    "tmp = [df.select(countDistinct(c).alias(c)).collect()[0] for c in [*colCat]] \n",
    "df.unpersist()\n",
    "print(\"Unique column values:\", tmp)\n",
    "\n",
    "logger.write2file(\"Unique column values\", str(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recheck the missing values\n",
    "#Check so that the output contains 0 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values[Row(Severity=0, Start_Hour=196, Start_Month=196, Weather_Hour=9292, Weather_Month=9292, position=0, Distance(mi)=0, Side=0, Temperature(F)=14956, Wind_Chill(F)=471554, Humidity(%)=16139, Pressure(in)=11097, Visibility(mi)=13058, Wind_Direction=11924, Wind_Speed(mph)=128584, Weather_Condition=12730, Amenity=0, Bump=0, Crossing=0, Give_Way=0, Junction=0, No_Exit=0, Railway=0, Roundabout=0, Station=0, Stop=0, Traffic_Calming=0, Traffic_Signal=0, Turning_Loop=0, Sunrise_Sunset=6, Civil_Twilight=6, Nautical_Twilight=6, Astronomical_Twilight=6)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Severity: int, Start_Hour: string, Start_Month: string, Weather_Hour: string, Weather_Month: string, position: string, Distance(mi): double, Side: string, Temperature(F): double, Wind_Chill(F): double, Humidity(%): double, Pressure(in): double, Visibility(mi): double, Wind_Direction: string, Wind_Speed(mph): double, Weather_Condition: string, Amenity: string, Bump: string, Crossing: string, Give_Way: string, Junction: string, No_Exit: string, Railway: string, Roundabout: string, Station: string, Stop: string, Traffic_Calming: string, Traffic_Signal: string, Turning_Loop: string, Sunrise_Sunset: string, Civil_Twilight: string, Nautical_Twilight: string, Astronomical_Twilight: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "printMissingValues(df,logger)\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove data with occurance less than 1%\n",
    "#Based on information from analysis. With further analysis lower procentage can be used to find better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|Weather_Condition| count|\n",
      "+-----------------+------+\n",
      "|            Clear|253070|\n",
      "|             Fair| 96006|\n",
      "|    Mostly Cloudy| 59929|\n",
      "|    Partly Cloudy| 58523|\n",
      "|         Overcast| 58472|\n",
      "| Scattered Clouds| 29356|\n",
      "|           Cloudy| 24063|\n",
      "|             Haze| 21561|\n",
      "|       Light Rain| 20421|\n",
      "|             null| 12730|\n",
      "|             Rain|  6438|\n",
      "|              Fog|  4100|\n",
      "|       Heavy Rain|  2246|\n",
      "|            Smoke|  1921|\n",
      "|     Fair / Windy|  1179|\n",
      "+-----------------+------+\n",
      "\n",
      "Rows removed: 15887\n"
     ]
    }
   ],
   "source": [
    "df.persist()\n",
    "original_rows = df.count()\n",
    "n = int(df.count()*0.001) # Limit the plot to ignore conditions below an limit\n",
    "\n",
    "weather_freq = df.groupBy('Weather_Condition').count().orderBy('count',ascending=False)\n",
    "df_filtered = weather_freq.filter(weather_freq['count'] > n)\n",
    "filtered_conditions = df_filtered.select(\"Weather_Condition\").rdd.flatMap(lambda x: x).collect()\n",
    "df = df.filter(df['Weather_Condition'].isin(*filtered_conditions))\n",
    "\n",
    "df_filtered.show()\n",
    "removed_rows = original_rows - df.count()\n",
    "\n",
    "df.unpersist()\n",
    "print(\"Rows removed:\",removed_rows)\n",
    "\n",
    "logger.write2file(\"Weather condition\", str(df_filtered.take(df_filtered.count())) + \"\\nRows removed: \" + str(removed_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove if distict values in category equals 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns with 1 class: ['Turning_Loop']\n"
     ]
    }
   ],
   "source": [
    "df.persist()\n",
    "tmp = [c for c in [*colCat] if df.select(countDistinct(c)).collect()[0][0] <= 1] \n",
    "df.unpersist()\n",
    "\n",
    "[colCat.remove(c) for c in tmp]\n",
    "print(\"Dropping columns with 1 class:\", tmp)\n",
    "logger.write2file(\"Dropping columns with 1 class\", str(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(inputCols=colNum, outputCols=colNum)\n",
    "imputer.setStrategy(\"median\")\n",
    "\n",
    "num_assembler = VectorAssembler(inputCols=colNum, outputCol=\"num_features\")\n",
    "scaler = MinMaxScaler(min=0.0, max=1.0, inputCol=\"num_features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "indexers = [StringIndexer(inputCol = c, outputCol = c +'_IDX', handleInvalid='skip') for c in colCat]\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[indexer.getOutputCol() for indexer in indexers], outputCols=[\"{0}_vec\".format(indexer.getOutputCol()) for indexer in indexers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical values\n",
    "\n",
    "numPipeline = Pipeline(stages=[imputer, num_assembler, scaler])\n",
    "catPipeline = Pipeline(stages=[*indexers, encoder])\n",
    "\n",
    "pipeline = Pipeline(stages=[numPipeline, catPipeline])\n",
    "\n",
    "preprocessed_df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+--------------------+---------------------+-----------------+-------------+----------------------+-------------------------+---------------+-------------+----------------+----------------+----------------+---------------+---------------+------------------+---------------+-------------+-----------------------+----------------------+----------------------+----------------------+-------------------------+-----------------------------+\n",
      "|scaledFeatures                                                                                                             |Start_Hour_IDX_vec|Start_Month_IDX_vec|Weather_Hour_IDX_vec|Weather_Month_IDX_vec|position_IDX_vec |Side_IDX_vec |Wind_Direction_IDX_vec|Weather_Condition_IDX_vec|Amenity_IDX_vec|Bump_IDX_vec |Crossing_IDX_vec|Give_Way_IDX_vec|Junction_IDX_vec|No_Exit_IDX_vec|Railway_IDX_vec|Roundabout_IDX_vec|Station_IDX_vec|Stop_IDX_vec |Traffic_Calming_IDX_vec|Traffic_Signal_IDX_vec|Sunrise_Sunset_IDX_vec|Civil_Twilight_IDX_vec|Nautical_Twilight_IDX_vec|Astronomical_Twilight_IDX_vec|\n",
      "+---------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+--------------------+---------------------+-----------------+-------------+----------------------+-------------------------+---------------+-------------+----------------+----------------+----------------+---------------+---------------+------------------+---------------+-------------+-----------------------+----------------------+----------------------+----------------------+-------------------------+-----------------------------+\n",
      "|[0.0,0.6668785759694851,0.6379044684129429,0.47474747474747475,0.9071472205253511,0.07142857142857142,0.007049100631988332]|(23,[10],[1.0])   |(11,[7],[1.0])     |(23,[8],[1.0])      |(11,[7],[1.0])       |(399,[137],[1.0])|(2,[0],[1.0])|(23,[7],[1.0])        |(13,[0],[1.0])           |(1,[0],[1.0])  |(1,[0],[1.0])|(1,[0],[1.0])   |(1,[0],[1.0])   |(1,[0],[1.0])   |(1,[0],[1.0])  |(1,[0],[1.0])  |(1,[0],[1.0])     |(1,[0],[1.0])  |(1,[0],[1.0])|(1,[0],[1.0])          |(1,[0],[1.0])         |(1,[0],[1.0])         |(1,[0],[1.0])         |(1,[0],[1.0])            |(1,[0],[1.0])                |\n",
      "+---------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+--------------------+---------------------+-----------------+-------------+----------------------+-------------------------+---------------+-------------+----------------+----------------+----------------+---------------+---------------+------------------+---------------+-------------+-----------------------+----------------------+----------------------+----------------------+-------------------------+-----------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df.select(\"scaledFeatures\",*[c + \"_IDX_vec\" for c in colCat]).show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving into: models/CA_20200516134433/df_features\n",
      "Saving through format 1\n",
      "Feature set size:  635496\n"
     ]
    }
   ],
   "source": [
    "va2 = VectorAssembler(inputCols=[\"scaledFeatures\", *[c + \"_IDX_vec\" for c in colCat]], outputCol=\"final_features\")\n",
    "\n",
    "df = va2.transform(preprocessed_df)\n",
    "df.persist()\n",
    "df = df.withColumn('label', col(\"Severity\"))\n",
    "df = df.withColumn('features', df.final_features).select(\"features\",\"label\")\n",
    "df.unpersist()\n",
    "\n",
    "\n",
    "logger.write2fileModel(df, \"df_features\")\n",
    "\n",
    "logger.write2file(\"Feature set size\", str(df.count()) + \"\\n\\n__Feature vector and label:__\\n\" + str(df.take(1)))\n",
    "logger.write2file(\"Number of rows\", str(df.count()))\n",
    "print(\"Feature set size: \",df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique column values: ['Turning_Loop']\n"
     ]
    }
   ],
   "source": [
    "# Checking how many classes that can be used\n",
    "df.persist()\n",
    "tmp = [df.select(countDistinct(c).alias(c)).collect()[0] for c in [*colCat, \"position\"]] \n",
    "df.unpersist()\n",
    "print(\"Unique column values:\", tmp)\n",
    "\n",
    "#logger.write2file(\"Unique column values\", str(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "#In order to understand how much the variance affect the dataset we check with PCA. Try to get above 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA - Feature Variance: Top 50:\n",
      "[0.09518584 0.04233791 0.03188333 0.02993041 0.02626405 0.02584982\n",
      " 0.0248223  0.02433016 0.02170134 0.02106393 0.01971681 0.01912112\n",
      " 0.01864251 0.01732487 0.01655825 0.01551064 0.01488142 0.01471481\n",
      " 0.01381917 0.01375734 0.01311017 0.01297961 0.01252857 0.01216356\n",
      " 0.01196153 0.01160826 0.01129605 0.01100749 0.01056157 0.0103407\n",
      " 0.00993814 0.00959634 0.00941    0.00864802 0.00854767 0.00800119\n",
      " 0.00739704 0.00730925 0.00703481 0.00665766 0.00658773 0.0061834\n",
      " 0.00589329 0.00584354 0.00550845 0.00537102 0.00485981 0.00478672\n",
      " 0.00463308 0.00446822]\n",
      "Number of items: 250\n",
      "Sum of variance: 0.9700605677790778\n",
      "Saving into: models/CA_20200516134433/pca_df\n",
      "Saving through format 1\n"
     ]
    }
   ],
   "source": [
    "length_of_feature_vec = 250\n",
    "pca = PCA(k=length_of_feature_vec, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df)\n",
    "pca_df = pca_model.transform(df)\n",
    "\n",
    "\n",
    "print(\"PCA - Feature Variance:\",\"Top 50:\\n\" + str(pca_model.explainedVariance[:50]) + \"\\nNumber of items: \"+str(length_of_feature_vec)+\"\\nSum of variance: \"+ str(sum(pca_model.explainedVariance)))\n",
    "logger.write2file(\"PCA - Feature variance\", \"Top 50:\\n\" + str(pca_model.explainedVariance[:50]) + \"\\nNumber of items: \"+ str(length_of_feature_vec) +\"\\nSum of variance: \"+ str(sum(pca_model.explainedVariance)))\n",
    "logger.write2fileModel(pca_df, \"pca_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChiSqSelector\n",
    "#Check top 100 which of the values in the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving into: models/CA_20200516134433/chi_df\n",
      "Saving through format 1\n",
      "Top selected features according to ChiSqSelector: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 16, 17, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 49, 51, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 108, 109, 110, 113, 114, 115, 116, 117, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 158, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 172, 173, 174, 175, 178, 179, 180, 181, 183, 184, 185, 186, 187, 189, 190, 192, 193, 194, 195, 196, 197, 198, 199, 200, 203, 204, 205, 206, 207, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 235, 236, 237, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265, 266, 267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294]\n",
      "Transformed selected features: (250,[1,2,3,4,5,6,14,32,40,57,173],[0.6668785759694851,0.6379044684129429,0.47474747474747475,0.9071472205253511,0.07142857142857142,0.007049100631988332,1.0,1.0,1.0,1.0,1.0])\n"
     ]
    }
   ],
   "source": [
    "selector = ChiSqSelector(numTopFeatures=length_of_feature_vec, \n",
    "                         labelCol='label', \n",
    "                         featuresCol='features', \n",
    "                         outputCol=\"selectedFeatures\",\n",
    "                         selectorType='numTopFeatures', \n",
    "                         percentile=0.1, \n",
    "                         fpr=0.05, fdr=0.05, fwe=0.05)\n",
    "\n",
    "chi_model = selector.fit(df)\n",
    "chi_df = chi_model.transform(df)\n",
    "\n",
    "logger.write2file(\"Top selected features according to ChiSqSelector\", str(chi_model.selectedFeatures)+ \"\\nNumber of features: \" + str(length_of_feature_vec) + \"\\nExample data:\\n\"+str(chi_df.take(5)))\n",
    "logger.write2fileModel(chi_df, \"chi_df\")\n",
    "\n",
    "print(\"Top selected features according to ChiSqSelector:\", chi_model.selectedFeatures)\n",
    "print(\"Transformed selected features:\",chi_df.head().selectedFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, testSet = chi_df.randomSplit([0.8 ,0.2], 1)\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                    |label|selectedFeatures                                                                                                                                                                                    |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(528,[0,1,2,3,4,5,6,7,30,41,64,75,474,480,501,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527],[4.4012146052121174E-5,0.6478067387158296,0.6379044684129429,0.6565656565656566,0.9062309102015882,0.07142857142857142,0.009844433641225085,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|2    |(250,[0,1,2,3,4,5,6,7,25,36,51,61],[4.4012146052121174E-5,0.6478067387158296,0.6379044684129429,0.6565656565656566,0.9062309102015882,0.07142857142857142,0.009844433641225085,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(528,[0,1,2,3,4,5,6,7,30,41,64,75,474,484,501,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527],[4.4012146052121174E-5,0.6293706293706294,0.6379044684129429,0.6262626262626263,0.9080635308491142,0.07142857142857142,0.009844433641225085,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|3    |(250,[0,1,2,3,4,5,6,7,25,36,51,61],[4.4012146052121174E-5,0.6293706293706294,0.6379044684129429,0.6262626262626263,0.9080635308491142,0.07142857142857142,0.009844433641225085,1.0,1.0,1.0,1.0,1.0])|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainSet.show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model..\n",
      "Predicting on testSet..\n",
      "Evaluating predictions..\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"selectedFeatures\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.1,0.01]) \\\n",
    "                .addGrid(lr.maxIter, [10]) \\\n",
    "                .addGrid(lr.elasticNetParam, [0.6]) \\\n",
    "                .build()\n",
    "try: \n",
    "    model, _ = evaluateModel(lr, paramGrid, \"LR_Model\", trainSet, testSet, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logger=logger)\n",
    "except Exception as e:\n",
    "    logger.write2file(\"Error:\", str(e))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"selectedFeatures\") \n",
    "\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "_, _ = evaluateModel(dt, paramGrid, \"DT_Model\", trainSet, testSet, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"selectedFeatures\") #numTrees=10\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(rf.numTrees,[15]) \\\n",
    "                .build()\n",
    "\n",
    "_, _ = evaluateModel(rf, paramGrid, \"RF_Model\", trainSet, testSet, evaluator=MulticlassClassificationEvaluator(),k=10, seed=None,logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.write2file(\"Program finished!\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.write2file(\"New model folder created\", \\\n",
    "               \"Model finished: Yes\" + \\\n",
    "               \"\\nFolder name: \" + logger.timeSignature + \\\n",
    "               \"\\nState: \"+ filter_state + \\\n",
    "               \"\\nLogs directory: \" + logger.logs_dir +  \\\n",
    "               \"\\nFile: \" + file + \\\n",
    "               \"\\nNote: \" + model_Note, \\\n",
    "               logs_dir=model_indexing_dir \\\n",
    "              )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
