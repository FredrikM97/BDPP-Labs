{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport ibmos2spark\\n# @hidden_cell\\ncredentials = {\\n    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\\n    'service_id': 'iam-ServiceId-1d71bac5-a341-449f-9ca7-b29de95e771a',\\n    'iam_service_endpoint': 'https://iam.eu-gb.bluemix.net/oidc/token',\\n    'api_key': 'ejujKveBRS3Bk7l3pzjemEQDhTSaOKmqCg6x6osXWkx4'\\n}\\n\\nconfiguration_name = 'os_2d970470a6354234a1716fe9f4db519b_configs'\\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\\n\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder.getOrCreate()\\n\\ndf = spark.read  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')  .option('header', 'true')  .load(cos.url('US_Accidents_Dec19.csv', 'bdppproject-donotdelete-pr-nejualq57kuqe4'))\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-1d71bac5-a341-449f-9ca7-b29de95e771a',\n",
    "    'iam_service_endpoint': 'https://iam.eu-gb.bluemix.net/oidc/token',\n",
    "    'api_key': 'ejujKveBRS3Bk7l3pzjemEQDhTSaOKmqCg6x6osXWkx4'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_2d970470a6354234a1716fe9f4db519b_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('US_Accidents_Dec19.csv', 'bdppproject-donotdelete-pr-nejualq57kuqe4'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "try:\n",
    "    spark.close()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    conf = SparkConf().setAppName('Spark Project')\n",
    "    sc = SparkContext(conf=conf)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "#.config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#SparkContext.setSystemProperty('spark.executor.memory', '8g')\n",
    "#spark = SparkSession \\\n",
    "#    .builder \\\n",
    "#    .appName(\"Spark Project\") \\\n",
    "#    .config(\"spark.executor.memory\", '5g') \\\n",
    "#    .config(\"spark.driver.memory\", '5g') \\\n",
    "#    .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Project\") \\\n",
    "    .config(\"spark.executor.memory\", \"2000mb\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\",200) \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.default.parallelism\", \"6\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.load(\"data/US_Accidents_Dec19.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, count, col, isnan, countDistinct,from_unixtime,from_utc_timestamp, unix_timestamp,split, to_timestamp, hour, month, lit,collect_list\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Due to problem in spark 3.0.0\n",
    "spark.conf.set(\"spark.sql.legacy.utcTimestampFunc.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data information\n",
    "\n",
    "# Numerical values\n",
    "\n",
    "colLabel = [\"Severity\"]\n",
    "\n",
    "colRem = ['ID', \n",
    "          'Source',\n",
    "          'End_Time',\n",
    "          'End_Lat',\n",
    "          'End_Lng',\n",
    "          'Description',\n",
    "        ]\n",
    "\n",
    "# Dropping data that cant help during model\n",
    "df = df.drop(*colRem)\n",
    "\n",
    "# Convert boolean to string since PCA cant handle boolean which should be a class\n",
    "df = df.select(*[col(c[0]).cast(\"string\").alias(c[0]) if c[1] == 'boolean' else col(c[0]).alias(c[0]) for c in df.dtypes])\n",
    "\n",
    "#renamedHousing.select([count(when(col(c).isNull(), c)).alias(c) for c in colNum]).show()\n",
    "rdd = sc.parallelize(df.dtypes)\n",
    "\n",
    "colCat = rdd.map(lambda i: i[0] if (i[1]=='string' or i[1]=='boolean' and i[0]) else None).filter(lambda i: i != None).collect()\n",
    "colNum = rdd.map(lambda i: i[0] if (i[1]=='double' and i[0]) else None).filter(lambda i: i != None).collect()\n",
    "\n",
    "print(colCat)\n",
    "print(colNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Categories:\\n \\\n",
    "Labels: {len(colLabel)}\\n \\\n",
    "Classes: {len(colCat)}\\n \\\n",
    "Removed: {len(colRem)}\\n \\\n",
    "Numerical: {len(colNum)}\") \n",
    "\n",
    "print(f\"Rows: {df.count()}\\nColumns {len(df.columns)}\")\n",
    "df.printSchema()\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(colNum).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data\n",
    "\n",
    "df_missingVals_cols = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "\n",
    "# Missing value in each column\n",
    "df_missingVals_cols.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how many classes that can be used\n",
    "df.agg(*(countDistinct(col(c)).alias(c) for c in colCat)).collect()[0].asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([*colNum, *colLabel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=colNum,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid = \"skip\")\n",
    "\n",
    "df_attributes = assembler.transform(df)\n",
    "df_attributes.select(\"features\").show(1,False)\n",
    "\n",
    "\n",
    "\n",
    "r1 = Correlation.corr(df_attributes, \"features\").head()\n",
    "\n",
    "print(\"correlation matrix:\\n\" + str(r1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "\n",
    "# Generate a large random dataset\n",
    "\n",
    "# Compute the correlation matrix\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "#mask = np.triu(np.ones_like(r1, dtype=np.bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(r1[0].toArray().tolist(), annot=True, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5},xticklabels=colNum,yticklabels=colNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_freq = df.groupBy('Severity').count().orderBy('count',ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(severity_freq)\n",
    "pd_severity = rdd.toDF().toPandas()\n",
    "\n",
    "# Plot data\n",
    "fig,ax = plt.subplots(figsize=(16,10))\n",
    "b = sns.barplot(pd_severity['Severity'],pd_severity['count'], color='blue')\n",
    "b.axes.set_title(\"Severity distribution\",fontsize=20)\n",
    "b.set_xlabel(\"Number of Accidents\",fontsize=15)\n",
    "b.set_ylabel(\"Severity\",fontsize=15)\n",
    "b.tick_params(labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of severity and state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_freq = df.groupBy('State').count().orderBy('count',ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(state_freq)\n",
    "pd_states = rdd.toDF().toPandas()\n",
    "\n",
    "# Plot data\n",
    "fig,ax = plt.subplots(figsize=(16,10))\n",
    "b = sns.barplot(pd_states['State'],pd_states['count'], color='blue')\n",
    "b.axes.set_title(\"Severity distribution for each state\",fontsize=20)\n",
    "b.set_xlabel(\"Number of Accidents\",fontsize=15)\n",
    "b.set_ylabel(\"State\",fontsize=15)\n",
    "b.tick_params(labelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_severity_freq = df.groupBy('State','Severity').count().orderBy('count',ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(state_severity_freq)\n",
    "pd_state_severity = rdd.toDF().toPandas()\n",
    "\n",
    "# Plot data\n",
    "fig,ax = plt.subplots(figsize=(16,10))\n",
    "b = sns.barplot(x=\"State\", y=\"count\", hue=\"Severity\", data=pd_state_severity)\n",
    "#b = sns.barplot(pd_state_severity['State', 'Severity'],pd_state_severity['count'])\n",
    "b.axes.set_title(\"Severity distribution for each state\",fontsize=20)\n",
    "b.set_xlabel(\"Number of Accidents\",fontsize=15)\n",
    "b.set_ylabel(\"State\",fontsize=15)\n",
    "b.tick_params(labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_freq = df.groupBy('Weather_Condition').count().orderBy('count',ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_limit = 12 # Limit on how many objects that should be displayed\n",
    "\n",
    "fig, ax=plt.subplots(figsize=(16,25))\n",
    "n = int(df.count()*0.001) # Limit the plot to ignore conditions below an limit\n",
    "\n",
    "rdd = sc.parallelize(weather_freq)\n",
    "rdd_filtered = rdd.filter(lambda x: x['count'] > n)\n",
    "print(rdd_filtered.collect())\n",
    "\n",
    "\n",
    "pd_weather = rdd_filtered.toDF().toPandas()\n",
    "\n",
    "b = sns.barplot(pd_weather['count'][:],pd_weather['Weather_Condition'][:], color=\"blue\")\n",
    "\n",
    "b.axes.set_title(\"Weather Condition for accidents above 1% of total set\",fontsize=20)\n",
    "b.set_xlabel(\"Number of Accidents\",fontsize=15)\n",
    "b.set_ylabel(\"Weather_Condition\",fontsize=15)\n",
    "b.tick_params(labelsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How severe was the condition for each type of weather?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time when accidents occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = df.selectExpr(\"hour(to_timestamp(from_utc_timestamp(Start_Time, Timezone), 'yyyy-MM-dd HH:mm:ss')) as Start_Time\")\n",
    "time_freq = df_time.groupBy('Start_Time').count().orderBy('count',ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(time_freq)\n",
    "pd_time = rdd.toDF().toPandas()\n",
    "\n",
    "# Plot data\n",
    "fig,ax = plt.subplots(figsize=(16,10))\n",
    "\n",
    "b = sns.barplot(pd_time['Start_Time'],pd_time['count'], color='blue')\n",
    "b.axes.set_title(\"Daytime for accidents\",fontsize=20)\n",
    "b.set_xlabel(\"Time (Hours)\",fontsize=15)\n",
    "b.set_ylabel(\"Number of Accidents\",fontsize=15)\n",
    "b.tick_params(labelsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development of accidents on Month basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = df.selectExpr(\"month(to_timestamp(from_utc_timestamp(Start_Time, Timezone), 'yyyy-MM-dd HH:mm:ss')) as Start_Time\")\n",
    "time_freq = df_time.groupBy('Start_Time').count().orderBy('count',ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(time_freq)\n",
    "pd_time = rdd.toDF().toPandas()\n",
    "\n",
    "# Plot data\n",
    "fig,ax = plt.subplots(figsize=(16,10))\n",
    "\n",
    "b = sns.barplot(pd_time['Start_Time'],pd_time['count'], color='blue')\n",
    "b.axes.set_title(\"Montly distribution for accidents\",fontsize=20)\n",
    "b.set_xlabel(\"Time (Month)\",fontsize=15)\n",
    "b.set_ylabel(\"Number of Accidents\",fontsize=15)\n",
    "b.tick_params(labelsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=df.select(collect_list('Start_Lat')).first()[0],y=df.select(collect_list('Start_Lng')).first()[0],height=10)\n",
    "plt.ylabel('Start_Lat', fontsize=12)\n",
    "plt.xlabel('Start_Lng', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "# PCA - Not very good for categorical values\n",
    "#pca_assembler = VectorAssembler(inputCols=df.columns, outputCol=\"pca_features\",handleInvalid=\"skip\")\n",
    "#pca_df = pca_assembler.transform(df)\n",
    "#print(pca_df)\n",
    "#pca = PCA(k=5, inputCol=\"pca_features\", outputCol=\"pcaFeatures\")\n",
    "#model = pca.fit(pca_df)\n",
    "\n",
    "#result = model.transform(pca_features).select(\"pcaFeatures\")\n",
    "#result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop most data here since the data will be casted later therefor it will be considered string and not NaN\n",
    "#df = df.drop('Number', 'Wind_Chill(F)', 'Precipitation(in)')\n",
    "print(df.count())\n",
    "#df.na.drop(*df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to int then cast to string\n",
    "\n",
    "df = df.withColumn('Start_Hour', hour(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "df = df.withColumn('Start_Month', month(to_timestamp(from_utc_timestamp(df['Start_Time'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "df = df.withColumn('Weather_Hour', hour(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "df = df.withColumn('Weather_Month', month(to_timestamp(from_utc_timestamp(df['Weather_Timestamp'], df['Timezone']).cast('string'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recheck the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missingVals_cols = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "# Missing value in each column\n",
    "df_missingVals_cols.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from string to integer values\n",
    "# TODO: Does not work since data contains Nan/Null?\n",
    "df = df.select(\n",
    "        col('TMC').cast('string'),\n",
    "        col('Severity').cast('int'),\n",
    "        #col('Start_Time').cast('int'),\n",
    "        col('Start_Hour').cast('string'),\n",
    "        col('Start_Month').cast('string'),\n",
    "        col('Weather_Hour').cast('string'),\n",
    "        col('Weather_Month').cast('string'),\n",
    "        #col('Start_Lat').cast('string'),\n",
    "        #col('Start_Lng').cast('string'),\n",
    "        col('Distance(mi)').cast('double'),\n",
    "        col('Street').cast('string'),       # Remove eventually?\n",
    "        col('Side').cast('string'),         # Remove eventually?\n",
    "        col('City').cast('string'),         # Remove eventually?\n",
    "        col('State').cast('string'),        # Remove eventually?\n",
    "        #col('Zipcode').cast('string'),      # Remove eventually?\n",
    "        #col('Country').cast('string'),      # Remove eventually?\n",
    "        #col('Airport_Code').cast('string'), # Remove eventually?\n",
    "      #  col('Weather_Timestamp').cast('int'),\n",
    "        col('Temperature(F)').cast('double'),\n",
    "        col('Wind_Chill(F)').cast('double'),\n",
    "        col('Humidity(%)').cast('double'),\n",
    "        col('Pressure(in)').cast('double'),\n",
    "        col('Visibility(mi)').cast('double'),\n",
    "        col('Wind_Direction').cast('string'),\n",
    "        col('Wind_Speed(mph)').cast('double'),\n",
    "        col('Weather_Condition').cast('string'),\n",
    "        col('Amenity').cast('string'),\n",
    "        col('Bump').cast('string'),\n",
    "        col('Crossing').cast('string'),\n",
    "        col('Give_Way').cast('string'),\n",
    "        col('Junction').cast('string'),\n",
    "        col('No_Exit').cast('string'),\n",
    "        col('Railway').cast('string'),\n",
    "        col('Roundabout').cast('string'),\n",
    "        col('Station').cast('string'),\n",
    "        col('Stop').cast('string'),\n",
    "        col('Traffic_Calming').cast('string'),\n",
    "        col('Traffic_Signal').cast('string'),\n",
    "        col('Turning_Loop').cast('string'),\n",
    "        col('Sunrise_Sunset').cast('string'),\n",
    "        col('Civil_Twilight').cast('string'),\n",
    "        col('Nautical_Twilight').cast('string'),\n",
    "        col('Astronomical_Twilight').cast('string')\n",
    "    ) \n",
    "\n",
    "# Since the last check can be unorganised we recreate a new list that contains all data\n",
    "colLabel = [\"Severity\"]\n",
    "\n",
    "rdd = sc.parallelize(df.dtypes)\n",
    "colCat = rdd.map(lambda i: i[0] if (i[1]=='string' or i[1]=='boolean' and i[0] not in colLabel) else None).filter(lambda i: i != None).collect()\n",
    "colNum = rdd.map(lambda i: i[0] if (i[1]=='double' and i[0] not in colLabel) else None).filter(lambda i: i != None).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colLabel)\n",
    "print(colCat)\n",
    "print(colNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantiles function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove City, Country, State, Zipcode, Airport_Code\n",
    "# In order to reduce dimensionality a kernel function will be applied upon the Start_Lat and Start_Lng then fused in order to create a new set of data\n",
    "# Start_Lat\n",
    "# Start_Lng\n",
    "\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\"\"\"\n",
    "dscretizer_Lat = QuantileDiscretizer(numBuckets=50, inputCol=\"Start_Lat\", outputCol=\"Start_Lat\")\n",
    "dscretizer_Lng = QuantileDiscretizer(numBuckets=50, inputCol=\"Start_Lng\", outputCol=\"Start_Lng\")\n",
    "\n",
    "# Convert into categorical values (two blocks)\n",
    "quant_Lat_model = quantiles_Lat.fit(df_features)\n",
    "quant_Lng_model = quantiles_Lng.fit(df_features) \n",
    "\n",
    "quanted_Lat = quant_Lat_model.transform(df_features)\n",
    "quanted_Lng = quant_Lng_model.transform(df_features)\n",
    "\n",
    "# Into categorical values\n",
    "stringIndexer = StringIndexer(inputCol=[\"Start_Lat\",\"Start_Lng\"], outputCol=[\"Start_Lat\",\"Start_Lng\"] + \"_num\")\n",
    "indexer_model = stringIndexer.fit(df_features) # Change into Quanted_Lat, Quanted_Lng\n",
    "indexed = indexer_model.transform(df_features)\n",
    "\n",
    "# One-hot \n",
    "encoder = OneHotEncoder(inputCols=[\"Start_Lat_num\",\"Start_Lng_num\"], outputCols=[s + \"_vec\" for s in colCat])\n",
    "\n",
    "encoded = encoder.fit(indexed)\n",
    "\"\"\"\n",
    "\n",
    "# Do multiplication into a final feature vector (Kernel trick)\n",
    "\n",
    "# Remove City, Country, State, Zipcode, Airport_Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probobly a bad Idea but why not?\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "#imputer = Imputer(inputCols=['Wind_Chill(F)', 'Wind_Speed(mph)'], outputCols=['Wind_Chill(F)', 'Wind_Speed(mph)'])\n",
    "imputer = Imputer(inputCols=colNum, outputCols=colNum)\n",
    "imputer.setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "#Create a vector\n",
    "num_assembler = VectorAssembler(inputCols=colNum, outputCol=\"num_features\",handleInvalid=\"keep\")\n",
    "scaler = StandardScaler(inputCol=\"num_features\", withMean=True, withStd=True, outputCol=\"scaledFeatures\")\n",
    "scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "#colCat\n",
    "#col_assembler = VectorAssembler(inputCols=colCat, outputCol=\"cat_features\",handleInvalid=\"keep\")\n",
    "#indexers = StringIndexer(inputCol=\"cat_features\", outputCol=\"indexedFeatures\",handleInvalid=\"keep\")\n",
    "\n",
    "indexers = [StringIndexer(inputCol = c, outputCol = c +'_IDX', handleInvalid='keep') for c in colCat]\n",
    "print(indexers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "#from pyspark.ml import Pipeline\n",
    "#pipeline = Pipeline(stages=indexers)\n",
    "#df_indexers = pipeline.fit(df).transform(df)\n",
    "\n",
    "#col_assembler = VectorAssembler(inputCols=[c +'_IDX' for c in colCat], outputCol=\"features\",handleInvalid=\"keep\")\n",
    "#data = col_assembler.transform(df_indexers)\n",
    "#df_features.withColumn('features', col(col_assembler.transform(df_indexers)))\n",
    "#df_indexers.select([c +'_IDX' for c in colCat]).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder \n",
    "\n",
    "#ohe = OneHotEncoder()\n",
    "#ohe.setInputCols([c + \"_IDX\" for c in colCat])\n",
    "#ohe.setOutputCols([c + \"_IDX_vec\" for c in colCat])\n",
    "\n",
    "#col_assembler = VectorAssembler(inputCols=[c + \"_IDX\" for c in colCat], outputCol=\"col_features\",handleInvalid=\"keep\")\n",
    "encoder = OneHotEncoder(inputCols=[c + \"_IDX\" for c in colCat], outputCols=[c + \"_IDX_vec\" for c in colCat],handleInvalid=\"keep\")\n",
    "#encoder = OneHotEncoder(inputCol=\"col_features\", outputCol=\"col_features_vec\",handleInvalid=\"keep\")\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "#from pyspark.ml import Pipeline \n",
    "#pipeline = Pipeline(stages=[*indexers,*encoder])\n",
    "#df_indexers = pipeline.fit(df).transform(df)\n",
    "#df_indexers.select([c +'_IDX_vec' for c in colCat]).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical values\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "numPipeline = Pipeline(stages=[imputer, num_assembler, scaler])\n",
    "catPipeline = Pipeline(stages=[*indexers, encoder])\n",
    "\n",
    "#.select(\"scaledFeatures\", *[c +'_IDX_vec' for c in colCat])\n",
    "pipeline = Pipeline(stages=[numPipeline, catPipeline])\n",
    "preprocessed_df = pipeline.fit(df).transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catPipeline.fit(df).transform(df).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df.select(\"scaledFeatures\",*[c + \"_IDX_vec\" for c in colCat]).show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va2 = VectorAssembler(inputCols=[\"scaledFeatures\",*[c + \"_IDX_vec\" for c in colCat]], outputCol=\"final_features\")\n",
    "\n",
    "temp1 = va2.transform(preprocessed_df)\n",
    "temp1.show(1)\n",
    "temp1 = temp1.withColumn('label', col(\"Severity\"))\n",
    "dataset = temp1.withColumn('features', temp1.final_features).select(\"features\",\"label\")\n",
    "dataset.show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChiSqSelector\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "selector = ChiSqSelector(numTopFeatures=3, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "\n",
    "result = selector.fit(dataset).transform(dataset)\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, testSet = dataset.randomSplit([0.8 ,0.2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"label\",maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(trainSet)\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "print(\"RMSE: %s\" % str(trainingSummary.rootMeanSquaredError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
